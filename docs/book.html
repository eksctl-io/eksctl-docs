<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.23">
<title>Eksctl User Guide</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body id="top" class="book toc2 toc-left">
<div id="header">
<h1>Eksctl User Guide</h1>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#what-is-eksctl">1. What is Amazon EKS?</a>
<ul class="sectlevel2">
<li><a href="#faq">1.1. Eksctl FAQ</a></li>
<li><a href="#dry-run">1.2. Dry Run</a></li>
</ul>
</li>
<li><a href="#get-started">2. Get started with eksctl</a></li>
<li><a href="#installation">3. Installation</a>
<ul class="sectlevel2">
<li><a href="#prerequisite">3.1. Prerequisite</a></li>
<li><a href="#for-unix">3.2. For Unix</a></li>
<li><a href="#for-windows">3.3. For Windows</a></li>
<li><a href="#docker">3.4. Docker</a></li>
<li><a href="#third-party-installers-not-recommended">3.5. Third-Party Installers (Not Recommended)</a></li>
<li><a href="#shell-completion">3.6. Shell Completion</a></li>
<li><a href="#features">3.7. Features</a></li>
</ul>
</li>
<li><a href="#clusters">4. Clusters</a>
<ul class="sectlevel2">
<li><a href="#creating-and-managing-clusters">4.1. Creating and managing clusters</a></li>
<li><a href="#eks-auto-mode">4.2. EKS Auto Mode</a></li>
<li><a href="#eks-access-entries">4.3. EKS Access Entries</a></li>
<li><a href="#non-eksctl-created-clusters">4.4. Non eksctl-created clusters</a></li>
<li><a href="#registering-non-eks-clusters-with-eks-connector">4.5. Registering non-EKS clusters with EKS Connector</a></li>
<li><a href="#customizing-the-kubelet">4.6. Customizing kubelet configuration</a></li>
<li><a href="#cloudwatch-logging">4.7. CloudWatch logging</a></li>
<li><a href="#eks-private-cluster">4.8. EKS Fully-Private Cluster</a></li>
<li><a href="#addons">4.9. Addons</a></li>
<li><a href="#enabling-access-for-amazon-emr">4.10. Enabling Access for Amazon EMR</a></li>
<li><a href="#cluster-upgrade">4.11. Cluster upgrades</a></li>
<li><a href="#default-add-on-updates">4.12. Default add-on updates</a></li>
<li><a href="#support-for-zonal-shift-in-eks-clusters">4.13. Support for Zonal Shift in EKS clusters</a></li>
<li><a href="#eksctl-karpenter">4.14. Karpenter Support</a></li>
</ul>
</li>
<li><a href="#nodegroups">5. Nodegroups</a>
<ul class="sectlevel2">
<li><a href="#unmanaged-nodegroups">5.1. Unmanaged nodegroups</a></li>
<li><a href="#eks-managed-nodegroups">5.2. EKS managed nodegroups</a></li>
<li><a href="#node-bootstrapping">5.3. Node bootstrapping</a></li>
<li><a href="#launch-template-support">5.4. Launch Template support for Managed Nodegroups</a></li>
<li><a href="#custom-subnets">5.5. Custom subnets</a></li>
<li><a href="#custom-dns">5.6. Custom DNS</a></li>
<li><a href="#taints">5.7. Taints</a></li>
<li><a href="#instance-selector">5.8. Instance Selector</a></li>
<li><a href="#spot-instances">5.9. Spot instances</a></li>
<li><a href="#gpu-support">5.10. GPU Support</a></li>
<li><a href="#arm-support">5.11. ARM Support</a></li>
<li><a href="#auto-scaling">5.12. Auto Scaling</a></li>
<li><a href="#custom-ami-support">5.13. Custom AMI support</a></li>
<li><a href="#define-container-runtime">5.14. Define Container Runtime</a></li>
<li><a href="#windows-worker-nodes">5.15. Windows Worker Nodes</a></li>
<li><a href="#additional-volume-mappings">5.16. Additional Volume Mappings</a></li>
<li><a href="#eks-hybrid-nodes">5.17. EKS Hybrid Nodes</a></li>
<li><a href="#support-for-node-repair-config-in-eks-managed-nodegroups">5.18. Support for Node Repair Config in EKS Managed Nodegroups</a></li>
<li><a href="#creating-nodegroups">5.19. Creating nodegroups</a></li>
<li><a href="#nodegroup-selection-in-config-files">5.20. Nodegroup selection in config files</a></li>
<li><a href="#listing-nodegroups">5.21. Listing nodegroups</a></li>
<li><a href="#nodegroup-immutability">5.22. Nodegroup immutability</a></li>
<li><a href="#scaling-nodegroups">5.23. Scaling nodegroups</a></li>
<li><a href="#nodegroup-delete">5.24. Deleting and draining nodegroups</a></li>
<li><a href="#other-features">5.25. Other features</a></li>
</ul>
</li>
<li><a href="#networking">6. Networking</a>
<ul class="sectlevel2">
<li><a href="#vpc-configuration">6.1. VPC Configuration</a></li>
<li><a href="#vpc-subnet-settings">6.2. Subnet Settings</a></li>
<li><a href="#vpc-cluster-access">6.3. Cluster Access</a></li>
<li><a href="#updating-control-plane-subnets-and-security-groups">6.4. Updating control plane subnets and security groups</a></li>
<li><a href="#vpc-ip-family">6.5. IPv6 Support</a></li>
</ul>
</li>
<li><a href="#iam">7. IAM</a>
<ul class="sectlevel2">
<li><a href="#minimum-iam-policies">7.1. Minimum IAM policies</a></li>
<li><a href="#iam-permissions-boundary">7.2. IAM permissions boundary</a></li>
<li><a href="#iam-policies">7.3. IAM policies</a></li>
<li><a href="#manage-iam-users-and-roles">7.4. Manage IAM users and roles</a></li>
<li><a href="#iamserviceaccounts">7.5. IAM Roles for Service Accounts</a></li>
<li><a href="#pod-id">7.6. EKS Pod Identity Associations</a></li>
</ul>
</li>
<li><a href="#gitops">8. GitOps with Flux v2</a>
<ul class="sectlevel2">
<li><a href="#installing-flux-v2-gitops-toolkit">8.1. Installing Flux v2 (GitOps Toolkit)</a></li>
<li><a href="#quickstart-profiles">8.2. Quickstart profiles</a></li>
<li><a href="#further-reading">8.3. Further reading</a></li>
</ul>
</li>
<li><a href="#deployment">9. Deployment options</a>
<ul class="sectlevel2">
<li><a href="#eksctl-anywhere">9.1. EKS Anywhere</a></li>
<li><a href="#outposts">9.2. AWS Outposts Support</a></li>
</ul>
</li>
<li><a href="#security">10. Security</a>
<ul class="sectlevel2">
<li><a href="#withoidc">10.1. <code>withOIDC</code></a></li>
<li><a href="#disablepodimds">10.2. <code>disablePodIMDS</code></a></li>
<li><a href="#kms-envelope-encryption-for-eks-clusters">10.3. KMS Envelope Encryption for EKS clusters</a></li>
</ul>
</li>
<li><a href="#troubleshooting">11. Troubleshooting</a>
<ul class="sectlevel2">
<li><a href="#failed-stack-creation">11.1. Failed stack creation</a></li>
<li><a href="#subnet-id-subnet-11111111-is-not-the-same-as-subnet-22222222">11.2. subnet ID "subnet-11111111" is not the same as "subnet-22222222"</a></li>
<li><a href="#deletion-issues">11.3. Deletion issues</a></li>
<li><a href="#kubectl-logs-and-kubectl-run-fails-with-authorization-error">11.4. kubectl logs and kubectl run fails with Authorization Error</a></li>
</ul>
</li>
<li><a href="#announcements">12. Announcements</a>
<ul class="sectlevel2">
<li><a href="#managed-nodegroups-default">12.1. Managed Nodegroups Default</a></li>
<li><a href="#nodegroup-bootstrap-override-for-custom-amis">12.2. Nodegroup Bootstrap Override For Custom AMIs</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="quoteblock abstract">
<blockquote>
<div class="paragraph">
<p>Eksctl User Guide</p>
</div>
</blockquote>
</div>
</div>
</div>
<div class="sect1">
<h2 id="what-is-eksctl">1. What is Amazon EKS?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>eksctl is a command-line utility tool that automates and simplifies the process of creating, managing, and operating Amazon Elastic Kubernetes Service (Amazon EKS) clusters. Written in Go, eksctl provides a declarative syntax through YAML configurations and CLI commands to handle complex EKS cluster operations that would otherwise require multiple manual steps across different AWS services.</p>
</div>
<div class="paragraph">
<p>eksctl is particularly valuable for DevOps engineers, platform teams, and Kubernetes administrators who need to consistently deploy and manage EKS clusters at scale. It&#8217;s especially useful for organizations transitioning from self-managed Kubernetes to EKS, or those implementing infrastructure as code (IaC) practices, as it can be integrated into existing CI/CD pipelines and automation workflows. The tool abstracts away many of the complex interactions between AWS services required for EKS cluster setup, such as VPC configuration, IAM role creation, and security group management.</p>
</div>
<div class="paragraph">
<p>Key features of eksctl include the ability to create fully functional EKS clusters with a single command, support for custom networking configurations, automated node group management, and GitOps workflow integration. The tool manages cluster upgrades, scales node groups, and handles add-on management through a declarative approach. eksctl also provides advanced capabilities such as Fargate profile configuration, managed node group customization, and spot instance integration, while maintaining compatibility with other AWS tools and services through native AWS SDK integration.</p>
</div>
<div class="sect2 topic">
<h3 id="faq">1.1. Eksctl FAQ</h3>
<div class="sect3">
<h4 id="general">1.1.1. General</h4>
<div class="paragraph">
<p><strong>Can I use <code>eksctl</code> to manage clusters which weren&#8217;t created by <code>eksctl</code>?</strong></p>
</div>
<div class="paragraph">
<p>Yes! From version <code>0.40.0</code> you can run <code>eksctl</code> against any cluster, whether it was created
by <code>eksctl</code> or not. Find out more [here](/usage/unowned-clusters).</p>
</div>
</div>
<div class="sect3">
<h4 id="nodegroup-faq">1.1.2. Nodegroups</h4>
<div class="paragraph">
<p><strong>How can I change the instance type of my nodegroup?</strong></p>
</div>
<div class="paragraph">
<p>From the point of view of <code>eksctl</code>, nodegroups are immutable. This means that once created the only thing <code>eksctl</code> can do is scale the nodegroup up or down.</p>
</div>
<div class="paragraph">
<p>To change the instance type, create a new nodegroup with the desired instance type, then drain it so that the workloads move to the new one. After that step is complete you can delete the old nodegroup.</p>
</div>
<div class="paragraph">
<p><strong>How can I see the generated userdata for a nodegroup?</strong></p>
</div>
<div class="paragraph">
<p>First you&#8217;ll need the name of the Cloudformation stack that manages the nodegroup:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl utils describe-stacks --region=us-west-2 --cluster NAME</code></pre>
</div>
</div>
<div class="paragraph">
<p>You&#8217;ll see a name similar to <code>eksctl-CLUSTER_NAME-nodegroup-NODEGROUP_NAME</code>.</p>
</div>
<div class="paragraph">
<p>You can execute the following to get the userdata. Note the final line which decodes from base64 and decompresses the gzipped data.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">NG_STACK=eksctl-scrumptious-monster-1595247364-nodegroup-ng-29b8862f # your stack here
LAUNCH_TEMPLATE_ID=$(aws cloudformation describe-stack-resources --stack-name $NG_STACK \
| jq -r '.StackResources | map(select(.LogicalResourceId == "NodeGroupLaunchTemplate") \
| .PhysicalResourceId)[0]')
aws ec2 describe-launch-template-versions --launch-template-id $LAUNCH_TEMPLATE_ID \
| jq -r '.LaunchTemplateVersions[0].LaunchTemplateData.UserData' \
| base64 -d | gunzip</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="ingress">1.1.3. Ingress</h4>
<div class="paragraph">
<p><strong>How do I set up ingress with <code>eksctl</code>?</strong></p>
</div>
<div class="paragraph">
<p>We recommend using the <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller">AWS Load Balancer Controller</a>.
Documentation on how to deploy the controller to your cluster, as well as how to migrate from the old ALB Ingress Controller, can be found <a href="eks/latest/userguide/alb-ingress.html">here</a>.</p>
</div>
<div class="paragraph">
<p>For the Nginx Ingress Controller, setup would be the same as <a href="https://kubernetes.github.io/ingress-nginx/deploy/#aws">any on other Kubernetes cluster</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="kubectl">1.1.4. Kubectl</h4>
<div class="paragraph">
<p><strong>I&#8217;m using an HTTPS proxy and cluster certificate validation fails, how can I use the system CAs?</strong></p>
</div>
<div class="paragraph">
<p>Set the environment variable <code>KUBECONFIG_USE_SYSTEM_CA</code> to make <code>kubeconfig</code> respect the system certificate authorities.</p>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="dry-run">1.2. Dry Run</h3>
<div class="paragraph">
<p>The dry-run feature allows you to inspect and change the instances matched by the instance selector before proceeding
to creating a nodegroup.</p>
</div>
<div class="paragraph">
<p>When <code>eksctl create cluster</code> is called with the instance selector options and <code>--dry-run</code>, eksctl will output a
ClusterConfig file containing a nodegroup representing the CLI options and the instance types set to the instances
matched by the instance selector resource criteria.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create cluster --name development --dry-run


apiVersion: eksctl.io/v1alpha5
cloudWatch:
  clusterLogging: {}
iam:
  vpcResourceControllerPolicy: true
  withOIDC: false
kind: ClusterConfig
managedNodeGroups:
- amiFamily: AmazonLinux2
  desiredCapacity: 2
  disableIMDSv1: true
  disablePodIMDS: false
  iam:
    withAddonPolicies:
      albIngress: false
      appMesh: false
      appMeshPreview: false
      autoScaler: false
      certManager: false
      cloudWatch: false
      ebs: false
      efs: false
      externalDNS: false
      fsx: false
      imageBuilder: false
      xRay: false
  instanceSelector: {}
  instanceType: m5.large
  labels:
    alpha.eksctl.io/cluster-name: development
    alpha.eksctl.io/nodegroup-name: ng-4aba8a47
  maxSize: 2
  minSize: 2
  name: ng-4aba8a47
  privateNetworking: false
  securityGroups:
    withLocal: null
    withShared: null
  ssh:
    allow: false
    enableSsm: false
    publicKeyPath: ""
  tags:
    alpha.eksctl.io/nodegroup-name: ng-4aba8a47
    alpha.eksctl.io/nodegroup-type: managed
  volumeIOPS: 3000
  volumeSize: 80
  volumeThroughput: 125
  volumeType: gp3
metadata:
  name: development
  region: us-west-2
  version: "1.24"
privateCluster:
  enabled: false
vpc:
  autoAllocateIPv6: false
  cidr: 192.168.0.0/16
  clusterEndpoints:
    privateAccess: false
    publicAccess: true
  manageSharedNodeSecurityGroupRules: true
  nat:
    gateway: Single</code></pre>
</div>
</div>
<div class="paragraph">
<p>The generated ClusterConfig can then be passed to <code>eksctl create cluster</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create cluster -f generated-cluster.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>When a ClusterConfig file is passed with <code>--dry-run</code>, eksctl will output a ClusterConfig file containing the values set in the file.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>There are certain one-off options that cannot be represented in the ClusterConfig file, e.g., <code>--install-vpc-controllers</code>. It is expected that <code>eksctl create cluster --&lt;options...&gt; --dry-run</code> &gt; config.yaml followed by <code>eksctl create cluster -f config.yaml</code> would be equivalent to running the first command without <code>--dry-run</code>. eksctl therefore disallows passing options that cannot be represented in the config file when <code>--dry-run</code> is passed. If you need to pass an AWS profile, set the <code>AWS_PROFILE</code> environment variable, instead of passing the <code>--profile</code> CLI option.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="get-started">2. Get started with eksctl</h2>
<div class="sectionbody">
<div class="paragraph">
<p>eksctl is a command-line utility tool that automates and simplifies the process of creating, managing, and operating Amazon Elastic Kubernetes Service (Amazon EKS) clusters. Written in Go, eksctl provides a declarative syntax through YAML configurations and CLI commands to handle complex EKS cluster operations that would otherwise require multiple manual steps across different AWS services.</p>
</div>
<div class="paragraph">
<p>eksctl is particularly valuable for DevOps engineers, platform teams, and Kubernetes administrators who need to consistently deploy and manage EKS clusters at scale. It&#8217;s especially useful for organizations transitioning from self-managed Kubernetes to EKS, or those implementing infrastructure as code (IaC) practices, as it can be integrated into existing CI/CD pipelines and automation workflows. The tool abstracts away many of the complex interactions between AWS services required for EKS cluster setup, such as VPC configuration, IAM role creation, and security group management.</p>
</div>
<div class="paragraph">
<p>Key features of eksctl include the ability to create fully functional EKS clusters with a single command, support for custom networking configurations, automated node group management, and GitOps workflow integration. The tool manages cluster upgrades, scales node groups, and handles add-on management through a declarative approach. eksctl also provides advanced capabilities such as Fargate profile configuration, managed node group customization, and spot instance integration, while maintaining compatibility with other AWS tools and services through native AWS SDK integration.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="installation">3. Installation</h2>
<div class="sectionbody">
<div class="paragraph">
<p><code>eksctl</code> is available to install from official releases as described below. We recommend that you install <code>eksctl</code> from only the official GitHub releases. You may opt to use a third-party installer, but please be advised that AWS does not maintain nor support these methods of installation. Use them at your own discretion.</p>
</div>
<div class="sect2">
<h3 id="prerequisite">3.1. Prerequisite</h3>
<div class="paragraph">
<p>You will need to have AWS API credentials configured. What works for AWS CLI or any other tools (kops, Terraform, etc.) should be sufficient. You can use <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html"><code>~/.aws/credentials</code> file</a>
or <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-environment.html">environment variables</a>. For more information read <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-environment.html">AWS documentation</a>.</p>
</div>
<div class="paragraph">
<p>You will also need <a href="https://github.com/kubernetes-sigs/aws-iam-authenticator">AWS IAM Authenticator for Kubernetes</a> command (either <code>aws-iam-authenticator</code> or <code>aws eks get-token</code> (available in version 1.16.156 or greater of AWS CLI) in your <code>PATH</code>.</p>
</div>
<div class="paragraph">
<p>The IAM account used for EKS cluster creation should have these minimal access levels.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">AWS Service</th>
<th class="tableblock halign-left valign-top">Access Level</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CloudFormation</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Full Access</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">EC2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Full:</strong> Tagging <strong>Limited:</strong> List, Read, Write</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">EC2 Auto Scaling</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Limited:</strong> List, Write</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">EKS</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Full Access</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">IAM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Limited:</strong> List, Read, Write, Permissions Management</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Systems Manager</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>Limited:</strong> List, Read</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="for-unix">3.2. For Unix</h3>
<div class="paragraph">
<p>To download the latest release, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sh" data-lang="sh"># for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
ARCH=amd64
PLATFORM=$(uname -s)_$ARCH

curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"

# (Optional) Verify checksum
curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check

tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp &amp;&amp; rm eksctl_$PLATFORM.tar.gz

sudo mv /tmp/eksctl /usr/local/bin</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="for-windows">3.3. For Windows</h3>
<div class="paragraph">
<p>Direct download (latest release):</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_windows_amd64.zip">AMD64/x86_64</a></p>
</li>
<li>
<p><a href="https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_windows_armv6.zip">ARMv6</a></p>
</li>
<li>
<p><a href="https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_windows_armv7.zip">ARMv7</a></p>
</li>
<li>
<p><a href="https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_windows_arm64.zip">ARM64</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Make sure to unzip the archive to a folder in the <code>PATH</code> variable.</p>
</div>
<div class="paragraph">
<p>Optionally, verify the checksum:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Download the checksum file: <a href="https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt">latest</a></p>
</li>
<li>
<p>Use Command Prompt to manually compare <code>CertUtil</code>'s output to the checksum file downloaded.</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-cmd" data-lang="cmd">  REM Replace amd64 with armv6, armv7 or arm64
  CertUtil -hashfile eksctl_Windows_amd64.zip SHA256</code></pre>
</div>
</div>
</li>
<li>
<p>Using PowerShell to automate the verification using the <code>-eq</code> operator to get a <code>True</code> or <code>False</code> result:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-pwsh" data-lang="pwsh"># Replace amd64 with armv6, armv7 or arm64
 (Get-FileHash -Algorithm SHA256 .\eksctl_Windows_amd64.zip).Hash -eq ((Get-Content .\eksctl_checksums.txt) -match 'eksctl_Windows_amd64.zip' -split ' ')[0]</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="sect3">
<h4 id="using-git-bash">3.3.1. Using Git Bash:</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sh" data-lang="sh"># for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
ARCH=amd64
PLATFORM=windows_$ARCH

curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.zip"

# (Optional) Verify checksum
curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check

unzip eksctl_$PLATFORM.zip -d $HOME/bin

rm eksctl_$PLATFORM.zip</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>eksctl</code> executable is placed in <code>$HOME/bin</code>, which is in <code>$PATH</code> from Git Bash.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="docker">3.4. Docker</h3>
<div class="paragraph">
<p>For every release and RC a container image is pushed to ECR repository <code>public.ecr.aws/eksctl/eksctl</code>. Learn more about the usage on <a href="https://gallery.ecr.aws/eksctl/eksctl">ECR Public Gallery - eksctl</a>. For example,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">docker run --rm -it public.ecr.aws/eksctl/eksctl version</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="third-party-installers-not-recommended">3.5. Third-Party Installers (Not Recommended)</h3>
<div class="sect3">
<h4 id="for-macos">3.5.1. For MacOS</h4>
<div class="paragraph">
<p><a href="https://brew.sh">Homebrew</a></p>
</div>
<div class="listingblock">
<div class="content">
<pre>brew tap weaveworks/tap
brew install weaveworks/tap/eksctl</pre>
</div>
</div>
<div class="paragraph">
<p><a href="https://www.macports.org">MacPorts</a></p>
</div>
<div class="listingblock">
<div class="content">
<pre>port install eksctl</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="for-windows-2">3.5.2. For Windows</h4>
<div class="paragraph">
<p><a href="https://chocolatey.org">Chocolatey</a></p>
</div>
<div class="listingblock">
<div class="content">
<pre>choco install eksctl</pre>
</div>
</div>
<div class="paragraph">
<p><a href="https://scoop.sh">Scoop</a></p>
</div>
<div class="listingblock">
<div class="content">
<pre>scoop install eksctl</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="shell-completion">3.6. Shell Completion</h3>
<div class="sect3">
<h4 id="bash">3.6.1. Bash</h4>
<div class="paragraph">
<p>To enable bash completion, run the following, or put it in <code>~/.bashrc</code> or <code>~/.profile</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>. &lt;(eksctl completion bash)</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="zsh">3.6.2. Zsh</h4>
<div class="paragraph">
<p>For zsh completion, please run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>mkdir -p ~/.zsh/completion/
eksctl completion zsh &gt; ~/.zsh/completion/_eksctl</pre>
</div>
</div>
<div class="paragraph">
<p>and put the following in <code>~/.zshrc</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>fpath=($fpath ~/.zsh/completion)</pre>
</div>
</div>
<div class="paragraph">
<p>Note if you&#8217;re not running a distribution like oh-my-zsh you may first have to enable autocompletion (and put in <code>~/.zshrc</code> to make it persistent):</p>
</div>
<div class="listingblock">
<div class="content">
<pre>autoload -U compinit
compinit</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="fish">3.6.3. Fish</h4>
<div class="paragraph">
<p>The below commands can be used for fish auto completion:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>mkdir -p ~/.config/fish/completions
eksctl completion fish &gt; ~/.config/fish/completions/eksctl.fish</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="powershell">3.6.4. Powershell</h4>
<div class="paragraph">
<p>The below command can be referred for setting it up. Please note that the path might be different depending on your
system settings.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl completion powershell &gt; C:\Users\Documents\WindowsPowerShell\Scripts\eksctl.ps1</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="features">3.7. Features</h3>
<div class="paragraph">
<p>The features that are currently implemented are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Create, get, list and delete clusters</p>
</li>
<li>
<p>Create, drain and delete nodegroups</p>
</li>
<li>
<p>Scale a nodegroup</p>
</li>
<li>
<p>Update a cluster</p>
</li>
<li>
<p>Use custom AMIs</p>
</li>
<li>
<p>Configure VPC Networking</p>
</li>
<li>
<p>Configure access to API endpoints</p>
</li>
<li>
<p>Support for GPU nodegroups</p>
</li>
<li>
<p>Spot instances and mixed instances</p>
</li>
<li>
<p>IAM Management and Add-on Policies</p>
</li>
<li>
<p>List cluster Cloudformation stacks</p>
</li>
<li>
<p>Install coredns</p>
</li>
<li>
<p>Write kubeconfig file for a cluster</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="clusters">4. Clusters</h2>
<div class="sectionbody">
<div class="paragraph">
<p>View information about Clusters in eksctl.</p>
</div>
<div class="sect2 topic">
<h3 id="creating-and-managing-clusters">4.1. Creating and managing clusters</h3>
<div class="sect3">
<h4 id="creating-a-cluster">4.1.1. Creating a cluster</h4>
<div class="paragraph">
<p>Create a simple cluster with the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sh" data-lang="sh">eksctl create cluster</code></pre>
</div>
</div>
<div class="paragraph">
<p>That will create an EKS cluster in your default region (as specified by your AWS CLI configuration) with one managed
nodegroup containing two m5.large nodes.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>eksctl now creates a managed nodegroup by default when a config file isn&#8217;t used. To create a self-managed nodegroup,
pass <code>--managed=false</code> to <code>eksctl create cluster</code> or <code>eksctl create nodegroup</code>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In <code>us-east-1</code> you are likely to get <code>UnsupportedAvailabilityZoneException</code>. If you do, copy the suggested zones and pass <code>--zones</code> flag, e.g. <code>eksctl create cluster --region=us-east-1 --zones=us-east-1a,us-east-1b,us-east-1d</code>. This may occur in other regions, but less likely. You shouldn&#8217;t need to use <code>--zone</code> flag otherwise.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>After the cluster has been created, the appropriate kubernetes configuration will be added to your kubeconfig file.
This is, the file that you have configured in the environment variable <code>KUBECONFIG</code> or <code>~/.kube/config</code> by default.
The path to the kubeconfig file can be overridden using the <code>--kubeconfig</code> flag.</p>
</div>
<div class="paragraph">
<p>Other flags that can change how the kubeconfig file is written:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">flag</th>
<th class="tableblock halign-left valign-top">type</th>
<th class="tableblock halign-left valign-top">use</th>
<th class="tableblock halign-left valign-top">default value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">--kubeconfig</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">string</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">path to write kubeconfig (incompatible with --auto-kubeconfig)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">$KUBECONFIG or ~/.kube/config</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">--set-kubeconfig-context</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">bool</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">if true then current-context will be set in kubeconfig; if a context is already set then it will be overwritten</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">true</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">--auto-kubeconfig</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">bool</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">save kubeconfig file by cluster name</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">true</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">--write-kubeconfig</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">bool</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">toggle writing of kubeconfig</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">true</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="using-config-files">4.1.2. Using Config Files</h4>
<div class="paragraph">
<p>You can create a cluster using a config file instead of flags.</p>
</div>
<div class="paragraph">
<p>First, create <code>cluster.yaml</code> file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: basic-cluster
  region: eu-north-1

nodeGroups:
  - name: ng-1
    instanceType: m5.large
    desiredCapacity: 10
    volumeSize: 80
    ssh:
      allow: true # will use ~/.ssh/id_rsa.pub as the default ssh key
  - name: ng-2
    instanceType: m5.xlarge
    desiredCapacity: 2
    volumeSize: 100
    ssh:
      publicKeyPath: ~/.ssh/ec2_id_rsa.pub</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, run this command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create cluster -f cluster.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>This will create a cluster as described.</p>
</div>
<div class="paragraph">
<p>If you needed to use an existing VPC, you can use a config file like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-in-existing-vpc
  region: eu-north-1

vpc:
  subnets:
    private:
      eu-north-1a: { id: subnet-0ff156e0c4a6d300c }
      eu-north-1b: { id: subnet-0549cdab573695c03 }
      eu-north-1c: { id: subnet-0426fb4a607393184 }

nodeGroups:
  - name: ng-1-workers
    labels: { role: workers }
    instanceType: m5.xlarge
    desiredCapacity: 10
    privateNetworking: true
  - name: ng-2-builders
    labels: { role: builders }
    instanceType: m5.2xlarge
    desiredCapacity: 2
    privateNetworking: true
    iam:
      withAddonPolicies:
        imageBuilder: true</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The cluster name or nodegroup name can contain only alphanumeric characters (case-sensitive) and hyphens. It must start with an alphabetic character and can&#8217;t be longer than 128 characters otherwise you will get a validation error. More information can be found <a href="AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-parameters.html">here</a></p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To delete this cluster, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl delete cluster -f cluster.yaml</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">

</td>
</tr>
</table>
</div>
<div class="literalblock">
<div class="content">
<pre>Without the `--wait` flag, this will only issue a delete operation to the cluster's CloudFormation stack and won't wait for its deletion.

In some cases, AWS resources using the cluster or its VPC may cause cluster deletion to fail. To ensure any deletion errors are propagated in `eksctl delete cluster`, the `--wait` flag must be used.
If your delete fails or you forget the wait flag, you may have to go to the CloudFormation GUI and delete the eks stacks from there.</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When deleting a cluster with nodegroups, in some scenarios, Pod Disruption Budget (PDB) policies can prevent nodes from being removed successfully from nodepools. E.g. a cluster with <code>aws-ebs-csi-driver</code> installed, by default, spins off two pods while having a PDB policy that allows at most one pod to be unavailable at a time. This will make the other pod unevictable during deletion. To successfully delete the cluster, one should use <code>disable-nodegroup-eviction</code> flag. This will bypass checking PDB policies.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="literalblock">
<div class="content">
<pre>```
eksctl delete cluster -f cluster.yaml --disable-nodegroup-eviction
```</pre>
</div>
</div>
<div class="paragraph">
<p>See <a href="https://github.com/eksctl-io/eksctl/tree/master/examples"><code>examples/</code></a> directory for more sample config files.</p>
</div>
</div>
<div class="sect3">
<h4 id="dry-run-2">4.1.3. Dry Run</h4>
<div class="paragraph">
<p>The dry-run feature enables generating a ClusterConfig file that skips cluster creation and outputs a ClusterConfig file that
represents the supplied CLI options and contains the default values set by eksctl.</p>
</div>
<div class="paragraph">
<p>More info can be found on the <a href="#dry-run">Dry Run</a> page.</p>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="eks-auto-mode">4.2. EKS Auto Mode</h3>
<div class="sect3">
<h4 id="introduction">4.2.1. Introduction</h4>
<div class="paragraph">
<p>eksctl supports <a href="eks/latest/userguide/automode.html">EKS Auto Mode</a>, a feature that extends AWS management of Kubernetes clusters beyond the cluster itself,
to allow AWS to also set up and manage the infrastructure that enables the smooth operation of your workloads.
This allows you to delegate key infrastructure decisions and leverage the expertise of AWS for day-to-day operations.
Cluster infrastructure managed by AWS includes many Kubernetes capabilities as core components, as opposed to add-ons,
such as compute autoscaling, pod and service networking, application load balancing, cluster DNS, block storage, and GPU support.</p>
</div>
</div>
<div class="sect3">
<h4 id="creating-an-eks-cluster-with-auto-mode-enabled">4.2.2. Creating an EKS cluster with Auto Mode enabled</h4>
<div class="paragraph">
<p><code>eksctl</code> has added a new <code>autoModeConfig</code> field to enable and configure Auto Mode. The shape of the <code>autoModeConfig</code> field is</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">autoModeConfig:
    # defaults to false
    enabled: boolean
    # optional, defaults to [general-purpose, system].
    # To disable creation of nodePools, set it to the empty array ([]).
    nodePools: []string
    # optional, eksctl creates a new role if this is not supplied
    # and nodePools are present.
    nodeRoleARN: string</code></pre>
</div>
</div>
<div class="paragraph">
<p>If <code>autoModeConfig.enabled</code> is true, eksctl creates an EKS cluster by passing <code>computeConfig.enabled: true</code>,
<code>kubernetesNetworkConfig.elasticLoadBalancing.enabled: true</code>, and <code>storageConfig.blockStorage.enabled: true</code> to the EKS API,
enabling management of data plane components like compute, storage and networking.</p>
</div>
<div class="paragraph">
<p>To create an EKS cluster with Auto Mode enabled, set <code>autoModeConfig.enabled: true</code>, as in</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># auto-mode-cluster.yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
    name: auto-mode-cluster
    region: us-west-2

autoModeConfig:
    enabled: true</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create cluster -f auto-mode-cluster.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>eksctl creates a node role to use for nodes launched by Auto Mode. eksctl also creates the <code>general-purpose</code> and <code>system</code> node pools.
To disable creation of the default node pools, e.g., to configure your own node pools that use a different set of subnets, set <code>nodePools: []</code>, as in</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
    name: auto-mode-cluster
    region: us-west-2

autoModeConfig:
    enabled: true
    nodePools: [] # disables creation of default node pools.</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="updating-an-eks-cluster-to-use-auto-mode">4.2.3. Updating an EKS cluster to use Auto Mode</h4>
<div class="paragraph">
<p>To update an existing EKS cluster to use Auto Mode, run</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># cluster.yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
    name: cluster
    region: us-west-2

autoModeConfig:
    enabled: true</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl update auto-mode-config -f cluster.yaml</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If the cluster was created by eksctl, and it uses public subnets as cluster subnets, Auto Mode will launch nodes in public subnets.
To use private subnets for worker nodes launched by Auto Mode, <a href="https://eksctl.io/usage/cluster-subnets-security-groups/">update the cluster to use private subnets</a>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="disabling-auto-mode">4.2.4. Disabling Auto Mode</h4>
<div class="paragraph">
<p>To disable Auto Mode, set <code>autoModeConfig.enabled: false</code> and run</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># cluster.yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
    name: auto-mode-cluster
    region: us-west-2

autoModeConfig:
    enabled: false</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl update auto-mode-config -f cluster.yaml</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="further-information">4.2.5. Further information</h4>
<div class="ulist">
<ul>
<li>
<p><a href="eks/latest/userguide/automode.html">EKS Auto Mode</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="eks-access-entries">4.3. EKS Access Entries</h3>
<div class="sect3">
<h4 id="introduction-2">4.3.1. Introduction</h4>
<div class="paragraph">
<p>AWS EKS has introduced a new set of controls, called access entries, for managing access of IAM principals to Kubernetes clusters. <code>eksctl</code> has fully integrated with this feature, allowing users to directly associate access policies to certain IAM principals, while doing work behind the scenes for others. More details in the <a href="#access-entry-resources">upcoming section</a>.</p>
</div>
<div class="paragraph">
<p>EKS predefines several managed access policies that mirror the default Kubernetes user facing roles. Predefined access policies can also include policies with permissions required by other AWS services such as Amazon EMR to run workloads on EKS clusters. See a list of predefined access policies as-well as a detailed description for each of those <a href="eks/latest/userguide/access-policies.html#access-policy-permissions">here</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For now, users can only use predefined EKS access policies. For more advanced requirements, one can continue to use <code>iamIdentityMappings</code>.
Bear in mind that the permissions associated with a predefined access policy are subject to change over time. EKS will periodically backfill policies to match upstream permissions.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="how-to-enable-the-access-entries-api">4.3.2. How to enable the access entries API?</h4>
<div class="paragraph">
<p><code>eksctl</code> has added a new <code>accessConfig.authenticationMode</code> field, which dictates how cluster access management is achieved, and can be set to one of the following three values:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>CONFIG_MAP</code> - default in EKS API - only <code>aws-auth</code> ConfigMap will be used</p>
</li>
<li>
<p><code>API</code> - only access entries API will be used</p>
</li>
<li>
<p><code>API_AND_CONFIG_MAP</code> - default in <code>eksctl</code> - both <code>aws-auth</code> ConfigMap and access entries API can be used</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">accessConfig:
  authenticationMode: &lt;&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>When creating a new cluster with access entries, using <code>eksctl</code>, if <code>authenticationMode</code> is not provided by the user, it is automatically set to <code>API_AND_CONFIG_MAP</code>. Thus, the access entries API will be enabled by default. If instead you want to use access entries on an already existing, non-eksctl created, cluster, where <code>CONFIG_MAP</code> option is used, the user will need to first set <code>authenticationMode</code> to <code>API_AND_CONFIG_MAP</code>. For that, <code>eksctl</code> has introduced a new command for updating the cluster authentication mode, which works both with CLI flags e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl utils update-authentication-mode --cluster my-cluster --authentication-mode API_AND_CONFIG_MAP</code></pre>
</div>
</div>
<div class="paragraph">
<p>and by providing a config file e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl utils update-authentication-mode -f config.yaml</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="access-entry-resources">4.3.3. How does this affect different resources?</h4>
<div class="sect4">
<h5 id="iam-entities">IAM Entities</h5>
<div class="paragraph">
<p>Cluster management access for these type of resources falls under user&#8217;s control. <code>eksctl</code> has added a new <code>accessConfig.accessEntries</code> field that maps one-to-one to the <a href="eks/latest/userguide/access-policies.html#access-policy-permissions">Access Entries EKS API</a>. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">accessConfig:
  authenticationMode: API_AND_CONFIG_MAP
  accessEntries:
    - principalARN: arn:aws:iam::111122223333:user/my-user-name
      type: STANDARD
      kubernetesGroups: # optional Kubernetes groups
        - group1 # groups can used to give permissions via RBAC
        - group2

    - principalARN: arn:aws:iam::111122223333:role/role-name-1
      accessPolicies: # optional access polices
        - policyARN: arn:aws:eks::aws:cluster-access-policy/AmazonEKSViewPolicy
          accessScope:
            type: namespace
            namespaces:
              - default
              - my-namespace
              - dev-*

    - principalARN: arn:aws:iam::111122223333:role/admin-role
      accessPolicies: # optional access polices
        - policyARN: arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy
          accessScope:
            type: cluster

    - principalARN: arn:aws:iam::111122223333:role/role-name-2
      type: EC2_LINUX</code></pre>
</div>
</div>
<div class="paragraph">
<p>In addition to associating EKS policies, one can also specify the Kubernetes groups to which an IAM entity belongs, thus granting permissions via RBAC.</p>
</div>
</div>
<div class="sect4">
<h5 id="managed-nodegroups-and-fargate">Managed nodegroups and Fargate</h5>
<div class="paragraph">
<p>The integration with access entries for these resources will be achieved behind the scenes, by the EKS API. Newly created managed node groups and Fargate pods will create API access entries, rather than using pre-loaded RBAC resources. Existing node groups and Fargate pods will not be changed, and continue to rely on the entries in the aws-auth config map.</p>
</div>
</div>
<div class="sect4">
<h5 id="self-managed-nodegroups">Self-managed nodegroups</h5>
<div class="paragraph">
<p>Each access entry has a type. For authorizing self-managed nodegroups, <code>eksctl</code> will create a unique access entry for each nodegroup with the principal ARN set to the node role ARN and type set to either <code>EC2_LINUX</code> or <code>EC2_WINDOWS</code> depending on nodegroup amiFamily.</p>
</div>
<div class="paragraph">
<p>When creating your own access entries, you can also specify <code>EC2_LINUX</code> (for an IAM role used with Linux or Bottlerocket self-managed nodes), <code>EC2_WINDOWS</code> (for an IAM roles used with Windows self-managed nodes), <code>FARGATE_LINUX</code> (for an IAM roles used with AWS Fargate (Fargate)), or <code>STANDARD</code> as a type. If you don&#8217;t specify a type, the default type is set to <code>STANDARD</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When deleting a nodegroup created with a pre-existing <code>instanceRoleARN</code>, it is the user&#8217;s responsibility to delete the corresponding access entry when no more nodegroups are associated with it. This is because eksctl does not attempt to find out if an access entry is still in use by non-eksctl created self-managed nodegroups as it is a complicated process.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="managing-access-entries">4.3.4. Managing access entries</h4>
<div class="sect4">
<h5 id="create-access-entries">Create access entries</h5>
<div class="paragraph">
<p>This can be done in two different ways, either during cluster creation, specifying the desired access entries as part of the config file and running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl create cluster -f config.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>OR post cluster creation, by running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl create accessentry -f config.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>An example config file for creating access entries can be found <a href="https://github.com/weaveworks/eksctl/blob/main/examples/40-access-entries.yaml">here</a>.</p>
</div>
</div>
<div class="sect4">
<h5 id="fetch-access-entries">Fetch access entries</h5>
<div class="paragraph">
<p>The user can retieve all access entries associated with a certain cluster by running one of the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl get accessentry -f config.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>OR</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl get accessentry --cluster my-cluster</code></pre>
</div>
</div>
<div class="paragraph">
<p>Alternatively, to retrieve only the access entry corresponding to a certain IAM entity one shall use the <code>--principal-arn</code> flag. e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl get accessentry --cluster my-cluster --principal-arn arn:aws:iam::111122223333:user/admin</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="delete-access-entries">Delete access entries</h5>
<div class="paragraph">
<p>To delete a single access entry at a time use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl delete accessentry --cluster my-cluster --principal-arn arn:aws:iam::111122223333:user/admin</code></pre>
</div>
</div>
<div class="paragraph">
<p>To delete multiple access entries, use the <code>--config-file</code> flag and specify all the <code>principalARN&#8217;s</code> corresponding with the access entries, under the top-level <code>accessEntry</code> field, e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">...
accessEntry:
  - principalARN: arn:aws:iam::111122223333:user/my-user-name
  - principalARN: arn:aws:iam::111122223333:role/role-name-1
  - principalARN: arn:aws:iam::111122223333:role/admin-role</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl delete accessentry -f config.yaml</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="migrate-iam-identity-mappings-to-access-entries">Migrate IAM identity mappings to access entries</h5>
<div class="paragraph">
<p>The user can migrate their existing IAM identities from <code>aws-auth</code> configmap to access entries by running the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl utils migrate-to-access-entry --cluster my-cluster --target-authentication-mode &lt;API or API_AND_CONFIG_MAP&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>When <code>--target-authentication-mode</code> flag is set to <code>API</code>, authentication mode is switched to <code>API</code> mode (skipped if already in <code>API</code> mode), IAM identity mappings will be migrated to access entries, and <code>aws-auth</code> configmap is deleted from the cluster.</p>
</div>
<div class="paragraph">
<p>When <code>--target-authentication-mode</code> flag is set to <code>API_AND_CONFIG_MAP</code>, authentication mode is switched to <code>API_AND_CONFIG_MAP</code> mode (skipped if already in <code>API_AND_CONFIG_MAP</code> mode), IAM identity mappings will be migrated to access entries, but <code>aws-auth</code> configmap is preserved.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When <code>--target-authentication-mode</code> flag is set to <code>API</code>, this command will not update authentication mode to <code>API</code> mode if <code>aws-auth</code> configmap has one of the below constraints.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="literalblock">
<div class="content">
<pre>* There is an Account level identity mapping.
* One or more Roles/Users are mapped to the kubernetes group(s) which begin with prefix `system:` (except for EKS specific groups i.e. `system:masters`, `system:bootstrappers`, `system:nodes` etc).
* One or more IAM identity mapping(s) are for a [Service Linked Role](link:IAM/latest/UserGuide/using-service-linked-roles.html).</pre>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="disabling-cluster-creator-admin-permissions">4.3.5. Disabling cluster creator admin permissions</h4>
<div class="paragraph">
<p><code>eksctl</code> has added a new field <code>accessConfig.bootstrapClusterCreatorAdminPermissions: boolean</code> that, when set to false, disables granting cluster-admin permissions to the IAM identity creating the cluster. i.e.</p>
</div>
<div class="paragraph">
<p>add the option to the config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>accessConfig:
  bootstrapClusterCreatorAdminPermissions: false</pre>
</div>
</div>
<div class="paragraph">
<p>and run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl create cluster -f config.yaml</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="non-eksctl-created-clusters">4.4. Non eksctl-created clusters</h3>
<div class="paragraph">
<p>From <code>eksctl</code> version <code>0.40.0</code> users can run <code>eksctl</code> commands against clusters which were
not created by <code>eksctl</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Eksctl can only support unowned clusters with names which comply with the guidelines mentioned <a href="AWSCloudFormation/latest/UserGuide/cfn-using-console-create-stack-parameters.html">here</a>. Any cluster names which do not match this will fail CloudFormation API validation check.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="supported-commands">4.4.1. Supported commands</h4>
<div class="paragraph">
<p>The following commands can be used against clusters created by any means other than <code>eksctl</code>.
The commands, flags and config file options can be used in exactly the same way.</p>
</div>
<div class="paragraph">
<p>If we have missed some functionality, please <a href="https://github.com/eksctl-io/eksctl/issues">let us know</a>.</p>
</div>
<div class="ulist checklist">
<ul class="checklist">
<li>
<p><i class="fa fa-check-square-o"></i> Create:</p>
<div class="ulist checklist">
<ul class="checklist">
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl create nodegroup</code> (<a href="#create-nodegroup">see note below</a>)</p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl create fargateprofile</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl create iamserviceaccount</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl create iamidentitymapping</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> Get:</p>
<div class="ulist checklist">
<ul class="checklist">
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl get clusters/cluster</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl get fargateprofile</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl get nodegroup</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl get labels</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> Delete:</p>
<div class="ulist checklist">
<ul class="checklist">
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl delete cluster</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl delete nodegroup</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl delete fargateprofile</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl delete iamserviceaccount</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl delete iamidentitymapping</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> Upgrade:</p>
<div class="ulist checklist">
<ul class="checklist">
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl upgrade cluster</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl upgrade nodegroup</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> Set/Unset:</p>
<div class="ulist checklist">
<ul class="checklist">
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl set labels</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl unset labels</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> Scale:</p>
<div class="ulist checklist">
<ul class="checklist">
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl scale nodegroup</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> Drain:</p>
<div class="ulist checklist">
<ul class="checklist">
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl drain nodegroup</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> Enable:</p>
<div class="ulist checklist">
<ul class="checklist">
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl enable profile</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl enable repo</code></p>
</li>
</ul>
</div>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> Utils:</p>
<div class="ulist checklist">
<ul class="checklist">
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl utils associate-iam-oidc-provider</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl utils describe-stacks</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl utils install-vpc-controllers</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl utils nodegroup-health</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl utils set-public-access-cidrs</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl utils update-cluster-endpoints</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl utils update-cluster-logging</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl utils write-kubeconfig</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl utils update-coredns</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl utils update-aws-node</code></p>
</li>
<li>
<p><i class="fa fa-check-square-o"></i> <code>eksctl utils update-kube-proxy</code></p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="create-nodegroup">4.4.2. Creating nodegroups</h4>
<div class="paragraph">
<p><code>eksctl create nodegroup</code> is the only command which requires specific input from the user.</p>
</div>
<div class="paragraph">
<p>Since users can create their clusters with any networking configuration they like,
for the time-being, <code>eksctl</code> will not attempt to retrieve or guess these values. This
may change in the future as we learn more about how people are using this command on non eksctl-created clusters.</p>
</div>
<div class="paragraph">
<p>This means that in order to create nodegroups or managed nodegroups on a cluster which was
not created by <code>eksctl</code>, a config file containing VPC details must be provided. At a minimum:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: non-eksctl-created-cluster
  region: us-west-2

vpc:
  id: "vpc-12345"
  securityGroup: "sg-12345"    # this is the ControlPlaneSecurityGroup
  subnets:
    private:
      private1:
          id: "subnet-12345"
      private2:
          id: "subnet-67890"
    public:
      public1:
          id: "subnet-12345"
      public2:
          id: "subnet-67890"

...</code></pre>
</div>
</div>
<div class="paragraph">
<p>Further information on VPC configuration options can be found <a href="#networking">here</a>.</p>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="registering-non-eks-clusters-with-eks-connector">4.5. Registering non-EKS clusters with EKS Connector</h3>
<div class="paragraph">
<p>The EKS Console provides a single pane of glass to manage all your Kubernetes clusters, including those hosted on
other cloud providers, via <a href="eks/latest/userguide/eks-connector.html">EKS Connector</a>. This process requires registering the cluster with EKS and running the
EKS Connector agent on the external Kubernetes cluster.</p>
</div>
<div class="paragraph">
<p><code>eksctl</code> simplifies registering non-EKS clusters by creating the required AWS resources and generating Kubernetes manifests
for EKS Connector to apply to the external cluster.</p>
</div>
<div class="sect3">
<h4 id="register-cluster">4.5.1. Register Cluster</h4>
<div class="paragraph">
<p>To register or connect a non-EKS Kubernetes cluster, run</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl register cluster --name &lt;name&gt; --provider &lt;provider&gt;
2021-08-19 13:47:26 [ℹ]  creating IAM role "eksctl-20210819194112186040"
2021-08-19 13:47:26 [ℹ]  registered cluster "&lt;name&gt;" successfully
2021-08-19 13:47:26 [ℹ]  wrote file eks-connector.yaml to &lt;current directory&gt;
2021-08-19 13:47:26 [ℹ]  wrote file eks-connector-clusterrole.yaml to &lt;current directory&gt;
2021-08-19 13:47:26 [ℹ]  wrote file eks-connector-console-dashboard-full-access-group.yaml to &lt;current directory&gt;
2021-08-19 13:47:26 [!]  note: "eks-connector-clusterrole.yaml" and "eks-connector-console-dashboard-full-access-group.yaml" give full EKS Console access to IAM identity "&lt;aws-arn&gt;", edit if required; read https://eksctl.io/usage/eks-connector for more info
2021-08-19 13:47:26 [ℹ]  run `kubectl apply -f eks-connector.yaml,eks-connector-clusterrole.yaml,eks-connector-console-dashboard-full-access-group.yaml` before &lt;expiry&gt; to connect the cluster</code></pre>
</div>
</div>
<div class="paragraph">
<p>This command will register the cluster and write three files that contain the Kubernetes manifests for EKS Connector
that must be applied to the external cluster before the registration expires.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><code>eks-connector-clusterrole.yaml</code> and <code>eks-connector-console-dashboard-full-access-clusterrole.yaml</code> give <code>get</code> and <code>list</code> permissions for Kubernetes resources
in all namespaces to the calling IAM identity and must be edited accordingly if required before applying them to the cluster.
To configure more restricted access, see <a href="eks/latest/userguide/connector-grant-access.html">Granting access to a user to view a cluster</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To provide an existing IAM role to use for EKS Connector, pass it via <code>--role-arn</code> as in:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl register cluster --name &lt;name&gt; --provider &lt;provider&gt; --role-arn=&lt;role-arn&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>If the cluster already exists, eksctl will return an error.</p>
</div>
</div>
<div class="sect3">
<h4 id="deregister-cluster">4.5.2. Deregister cluster</h4>
<div class="paragraph">
<p>To deregister or disconnect a registered cluster, run</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl deregister cluster --name &lt;name&gt;
2021-08-19 16:04:09 [ℹ]  unregistered cluster "&lt;name&gt;" successfully
2021-08-19 16:04:09 [ℹ]  run `kubectl delete namespace eks-connector` and `kubectl delete -f eks-connector-binding.yaml` on your cluster to remove EKS Connector resources</code></pre>
</div>
</div>
<div class="paragraph">
<p>This command will deregister the external cluster and remove its associated AWS resources, but you are required to remove the
EKS connector Kubernetes resources from the cluster.</p>
</div>
</div>
<div class="sect3">
<h4 id="further-information-2">4.5.3. Further information</h4>
<div class="ulist">
<ul>
<li>
<p><a href="eks/latest/userguide/eks-connector.html">EKS Connector</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="customizing-the-kubelet">4.6. Customizing kubelet configuration</h3>
<div class="sect3">
<h4 id="customizing-kubelet-configuration">4.6.1. Customizing kubelet configuration</h4>
<div class="paragraph">
<p>System resources can be reserved through the configuration of the kubelet. This is recommended, because in the case
of resource starvation the kubelet might not be able to evict pods and eventually make the node become <code>NotReady</code>. To
 do this, config files can include the <code>kubeletExtraConfig</code> field which accepts a free form yaml that will be embedded
 into the <code>kubelet.yaml</code>.</p>
</div>
<div class="paragraph">
<p>Some fields in the <code>kubelet.yaml</code> are set by eksctl and therefore are not overwritable, such as the <code>address</code>,
<code>clusterDomain</code>, <code>authentication</code>, <code>authorization</code>, or <code>serverTLSBootstrap</code>.</p>
</div>
<div class="paragraph">
<p>The following example config file creates a nodegroup that reserves <code>300m</code> vCPU, <code>300Mi</code> of memory and <code>1Gi</code> of
ephemeral-storage for the kubelet; <code>300m</code> vCPU, <code>300Mi</code> of memory and <code>1Gi</code>of ephemeral storage for OS system
daemons; and kicks in eviction of pods when there is less than <code>200Mi</code> of memory available or less than  10% of the
root filesystem.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: dev-cluster-1
  region: eu-north-1

nodeGroups:
  - name: ng-1
    instanceType: m5a.xlarge
    desiredCapacity: 1
    kubeletExtraConfig:
        kubeReserved:
            cpu: "300m"
            memory: "300Mi"
            ephemeral-storage: "1Gi"
        kubeReservedCgroup: "/kube-reserved"
        systemReserved:
            cpu: "300m"
            memory: "300Mi"
            ephemeral-storage: "1Gi"
        evictionHard:
            memory.available:  "200Mi"
            nodefs.available: "10%"
        featureGates:
            RotateKubeletServerCertificate: true # has to be enabled, otherwise it will be disabled</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this example, given instances of type <code>m5a.xlarge</code> which have 4 vCPUs and 16GiB of memory, the <code>Allocatable</code> amount
of CPUs would be 3.4 and 15.4 GiB of memory. It is important to know that the values specified in the config file for
the the fields in <code>kubeletExtraconfig</code> will completely overwrite the default values specified by eksctl.
However, omitting one or more <code>kubeReserved</code> parameters will cause the missing parameters to be defaulted to sane
values based on the aws instance type being used.</p>
</div>
<div class="sect4">
<h5 id="a-note-on-the-kubereserved-calculation-for-nodegroups-with-mixed-instances">A note on the <code>kubeReserved</code> calculation for NodeGroups with mixed instances</h5>
<div class="paragraph">
<p>While it is generally recommended to configure a mixed instance NodeGroup to use instances with the same CPU and RAM
configuration; that&#8217;s not a strict requirement. Therefore the <code>kubeReserved</code> calculation uses the <em>smallest instance</em> in
the <code>InstanceDistribution.InstanceTypes</code> field. This way NodeGroups with disparate instance types will not reserve too
many resources on the smallest instance. However, this could lead to a reservation that is too small for the largest
instance type.</p>
</div>
<div class="paragraph">
<p>!!! warning
    By default <code>eksctl</code> sets <code>featureGates.RotateKubeletServerCertificate=true</code>, but when custom <code>featureGates</code> are
    provided, it will be unset. You should always include <code>featureGates.RotateKubeletServerCertificate=true</code>, unless
    you have to disable it.</p>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="cloudwatch-logging">4.7. CloudWatch logging</h3>
<div class="sect3">
<h4 id="enabling-cloudwatch-logging">4.7.1. Enabling CloudWatch logging</h4>
<div class="paragraph">
<p><a href="eks/latest/userguide/control-plane-logs.html">CloudWatch logging</a> for EKS control plane is not enabled by default due to data
ingestion and storage costs.</p>
</div>
<div class="paragraph">
<p>To enable control plane logging when cluster is created, you will need to define <strong><code>cloudWatch.clusterLogging.enableTypes</code></strong> setting in your <code>ClusterConfig</code> (see below for examples).</p>
</div>
<div class="paragraph">
<p>So if you have a config file with correct <strong><code>cloudWatch.clusterLogging.enableTypes</code></strong>
setting, you can create a cluster with <code>eksctl create cluster --config-file=&lt;path&gt;</code>.</p>
</div>
<div class="paragraph">
<p>If you have created a cluster already, you can use <code>eksctl utils update-cluster-logging</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>this command runs in plan mode by default, you will need to specify <code>--approve</code> flag to
apply the changes to your cluster.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>If you are using a config file, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils update-cluster-logging --config-file=&lt;path&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Alternatively, you can use CLI flags.</p>
</div>
<div class="paragraph">
<p>To enable all types of logs, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils update-cluster-logging --enable-types all</pre>
</div>
</div>
<div class="paragraph">
<p>To enable <code>audit</code> logs, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils update-cluster-logging --enable-types audit</pre>
</div>
</div>
<div class="paragraph">
<p>To enable all but <code>controllerManager</code> logs, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils update-cluster-logging --enable-types=all --disable-types=controllerManager</pre>
</div>
</div>
<div class="paragraph">
<p>If the <code>api</code> and <code>scheduler</code> log types were already enabled, to disable <code>scheduler</code> and enable <code>controllerManager</code> at
the same time, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils update-cluster-logging --enable-types=controllerManager --disable-types=scheduler</pre>
</div>
</div>
<div class="paragraph">
<p>This will leave <code>api</code> and <code>controllerManager</code> as the only log types enabled.</p>
</div>
<div class="paragraph">
<p>To disable all types of logs, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils update-cluster-logging --disable-types all</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="clusterconfig-examples">4.7.2. <code>ClusterConfig</code> Examples</h4>
<div class="paragraph">
<p>In an EKS cluster, the <code>enableTypes</code> field under <code>clusterLogging</code> can take a list of possible values to enable the different types of logs for the control plane components.</p>
</div>
<div class="paragraph">
<p>The following are the possible values:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>api</code>: Enables the Kubernetes API server logs.</p>
</li>
<li>
<p><code>audit</code>: Enables the Kubernetes audit logs.</p>
</li>
<li>
<p><code>authenticator</code>: Enables the authenticator logs.</p>
</li>
<li>
<p><code>controllerManager</code>: Enables the Kubernetes controller manager logs.</p>
</li>
<li>
<p><code>scheduler</code>: Enables the Kubernetes scheduler logs.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To learn more, see <a href="eks/latest/userguide/control-plane-logs.html">EKS documentation</a>.</p>
</div>
<div class="sect4">
<h5 id="disable-all-logs">Disable all logs</h5>
<div class="paragraph">
<p>To disable all types, use <code>[]</code> or remove the <code>cloudWatch</code> section completely.</p>
</div>
</div>
<div class="sect4">
<h5 id="enable-all-logs">Enable all logs</h5>
<div class="paragraph">
<p>You can enable all types with <code>"*"</code> or <code>"all"</code>. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">cloudWatch:
  clusterLogging:
    enableTypes: ["*"]</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="enable-one-or-more-logs">Enable one or more logs</h5>
<div class="paragraph">
<p>You can enable a subset of types by listing the types you want to enable. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">cloudWatch:
  clusterLogging:
    enableTypes:
      - "audit"
      - "authenticator"</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="log-retention-period">Log retention period</h5>
<div class="paragraph">
<p>By default, logs are stored in CloudWatch Logs, indefinitely. You can specify the number of days for which the control plane logs should be retained in CloudWatch Logs. The following example retains logs for 7 days:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">cloudWatch:
  clusterLogging:
    logRetentionInDays: 7</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="complete-example">Complete example</h5>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-11
  region: eu-west-2

nodeGroups:
  - name: ng-1
    instanceType: m5.large
    desiredCapacity: 1

cloudWatch:
  clusterLogging:
    enableTypes: ["audit", "authenticator"]
    logRetentionInDays: 7</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="eks-private-cluster">4.8. EKS Fully-Private Cluster</h3>
<div class="paragraph">
<p>eksctl supports creation of fully-private clusters that have no outbound internet access and have only private subnets.
VPC endpoints are used to enable private access to AWS services.</p>
</div>
<div class="paragraph">
<p>This guide describes how to create a private cluster without outbound internet access.</p>
</div>
<div class="sect3">
<h4 id="creating-a-fully-private-cluster">4.8.1. Creating a fully-private cluster</h4>
<div class="paragraph">
<p>The only required field to create a fully-private cluster is <code>privateCluster.enabled</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">privateCluster:
  enabled: true</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Post cluster creation, eksctl commands that need access to the Kubernetes API server will have to be run from within the cluster&#8217;s VPC, a peered VPC or using some other means like AWS Direct Connect. eksctl commands that need access to the EKS APIs will not work if they&#8217;re being run from within the cluster&#8217;s VPC. To fix this, <a href="eks/latest/userguide/vpc-interface-endpoints.html">create an interface endpoint for Amazon EKS</a> to privately access the Amazon Elastic Kubernetes Service (Amazon EKS) management APIs from your Amazon Virtual Private Cloud (VPC). In a future release, eksctl will add support to create this endpoint so it does not need to be manually created.
Commands that need access to the OpenID Connect provider URL will need to be run from outside of your cluster&#8217;s VPC once you&#8217;ve enabled AWS PrivateLink for Amazon EKS.
Creating managed nodegroups will continue to work, and creating self-managed nodegroups will work as it needs access to the API server via the EKS <a href="https://aws.amazon.com/about-aws/whats-new/2022/12/amazon-eks-supports-aws-privatelink/">interface endpoint</a> if the command is run from within the cluster&#8217;s VPC, a peered VPC or using some other means like AWS Direct Connect.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>VPC endpoints are charged by the hour and based on usage. More details about pricing can be found at
<a href="https://aws.amazon.com/privatelink/pricing/">AWS PrivateLink pricing</a></p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>!!! warning
    Fully-private clusters are not supported in <code>eu-south-1</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="configuring-private-access-to-additional-aws-services">4.8.2. Configuring private access to additional AWS services</h4>
<div class="paragraph">
<p>To enable worker nodes to access AWS services privately, eksctl creates VPC endpoints for the following services:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Interface endpoints for ECR (both <code>ecr.api</code> and <code>ecr.dkr</code>) to pull container images (AWS CNI plugin etc)</p>
</li>
<li>
<p>A gateway endpoint for S3 to pull the actual image layers</p>
</li>
<li>
<p>An interface endpoint for EC2 required by the <code>aws-cloud-provider</code> integration</p>
</li>
<li>
<p>An interface endpoint for STS to support Fargate and IAM Roles for Services Accounts (IRSA)</p>
</li>
<li>
<p>An interface endpoint for CloudWatch logging (<code>logs</code>) if CloudWatch logging is enabled</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>These VPC endpoints are essential for a functional private cluster, and as such, eksctl does not support configuring or
disabling them. However, a cluster might need private access to other AWS services (e.g., Autoscaling required by the Cluster Autoscaler).
These services can be specified in <code>privateCluster.additionalEndpointServices</code>, which instructs eksctl to create a VPC endpoint
for each of them.</p>
</div>
<div class="paragraph">
<p>For example, to allow private access to Autoscaling and CloudWatch logging:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">privateCluster:
  enabled: true
  additionalEndpointServices:
  # For Cluster Autoscaler
  - "autoscaling"
  # CloudWatch logging
  - "logs"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The endpoints supported in <code>additionalEndpointServices</code> are <code>autoscaling</code>, <code>cloudformation</code> and <code>logs</code>.</p>
</div>
<div class="sect4">
<h5 id="skipping-endpoint-creations">Skipping endpoint creations</h5>
<div class="paragraph">
<p>If a VPC has already been created with the necessary AWS endpoints set up and linked to the subnets described in the EKS documentation,
<code>eksctl</code> can skip creating them by providing the option <code>skipEndpointCreation</code> like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">privateCluster:
  enabled: true
  skipEndpointCreation: true</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
this setting cannot be used together with <code>additionalEndpointServices</code>. It will skip all endpoint creation. Also, this setting is
only recommended if the endpoint \&lt;&#8594; subnet topology is correctly set up. I.e.: subnet ids are correct, <code>vpce</code> routing is set up with prefix addresses,
all the necessary EKS endpoints are created and linked to the provided VPC. <code>eksctl</code> will not alter any of these resources.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="private-nodegroups">4.8.3. Nodegroups</h4>
<div class="paragraph">
<p>Only private nodegroups (both managed and self-managed) are supported in a fully-private cluster because the cluster&#8217;s VPC is created without
any public subnets. The <code>privateNetworking</code> field (<code>nodeGroup[<strong>].privateNetworking</code> and <code>managedNodeGroup[</strong>].privateNetworking</code>) must be
explicitly set. It is an error to leave <code>privateNetworking</code> unset in a fully-private cluster.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
- name: ng1
  instanceType: m5.large
  desiredCapacity: 2
  # privateNetworking must be explicitly set for a fully-private cluster
  # Rather than defaulting this field to `true`,
  # we require users to explicitly set it to make the behaviour
  # explicit and avoid confusion.
  privateNetworking: true

managedNodeGroups:
- name: m1
  instanceType: m5.large
  desiredCapacity: 2
  privateNetworking: true</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="cluster-endpoint-access">4.8.4. Cluster Endpoint Access</h4>
<div class="paragraph">
<p>A fully-private cluster does not support modifying <code>clusterEndpointAccess</code> during cluster creation.
It is an error to set either <code>clusterEndpoints.publicAccess</code> or <code>clusterEndpoints.privateAccess</code>, as a fully-private cluster
can have private access only, and allowing modification of these fields can break the cluster.</p>
</div>
</div>
<div class="sect3">
<h4 id="user-supplied-vpc-and-subnets">4.8.5. User-supplied VPC and subnets</h4>
<div class="paragraph">
<p>eksctl supports creation of fully-private clusters using a pre-existing VPC and subnets. Only private subnets can be
specified and it&#8217;s an error to specify subnets under <code>vpc.subnets.public</code>.</p>
</div>
<div class="paragraph">
<p>eksctl creates VPC endpoints in the supplied VPC and modifies route tables for the supplied subnets. Each subnet should
have an explicit route table associated with it because eksctl does not modify the main route table.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: private-cluster
  region: us-west-2

privateCluster:
  enabled: true
  additionalEndpointServices:
  - "autoscaling"

vpc:
  subnets:
    private:
      us-west-2b:
        id: subnet-0818beec303f8419b
      us-west-2c:
        id: subnet-0d42ef09490805e2a
      us-west-2d:
        id: subnet-0da7418077077c5f9


nodeGroups:
- name: ng1
  instanceType: m5.large
  desiredCapacity: 2
  # privateNetworking must be explicitly set for a fully-private cluster
  # Rather than defaulting this field to true for a fully-private cluster, we require users to explicitly set it
  # to make the behaviour explicit and avoid confusion.
  privateNetworking: true

managedNodeGroups:
- name: m1
  instanceType: m5.large
  desiredCapacity: 2
  privateNetworking: true</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="managing-a-fully-private-cluster">4.8.6. Managing a fully-private cluster</h4>
<div class="paragraph">
<p>For all commands to work post cluster creation, eksctl will need private access to the EKS API server endpoint, and outbound
internet access (for <code>EKS:DescribeCluster</code>). Commands that do not need access to the API server will be supported if eksctl has
outbound internet access.</p>
</div>
</div>
<div class="sect3">
<h4 id="force-delete-a-fully-private-cluster">4.8.7. Force-delete a fully-private cluster</h4>
<div class="paragraph">
<p>Errors are likely to occur when deleting a fully-private cluster through eksctl since eksctl does not automatically have access to all of the cluster&#8217;s resources. <code>--force</code> exists to solve this: it will force delete the cluster and continue when errors occur.</p>
</div>
</div>
<div class="sect3">
<h4 id="limitations">4.8.8. Limitations</h4>
<div class="paragraph">
<p>A limitation of the current implementation is that eksctl initially creates the cluster with both public and private endpoint
access enabled, and disables public endpoint access after all operations have completed.
This is required because eksctl needs access to the Kubernetes API server to allow self-managed nodes to join the cluster and
to support GitOps and Fargate. After these operations have completed, eksctl switches the cluster endpoint access to private-only.
This additional update does mean that creation of a fully-private cluster will take longer than for a standard cluster.
In the future, eksctl may switch to a VPC-enabled Lambda function to perform these API operations.</p>
</div>
</div>
<div class="sect3">
<h4 id="outbound-access-via-http-proxy-servers">4.8.9. Outbound access via HTTP proxy servers</h4>
<div class="paragraph">
<p>eksctl is able to talk to the AWS APIs via a configured HTTP(S) proxy server,
however you will need to ensure you set your proxy exclusion list correctly.</p>
</div>
<div class="paragraph">
<p>Generally, you will need to ensure that requests for the VPC endpoint for your
cluster are not routed via your proxies by setting an appropriate <code>no_proxy</code>
environment variable including the value <code>.eks.amazonaws.com</code>.</p>
</div>
<div class="paragraph">
<p>If your proxy server performs "SSL interception" and you are using IAM Roles
for Service Accounts (IRSA), you will need to ensure that you explicitly bypass
SSL Man-in-the-Middle for the domain <code>oidc.&lt;region&gt;.amazonaws.com</code>. Failure to
do so will result in eksctl obtaining the incorrect root certificate thumbprint
for the OIDC provider, and the AWS VPC CNI plugin will fail to start due to
being unable to obtain IAM credentials, rendering your cluster inoperative.</p>
</div>
</div>
<div class="sect3">
<h4 id="further-information-3">4.8.10. Further information</h4>
<div class="ulist">
<ul>
<li>
<p><a href="eks/latest/userguide/private-clusters.html">EKS Private Clusters</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="addons">4.9. Addons</h3>
<div class="paragraph">
<p>EKS Add-Ons is a new feature that lets you enable and manage Kubernetes operational
software for your AWS EKS clusters. At launch, EKS add-ons supports controlling the launch and version of the AWS VPC
CNI plugin through the EKS API</p>
</div>
<div class="sect3">
<h4 id="addons-create">4.9.1. Creating addons (and providing IAM permissions via IRSA)</h4>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>New for 2024:</strong></p>
</div>
<div class="paragraph">
<p>eksctl now supports creating clusters without any default networking addons: <a href="#barecluster">Cluster creation flexibility for default networking addons</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>New for 2024:</strong></p>
</div>
<div class="paragraph">
<p>eksctl now installs default addons as EKS addons instead of self-managed addons. Read more about its implications in <a href="#barecluster">Cluster creation flexibility for default networking addons</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>New for 2024:</strong></p>
</div>
<div class="paragraph">
<p>EKS Add-ons now support receiving IAM permissions, required to connect with AWS services outside of cluster, via <a href="#pod-id-support">EKS Pod Identity Associations</a></p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In your config file, you can specify the addons you want and (if required) the role or policies to attach to them:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: example-cluster
  region: us-west-2

iam:
  withOIDC: true

addons:
- name: vpc-cni
  # all below properties are optional
  version: 1.7.5
  tags:
    team: eks
  # you can specify at most one of:
  attachPolicyARNs:
  - arn:aws:iam::account:policy/AmazonEKS_CNI_Policy
  # or
  serviceAccountRoleARN: arn:aws:iam::account:role/AmazonEKSCNIAccess
  # or
  attachPolicy:
    Statement:
    - Effect: Allow
      Action:
      - ec2:AssignPrivateIpAddresses
      - ec2:AttachNetworkInterface
      - ec2:CreateNetworkInterface
      - ec2:DeleteNetworkInterface
      - ec2:DescribeInstances
      - ec2:DescribeTags
      - ec2:DescribeNetworkInterfaces
      - ec2:DescribeInstanceTypes
      - ec2:DetachNetworkInterface
      - ec2:ModifyNetworkInterfaceAttribute
      - ec2:UnassignPrivateIpAddresses
      Resource: '*'</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can specify at most one of <code>attachPolicy</code>, <code>attachPolicyARNs</code> and <code>serviceAccountRoleARN</code>.</p>
</div>
<div class="paragraph">
<p>If none of these are specified, the addon will be created with a role that has all recommended policies attached.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In order to attach policies to addons your cluster must have <code>OIDC</code> enabled. If it&#8217;s not enabled we ignore any policies
attached.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You can then either have these addons created during the cluster creation process:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create cluster -f config.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or create the addons explicitly after cluster creation using the config file or CLI flags:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create addon -f config.yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create addon --name vpc-cni --version 1.7.5 --service-account-role-arn &lt;role-arn&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>During addon creation, if a self-managed version of the addon already exists on the cluster, you can choose how potential <code>configMap</code> conflicts shall be resolved by setting <code>resolveConflicts</code> option via the config file, e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">addons:
- name: vpc-cni
  attachPolicyARNs:
    - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
  resolveConflicts: overwrite</code></pre>
</div>
</div>
<div class="paragraph">
<p>For addon create, the <code>resolveConflicts</code> field supports three distinct values:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>none</code> - EKS doesn&#8217;t change the value. The create might fail.</p>
</li>
<li>
<p><code>overwrite</code> - EKS overwrites any config changes back to EKS default values.</p>
</li>
<li>
<p><code>preserve</code> - EKS doesn&#8217;t change the value. The create might fail. (Similarly to <code>none</code>, but different from <a href="#update-addons"><code>preserve</code> in updating addons</a>).</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="listing-enabled-addons">4.9.2. Listing enabled addons</h4>
<div class="paragraph">
<p>You can see what addons are enabled in your cluster by running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl get addons --cluster &lt;cluster-name&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>or</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl get addons -f config.yaml</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="setting-the-addons-version">4.9.3. Setting the addon&#8217;s version</h4>
<div class="paragraph">
<p>Setting the version of the addon is optional. If the <code>version</code> field is left empty <code>eksctl</code> will resolve the default version for the addon. More information about which version is the default version for specific addons can be found in the AWS documentation about EKS. Note that the default version might not necessarily be the latest version available.</p>
</div>
<div class="paragraph">
<p>The addon version can be set to <code>latest</code>. Alternatively, the version can be set with the EKS build tag specified, such as <code>v1.7.5-eksbuild.1</code> or <code>v1.7.5-eksbuild.2</code>. It can also be set to the release version of the addon, such as <code>v1.7.5</code> or <code>1.7.5</code>, and the <code>eksbuild</code> suffix tag will be discovered and set for you.</p>
</div>
<div class="paragraph">
<p>See the section below on how to discover available addons and their versions.</p>
</div>
</div>
<div class="sect3">
<h4 id="discovering-addons">4.9.4. Discovering addons</h4>
<div class="paragraph">
<p>You can discover what addons are available to install on your cluster by running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils describe-addon-versions --cluster &lt;cluster-name&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will discover your cluster&#8217;s kubernetes version and filter on that. Alternatively if you want to see what
addons are available for a particular kubernetes version you can run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils describe-addon-versions --kubernetes-version &lt;version&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can also discover addons by filtering on their <code>type</code>, <code>owner</code> and/or <code>publisher</code>.
For e.g., to see addons for a particular owner and type you can run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils describe-addon-versions --kubernetes-version 1.22 --types "infra-management, policy-management" --owners "aws-marketplace"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>types</code>, <code>owners</code> and <code>publishers</code> flags are optional and can be specified together or individually to filter the results.</p>
</div>
</div>
<div class="sect3">
<h4 id="discovering-the-configuration-schema-for-addons">4.9.5. Discovering the configuration schema for addons</h4>
<div class="paragraph">
<p>After discovering the addon and version, you can view the customization options by fetching its JSON configuration schema.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils describe-addon-configuration --name vpc-cni --version v1.12.0-eksbuild.1</code></pre>
</div>
</div>
<div class="paragraph">
<p>This returns a JSON schema of the various options available for this addon.</p>
</div>
</div>
<div class="sect3">
<h4 id="working-with-configuration-values">4.9.6. Working with configuration values</h4>
<div class="paragraph">
<p><code>ConfigurationValues</code> can be provided in the configuration file during the creation or update of addons. Only JSON and YAML formats are supported.</p>
</div>
<div class="paragraph">
<p>For eg.,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">addons:
- name: coredns
  configurationValues: |-
    replicaCount: 2</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">addons:
- name: coredns
  version: latest
  configurationValues: "{\"replicaCount\":3}"
  resolveConflicts: overwrite</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Bear in mind that when addon configuration values are being modified, configuration conflicts will arise.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="literalblock">
<div class="content">
<pre>Thus, we need to specify how to deal with those by setting the `resolveConflicts` field accordingly.
As in this scenario we want to modify these values, we'd set `resolveConflicts: overwrite`.</pre>
</div>
</div>
<div class="paragraph">
<p>Additionally, the get command will now also retrieve <code>ConfigurationValues</code> for the addon. e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl get addon --cluster my-cluster --output yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">- ConfigurationValues: '{"replicaCount":3}'
  IAMRole: ""
  Issues: null
  Name: coredns
  NewerVersion: ""
  Status: ACTIVE
  Version: v1.8.7-eksbuild.3</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="update-addons">4.9.7. Updating addons</h4>
<div class="paragraph">
<p>You can update your addons to newer versions and change what policies are attached by running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl update addon -f config.yaml</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl update addon --name vpc-cni --version 1.8.0 --service-account-role-arn &lt;new-role&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Similarly to addon creation, When updating an addon, you have full control over the config changes that you may have previously applied on that add-on&#8217;s <code>configMap</code>. Specifically, you can preserve, or overwrite them. This optional functionality is available via the same config file field <code>resolveConflicts</code>. e.g.,</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">addons:
- name: vpc-cni
  attachPolicyARNs:
    - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
  resolveConflicts: preserve</code></pre>
</div>
</div>
<div class="paragraph">
<p>For addon update, the <code>resolveConflicts</code> field accepts three distinct values:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>none</code> - EKS doesn&#8217;t change the value. The update might fail.</p>
</li>
<li>
<p><code>overwrite</code> - EKS overwrites any config changes back to EKS default values.</p>
</li>
<li>
<p><code>preserve</code> - EKS preserves the value. If you choose this option, we recommend that you test any field and value changes on a non-production cluster before updating the add-on on your production cluster.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="deleting-addons">4.9.8. Deleting addons</h4>
<div class="paragraph">
<p>You can delete an addon by running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl delete addon --cluster &lt;cluster-name&gt; --name &lt;addon-name&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will delete the addon and any IAM roles associated to it.</p>
</div>
<div class="paragraph">
<p>When you delete your cluster all IAM roles associated to addons are also deleted.</p>
</div>
</div>
<div class="sect3">
<h4 id="barecluster">4.9.9. Cluster creation flexibility for default networking addons</h4>
<div class="paragraph">
<p>When a cluster is created, EKS automatically installs VPC CNI, CoreDNS and kube-proxy as self-managed addons.
To disable this behavior in order to use other CNI plugins like Cilium and Calico, eksctl now supports creating a cluster
without any default networking addons. To create such a cluster, set <code>addonsConfig.disableDefaultAddons</code>, as in:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">addonsConfig:
  disableDefaultAddons: true</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create cluster -f cluster.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>To create a cluster with only CoreDNS and kube-proxy and not VPC CNI, specify the addons explicitly in <code>addons</code>
and set <code>addonsConfig.disableDefaultAddons</code>, as in:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">addonsConfig:
  disableDefaultAddons: true
addons:
  - name: kube-proxy
  - name: coredns</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create cluster -f cluster.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>As part of this change, eksctl now installs default addons as EKS addons instead of self-managed addons during cluster creation
if <code>addonsConfig.disableDefaultAddons</code> is not explicitly set to true. As such, <code>eksctl utils update-*</code> commands can no
longer be used for updating addons for clusters created with eksctl v0.184.0 and above:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>eksctl utils update-aws-node</code></p>
</li>
<li>
<p><code>eksctl utils update-coredns</code></p>
</li>
<li>
<p><code>eksctl utils update-kube-proxy</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Instead, <code>eksctl update addon</code> should be used now.</p>
</div>
<div class="paragraph">
<p>To learn more, see <a href="https://aws.amazon.com/about-aws/whats-new/2024/06/amazon-eks-cluster-creation-flexibility-networking-add-ons/">EKS documentation</a>.</p>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="enabling-access-for-amazon-emr">4.10. Enabling Access for Amazon EMR</h3>
<div class="paragraph">
<p>In order to allow <a href="https://aws.amazon.com/emr">EMR</a> to perform operations on the Kubernetes API, its SLR needs to be granted the required RBAC permissions.
eksctl provides a command that creates the required RBAC resources for EMR, and updates the <code>aws-auth</code> ConfigMap to bind
the role with the SLR for EMR.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create iamidentitymapping --cluster dev --service-name emr-containers --namespace default</code></pre>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="cluster-upgrade">4.11. Cluster upgrades</h3>
<div class="paragraph">
<p>An <em>`eksctl`-managed</em> cluster can be upgraded in 3 easy steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>upgrade control plane version with <code>eksctl upgrade cluster</code></p>
</li>
<li>
<p>replace each of the nodegroups by creating a new one and deleting the old one</p>
</li>
<li>
<p>update default add-ons (more about this <a href="https://eksctl.io/usage/addon-upgrade/">here</a>):</p>
<div class="ulist">
<ul>
<li>
<p><code>kube-proxy</code></p>
</li>
<li>
<p><code>aws-node</code></p>
</li>
<li>
<p><code>coredns</code></p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>Please make sure to read this section in full before you proceed.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Kubernetes supports version drift of up to two minor versions during the upgrade
process. Nodes can be up to two minor versions behind, but never ahead of the control plane
version. You can only upgrade the control plane one minor version at a time, but
nodes can be upgraded more than one minor version at a time, provided their version
does not become greater than the control plane version.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The old <code>eksctl update cluster</code> will be deprecated. Use <code>eksctl upgrade cluster</code> instead.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="updating-control-plane-version">4.11.1. Updating control plane version</h4>
<div class="paragraph">
<p>Control plane version upgrades must be done for one minor version at a time.</p>
</div>
<div class="paragraph">
<p>To upgrade control plane to the next available version run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl upgrade cluster --name=&lt;clusterName&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>This command will not apply any changes right away, you will need to re-run it with
<code>--approve</code> to apply the changes.</p>
</div>
<div class="paragraph">
<p>The target version for the cluster upgrade can be specified both with the CLI flag:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl upgrade cluster --name=&lt;clusterName&gt; --version=1.16</pre>
</div>
</div>
<div class="paragraph">
<p>or with the config file</p>
</div>
<div class="listingblock">
<div class="content">
<pre>cat cluster1.yaml
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-1
  region: eu-north-1
  version: "1.16"

eksctl upgrade cluster --config-file cluster1.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>!!! warning
    The only values allowed for the <code>--version</code> and <code>metadata.version</code> arguments are the current version of the cluster
    or one version higher. Upgrades of more than one Kubernetes version are not supported at the moment.</p>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="default-add-on-updates">4.12. Default add-on updates</h3>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>New for 2024:</strong></p>
</div>
<div class="paragraph">
<p>eksctl now installs default addons as EKS addons instead of self-managed addons. Read more about its implications in <a href="#barecluster">Cluster creation flexibility for default networking addons</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>New for 2024:</strong></p>
</div>
<div class="paragraph">
<p>For updating addons, <code>eksctl utils update-*</code> cannot be used for clusters created with eksctl v0.184.0 and above.
This guide is only valid for clusters created before this change.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>There are 3 default add-ons that get included in each EKS cluster:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>kube-proxy</code></p>
</li>
<li>
<p><code>aws-node</code></p>
</li>
<li>
<p><code>coredns</code></p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For official EKS addons that are created manually through <code>eksctl create addons</code> or upon cluster creation, the way to manage them is
through <code>eksctl create/get/update/delete addon</code>. In such cases, please refer to the docs about <a href="https://eksctl.io/usage/addons/">EKS Add-Ons</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The process for updating each of them is different, hence there are 3 distinct commands that you will need to run.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>All of the following commands accept <code>--config-file</code>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>By default each of these commands runs in plan mode, if you are happy with the proposed changes, re-run with <code>--approve</code>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To update <code>kube-proxy</code>, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils update-kube-proxy --cluster=&lt;clusterName&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>To update <code>aws-node</code>, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils update-aws-node --cluster=&lt;clusterName&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>To update <code>coredns</code>, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils update-coredns --cluster=&lt;clusterName&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Once upgraded, be sure to run <code>kubectl get pods -n kube-system</code> and check if all addon pods are in ready state, you should see
something like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>NAME                       READY   STATUS    RESTARTS   AGE
aws-node-g5ghn             1/1     Running   0          2m
aws-node-zfc9s             1/1     Running   0          2m
coredns-7bcbfc4774-g6gg8   1/1     Running   0          1m
coredns-7bcbfc4774-hftng   1/1     Running   0          1m
kube-proxy-djkp7           1/1     Running   0          3m
kube-proxy-mpdsp           1/1     Running   0          3m</pre>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="support-for-zonal-shift-in-eks-clusters">4.13. Support for Zonal Shift in EKS clusters</h3>
<div class="paragraph">
<p>EKS now supports Amazon Application Recovery Controller (ARC) zonal shift and zonal autoshift that enhances the
resiliency of multi-AZ cluster environments. With AWS Zonal Shift, customers can shift in-cluster traffic away
from an impaired availability zone, ensuring new Kubernetes pods and nodes are launched in healthy availability zones only.</p>
</div>
<div class="sect3">
<h4 id="creating-a-cluster-with-zonal-shift-enabled">4.13.1. Creating a cluster with zonal shift enabled</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># zonal-shift-cluster.yaml
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: highly-available-cluster
  region: us-west-2


zonalShiftConfig:
  enabled: true</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create cluster -f zonal-shift-cluster.yaml</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="enabling-zonal-shift-on-an-existing-cluster">4.13.2. Enabling zonal shift on an existing cluster</h4>
<div class="paragraph">
<p>To enable or disable zonal shift on an existing cluster, run</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl utils update-zonal-shift-config -f zonal-shift-cluster.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>or without a config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl utils update-zonal-shift-config --cluster=zonal-shift-cluster --enabled</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="further-information-4">4.13.3. Further information</h4>
<div class="ulist">
<ul>
<li>
<p><a href="eks/latest/userguide/zone-shift.html">EKS Zonal Shift</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="eksctl-karpenter">4.14. Karpenter Support</h3>
<div class="paragraph">
<p><code>eksctl</code> provides adding <a href="https://karpenter.sh/">Karpenter</a> to a newly created cluster. It will create all the necessary
prerequisites outlined in Karpenter&#8217;s <a href="https://karpenter.sh/docs/getting-started/">Getting Started</a> section including installing
Karpenter itself using Helm. We currently support installing versions starting <code>0.20.0</code> and above.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>With <a href="https://karpenter.sh/docs/upgrade-guide/#upgrading-to-v0170">v0.17.0</a> Karpenter&#8217;s Helm chart package is now stored in Karpenter&#8217;s OCI (Open Container Initiative) registry.
Clusters created on previous versions shouldn&#8217;t be affected by this change. If you wish to upgrade your current installation of Karpenter please refer to the <a href="https://karpenter.sh/docs/upgrade-guide/">upgrade guide</a>
You have to be logged out of ECR repositories to be able to pull the OCI artifact by running <code>helm registry logout public.ecr.aws</code> or <code>docker logout public.ecr.aws</code>, failure to do so will result in a 403 error when trying to pull the chart.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To that end, a new configuration value has been introduced into <code>eksctl</code> cluster config called <code>karpenter</code>. The following
yaml outlines a typical installation configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-with-karpenter
  region: us-west-2
  version: '1.24'
  tags:
    karpenter.sh/discovery: cluster-with-karpenter # here, it is set to the cluster name
iam:
  withOIDC: true # required

karpenter:
  version: 'v0.20.0' # Exact version must be specified

managedNodeGroups:
  - name: managed-ng-1
    minSize: 1
    maxSize: 2
    desiredCapacity: 1</code></pre>
</div>
</div>
<div class="paragraph">
<p>The version is Karpenter&#8217;s version as it can be found in their Helm Repository. The following options are also available
to be set:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">karpenter:
  version: 'v0.20.0'
  createServiceAccount: true # default is false
  defaultInstanceProfile: 'KarpenterNodeInstanceProfile' # default is to use the IAM instance profile created by eksctl
  withSpotInterruptionQueue: true # adds all required policies and rules for supporting Spot Interruption Queue, default is false</code></pre>
</div>
</div>
<div class="paragraph">
<p>OIDC must be defined in order to install Karpenter.</p>
</div>
<div class="paragraph">
<p>Once Karpenter is successfully installed, add a <a href="https://karpenter.sh/docs/concepts/provisioners/">Provisioner</a> so Karpenter
can start adding the right nodes to the cluster.</p>
</div>
<div class="paragraph">
<p>The provisioner&#8217;s <code>instanceProfile</code> section must match the created <code>NodeInstanceProfile</code> role&#8217;s name. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: default
spec:
  requirements:
    - key: karpenter.sh/capacity-type
      operator: In
      values: ["on-demand"]
  limits:
    resources:
      cpu: 1000
  provider:
    instanceProfile: eksctl-KarpenterNodeInstanceProfile-${CLUSTER_NAME}
    subnetSelector:
      karpenter.sh/discovery: cluster-with-karpenter # must match the tag set in the config file
    securityGroupSelector:
      karpenter.sh/discovery: cluster-with-karpenter # must match the tag set in the config file
  ttlSecondsAfterEmpty: 30</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that unless <code>defaultInstanceProfile</code> is defined, the name used for <code>instanceProfile</code> is
<code>eksctl-KarpenterNodeInstanceProfile-&lt;cluster-name&gt;</code>.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="nodegroups">5. Nodegroups</h2>
<div class="sectionbody">
<div class="sect2 topic">
<h3 id="unmanaged-nodegroups">5.1. Unmanaged nodegroups</h3>
<div class="paragraph">
<p>In <code>eksctl</code>, setting <code>--managed=false</code> or using the <code>nodeGroups</code> field creates an unmanaged nodegroup. Bear in mind that
unmanaged nodegroups do not appear in the EKS console, which as a general rule only knows about EKS-managed nodegroups.</p>
</div>
<div class="paragraph">
<p>You should be upgrading nodegroups only after you ran <code>eksctl upgrade cluster</code>.
(See <a href="#cluster-upgrade">Upgrading clusters</a>.)</p>
</div>
<div class="paragraph">
<p>If you have a simple cluster with just an initial nodegroup (i.e. created with
<code>eksctl create cluster</code>), the process is very simple:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Get the name of old nodegroup:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell"> eksctl get nodegroups --cluster=&lt;clusterName&gt; --region=&lt;region&gt;</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="literalblock">
<div class="content">
<pre>You should see only one nodegroup here, if you see more - read the next section.</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create a new nodegroup:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell"> eksctl create nodegroup --cluster=&lt;clusterName&gt; --region=&lt;region&gt; --name=&lt;newNodeGroupName&gt; --managed=false</code></pre>
</div>
</div>
</li>
<li>
<p>Delete the old nodegroup:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell"> eksctl delete nodegroup --cluster=&lt;clusterName&gt; --region=&lt;region&gt; --name=&lt;oldNodeGroupName&gt;</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="literalblock">
<div class="content">
<pre>This will drain all pods from that nodegroup before the instances are deleted. In some scenarios, Pod Disruption Budget (PDB) policies can prevent pods to be evicted. To delete the nodegroup regardless of PDB, one should use the `--disable-eviction` flag, will bypass checking PDB policies.</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="sect3">
<h4 id="updating-multiple-nodegroups">5.1.1. Updating multiple nodegroups</h4>
<div class="paragraph">
<p>If you have multiple nodegroups, it&#8217;s your responsibility to track how each one was configured.
You can do this by using config files, but if you haven&#8217;t used it already, you will need to inspect
your cluster to find out how each nodegroup was configured.</p>
</div>
<div class="paragraph">
<p>In general terms, you are looking to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>review which nodegroups you have and which ones can be deleted or must be replaced for the new version</p>
</li>
<li>
<p>note down configuration of each nodegroup, consider using config file to ease upgrades next time</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="updating-with-config-file">Updating with config file</h5>
<div class="paragraph">
<p>If you are using config file, you will need to do the following.</p>
</div>
<div class="paragraph">
<p>Edit config file to add new nodegroups, and remove old nodegroups.
If you just want to upgrade nodegroups and keep the same configuration,
you can just change nodegroup names, e.g. append <code>-v2</code> to the name.</p>
</div>
<div class="paragraph">
<p>To create all of new nodegroups defined in the config file, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create nodegroup --config-file=&lt;path&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Once you have new nodegroups in place, you can delete old ones:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl delete nodegroup --config-file=&lt;path&gt; --only-missing</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>First run is in plan mode, if you are happy with the proposed changes, re-run with <code>--approve</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="updating-default-add-ons">5.1.2. Updating default add-ons</h4>
<div class="paragraph">
<p>There are 3 default add-ons that get included in each EKS cluster, the process for updating each of them is different, hence
there are 3 distinct commands that you will need to run.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>All of the following commands accept <code>--config-file</code>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>By default each of these commands runs in plan mode, if you are happy with the proposed changes, re-run with <code>--approve</code>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To update <code>kube-proxy</code>, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils update-kube-proxy --cluster=&lt;clusterName&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>To update <code>aws-node</code>, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils update-aws-node --cluster=&lt;clusterName&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>To update <code>coredns</code>, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils update-coredns --cluster=&lt;clusterName&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Once upgraded, be sure to run <code>kubectl get pods -n kube-system</code> and check if all addon pods are in ready state, you should see
something like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>NAME                       READY   STATUS    RESTARTS   AGE
aws-node-g5ghn             1/1     Running   0          2m
aws-node-zfc9s             1/1     Running   0          2m
coredns-7bcbfc4774-g6gg8   1/1     Running   0          1m
coredns-7bcbfc4774-hftng   1/1     Running   0          1m
kube-proxy-djkp7           1/1     Running   0          3m
kube-proxy-mpdsp           1/1     Running   0          3m</pre>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="eks-managed-nodegroups">5.2. EKS managed nodegroups</h3>
<div class="paragraph">
<p><a href="eks/latest/userguide/managed-node-groups.html">Amazon EKS managed nodegroups</a> is a feature that automates the provisioning and lifecycle management of nodes (EC2 instances) for Amazon EKS Kubernetes clusters. Customers can provision optimized groups of nodes for their clusters and EKS will keep their nodes up to date with the latest Kubernetes and host OS versions.</p>
</div>
<div class="paragraph">
<p>An EKS managed node group is an autoscaling group and associated EC2 instances that are managed by AWS for an Amazon EKS cluster. Each node group uses the Amazon EKS-optimized Amazon Linux 2 AMI. Amazon EKS makes it easy to apply bug fixes and security patches to nodes, as well as update them to the latest Kubernetes versions. Each node group launches an autoscaling group for your cluster, which can span multiple AWS VPC availability zones and subnets for high-availability.</p>
</div>
<div class="paragraph">
<p><strong>NEW</strong> <a href="#launch-template-support">Launch Template support for managed nodegroups</a></p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The term "unmanaged nodegroups" has been used to refer to nodegroups that eksctl has supported since the beginning (represented via the <code>nodeGroups</code> field). The <code>ClusterConfig</code> file continues to use the <code>nodeGroups</code> field for defining unmanaged nodegroups, and managed nodegroups are defined with the <code>managedNodeGroups</code> field.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="creating-managed-nodegroups">5.2.1. Creating managed nodegroups</h4>
<div class="literalblock">
<div class="content">
<pre>$ eksctl create nodegroup</pre>
</div>
</div>
<div class="sect4">
<h5 id="new-clusters">New clusters</h5>
<div class="paragraph">
<p>To create a new cluster with a managed nodegroup, run</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create cluster</code></pre>
</div>
</div>
<div class="paragraph">
<p>To create multiple managed nodegroups and have more control over the configuration, a config file can be used.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Managed nodegroups do not have complete feature parity with unmanaged nodegroups.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># cluster.yaml
# A cluster with two managed nodegroups
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: managed-cluster
  region: us-west-2

managedNodeGroups:
  - name: managed-ng-1
    minSize: 2
    maxSize: 4
    desiredCapacity: 3
    volumeSize: 20
    ssh:
      allow: true
      publicKeyPath: ~/.ssh/ec2_id_rsa.pub
      # new feature for restricting SSH access to certain AWS security group IDs
      sourceSecurityGroupIds: ["sg-00241fbb12c607007"]
    labels: {role: worker}
    tags:
      nodegroup-role: worker
    iam:
      withAddonPolicies:
        externalDNS: true
        certManager: true

  - name: managed-ng-2
    instanceType: t2.large
    minSize: 2
    maxSize: 3</code></pre>
</div>
</div>
<div class="paragraph">
<p>Another example of a config file for creating a managed nodegroup can be found <a href="https://github.com/eksctl-io/eksctl/blob/main/examples/15-managed-nodes.yaml">here</a>.</p>
</div>
<div class="paragraph">
<p>It&#8217;s possible to have a cluster with both managed and unmanaged nodegroups. Unmanaged nodegroups do not show up in
the AWS EKS console but <code>eksctl get nodegroup</code> will list both types of nodegroups.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># cluster.yaml
# A cluster with an unmanaged nodegroup and two managed nodegroups.
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: managed-cluster
  region: us-west-2

nodeGroups:
  - name: ng-1
    minSize: 2

managedNodeGroups:
  - name: managed-ng-1
    minSize: 2
    maxSize: 4
    desiredCapacity: 3
    volumeSize: 20
    ssh:
      allow: true
      publicKeyPath: ~/.ssh/ec2_id_rsa.pub
      # new feature for restricting SSH access to certain AWS security group IDs
      sourceSecurityGroupIds: ["sg-00241fbb12c607007"]
    labels: {role: worker}
    tags:
      nodegroup-role: worker
    iam:
      withAddonPolicies:
        externalDNS: true
        certManager: true

  - name: managed-ng-2
    instanceType: t2.large
    privateNetworking: true
    minSize: 2
    maxSize: 3</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>NEW</strong> Support for custom AMI, security groups, <code>instancePrefix</code>, <code>instanceName</code>, <code>ebsOptimized</code>, <code>volumeType</code>, <code>volumeName</code>,
<code>volumeEncrypted</code>, <code>volumeKmsKeyID</code>, <code>volumeIOPS</code>, <code>maxPodsPerNode</code>, <code>preBootstrapCommands</code>, <code>overrideBootstrapCommand</code>, and <code>disableIMDSv1</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># cluster.yaml
# A cluster with a managed nodegroup with customization.
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: managed-cluster
  region: us-west-2

managedNodeGroups:
  - name: custom-ng
    ami: ami-0e124de4755b2734d
    securityGroups:
      attachIDs: ["sg-1234"]
    maxPodsPerNode: 80
    ssh:
      allow: true
    volumeSize: 100
    volumeName: /dev/xvda
    volumeEncrypted: true
    # defaults to true, which enforces the use of IMDSv2 tokens
    disableIMDSv1: false
    overrideBootstrapCommand: |
      #!/bin/bash
      /etc/eks/bootstrap.sh managed-cluster --kubelet-extra-args '--node-labels=eks.amazonaws.com/nodegroup=custom-ng,eks.amazonaws.com/nodegroup-image=ami-0e124de4755b2734d'</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you are requesting an instance type that is only available in one zone (and the eksctl config requires
specification of two) make sure to add the availability zone to your node group request:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># cluster.yaml
# A cluster with a managed nodegroup with "availabilityZones"
---

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: flux-cluster
  region: us-east-2
  version: "1.23"

availabilityZones: ["us-east-2b", "us-east-2c"]
managedNodeGroups:
  - name: workers
    instanceType: hpc6a.48xlarge
    minSize: 64
    maxSize: 64
    labels: { "fluxoperator": "true" }
    availabilityZones: ["us-east-2b"]
    efaEnabled: true
    placement:
      groupName: eks-efa-testing</code></pre>
</div>
</div>
<div class="paragraph">
<p>This can be true for instance types like <a href="https://aws.amazon.com/ec2/instance-types/hpc6/">the Hpc6 family</a> that are only available
in one zone.</p>
</div>
</div>
<div class="sect4">
<h5 id="existing-clusters">Existing clusters</h5>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create nodegroup --managed</code></pre>
</div>
</div>
<div class="paragraph">
<p>Tip : if you are using a <code>ClusterConfig</code> file to describe your whole cluster, describe your new managed node group in the <code>managedNodeGroups</code> field and run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create nodegroup --config-file=YOUR_CLUSTER.yaml</code></pre>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="upgrading-managed-nodegroups">5.2.2. Upgrading managed nodegroups</h4>
<div class="paragraph">
<p>You can update a nodegroup to the latest EKS-optimized AMI release version for the AMI type you are using at any time.</p>
</div>
<div class="paragraph">
<p>If your nodegroup is the same Kubernetes version as the cluster, you can update to the latest AMI release version
for that Kubernetes version of the AMI type you are using. If your nodegroup is the previous Kubernetes version from
the cluster&#8217;s Kubernetes version, you can update the nodegroup to the latest AMI release version that matches the
nodegroup&#8217;s Kubernetes version, or update to the latest AMI release version that matches the clusters Kubernetes
version. You cannot roll back a nodegroup to an earlier Kubernetes version.</p>
</div>
<div class="paragraph">
<p>To upgrade a managed nodegroup to the latest AMI release version:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl upgrade nodegroup --name=managed-ng-1 --cluster=managed-cluster</code></pre>
</div>
</div>
<div class="paragraph">
<p>If a nodegroup is on Kubernetes 1.14, and the cluster&#8217;s Kubernetes version is 1.15, the nodegroup can be upgraded to
the latest AMI release for Kubernetes 1.15 using:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl upgrade nodegroup --name=managed-ng-1 --cluster=managed-cluster --kubernetes-version=1.15</code></pre>
</div>
</div>
<div class="paragraph">
<p>To upgrade to a specific AMI release version instead of the latest version, pass <code>--release-version</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl upgrade nodegroup --name=managed-ng-1 --cluster=managed-cluster --release-version=1.19.6-20210310</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If the managed nodes are deployed using custom AMIs, the following workflow must be followed in order to deploy a new version of the custom AMI.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="literalblock">
<div class="content">
<pre>- initial deployment of the nodegroup must be done using a launch template. e.g.
  ```yaml
  managedNodeGroups:
    - name: launch-template-ng
      launchTemplate:
        id: lt-1234
        version: "2" #optional (uses the default version of the launch template if unspecified)
  ```
- create a new version of the custom AMI (using AWS EKS console).
- create a new launch template version with the new AMI ID (using AWS EKS console).
- upgrade the nodes to the new version of the launch template. e.g.
  ```
  eksctl upgrade nodegroup --name nodegroup-name --cluster cluster-name --launch-template-version new-template-version
  ```</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="handling-parallel-upgrades-for-nodes">5.2.3. Handling parallel upgrades for nodes</h4>
<div class="paragraph">
<p>Multiple managed nodes can be upgraded simultaneously. To configure parallel upgrades, define the <code>updateConfig</code> of a nodegroup when creating the nodegroup. An example <code>updateConfig</code> can be found <a href="https://github.com/eksctl-io/eksctl/blob/main/examples/15-managed-nodes.yaml">here</a>.</p>
</div>
<div class="paragraph">
<p>To avoid any downtime to your workloads due to upgrading multiple nodes at once, you can limit the number of nodes that can become unavailable during an upgrade by specifying this in the <code>maxUnavailable</code> field of an <code>updateConfig</code>. Alternatively, use <code>maxUnavailablePercentage</code>, which defines the maximum number of unavailable nodes as a percentage of the total number of nodes.</p>
</div>
<div class="paragraph">
<p>Note that <code>maxUnavailable</code> cannot be higher than <code>maxSize</code>. Also, <code>maxUnavailable</code> and <code>maxUnavailablePercentage</code> cannot be used simultaneously.</p>
</div>
<div class="paragraph">
<p>This feature is only available for managed nodes.</p>
</div>
</div>
<div class="sect3">
<h4 id="updating-managed-nodegroups">5.2.4. Updating managed nodegroups</h4>
<div class="paragraph">
<p><code>eksctl</code> allows updating the <a href="AWSCloudFormation/latest/UserGuide/aws-properties-eks-nodegroup-updateconfig.html">UpdateConfig</a> section of a managed nodegroup.
This section defines two fields. <code>MaxUnavailable</code> and <code>MaxUnavailablePercentage</code>. Your nodegroups are unaffected during
the update, thus downtime shouldn&#8217;t be expected.</p>
</div>
</div>
<div class="sect3">
<h4 id="nodegroup-health-issues">5.2.5. Nodegroup Health issues</h4>
<div class="paragraph">
<p>EKS Managed Nodegroups automatically checks the configuration of your nodegroup and nodes for health issues and reports
them through the EKS API and console.
To view health issues for a nodegroup:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils nodegroup-health --name=managed-ng-1 --cluster=managed-cluster</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="managing-labels">5.2.6. Managing Labels</h4>
<div class="paragraph">
<p>EKS Managed Nodegroups supports attaching labels that are applied to the Kubernetes nodes in the nodegroup. This is
specified via the <code>labels</code> field in eksctl during cluster or nodegroup creation.</p>
</div>
<div class="paragraph">
<p>To set new labels or updating existing labels on a nodegroup:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl set labels --cluster managed-cluster --nodegroup managed-ng-1 --labels kubernetes.io/managed-by=eks,kubernetes.io/role=worker</code></pre>
</div>
</div>
<div class="paragraph">
<p>To unset or remove labels from a nodegroup:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl unset labels --cluster managed-cluster --nodegroup managed-ng-1 --labels kubernetes.io/managed-by,kubernetes.io/role</code></pre>
</div>
</div>
<div class="paragraph">
<p>To view all labels set on a nodegroup:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl get labels --cluster managed-cluster --nodegroup managed-ng-1</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="scaling-managed-nodegroups">5.2.7. Scaling Managed Nodegroups</h4>
<div class="paragraph">
<p><code>eksctl scale nodegroup</code> also supports managed nodegroups. The syntax for scaling a managed or unmanaged nodegroup is
the same.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl scale nodegroup --name=managed-ng-1 --cluster=managed-cluster --nodes=4 --nodes-min=3 --nodes-max=5</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="feature-parity-with-unmanaged-nodegroups">5.2.8. Feature parity with unmanaged nodegroups</h4>
<div class="paragraph">
<p>EKS Managed Nodegroups are managed by AWS EKS and do not offer the same level of configuration as unmanaged nodegroups.
The unsupported options are noted below.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>iam.instanceProfileARN</code> is not supported for managed nodegroups.</p>
</li>
<li>
<p><code>instancesDistribution</code> field is not supported</p>
</li>
<li>
<p>Full control over the node bootstrapping process and customization of the kubelet are not supported. This includes the
following fields: <code>classicLoadBalancerNames</code>, <code>targetGroupARNs</code>, <code>clusterDNS</code> and <code>kubeletExtraConfig</code>.</p>
</li>
<li>
<p>No support for enabling metrics on AutoScalingGroups using <code>asgMetricsCollection</code></p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="further-information-5">5.2.9. Further information</h4>
<div class="ulist">
<ul>
<li>
<p><a href="eks/latest/userguide/managed-node-groups.html">EKS Managed Nodegroups</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="node-bootstrapping">5.3. Node bootstrapping</h3>
<div class="sect3">
<h4 id="amazonlinux2023">5.3.1. AmazonLinux2023</h4>
<div class="paragraph">
<p>AL2023 introduced a new node initialization process <a href="https://awslabs.github.io/amazon-eks-ami/nodeadm/">nodeadm</a> that uses a YAML configuration schema, dropping the use of <code>/etc/eks/bootstrap.sh</code> script.</p>
</div>
<div class="sect4">
<h5 id="default-settings">Default settings</h5>
<div class="paragraph">
<p>For self-managed nodes and EKS-managed nodes based on custom AMIs, <code>eksctl</code> creates a default, minimal, <code>NodeConfig</code> and automatically injects it into the nodegroups&#8217;s launch template userdata. i.e.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">MIME-Version: 1.0
Content-Type: multipart/mixed; boundary=//

--//
Content-Type: application/node.eks.aws

apiVersion: node.eks.aws/v1alpha1
kind: NodeConfig
spec:
  cluster:
    apiServerEndpoint: https://XXXX.us-west-2.eks.amazonaws.com
    certificateAuthority: XXXX
    cidr: 10.100.0.0/16
    name: my-cluster
  kubelet:
    config:
      clusterDNS:
      - 10.100.0.10
    flags:
    - --node-labels=alpha.eksctl.io/cluster-name=my-cluster,alpha.eksctl.io/nodegroup-name=my-nodegroup
    - --register-with-taints=special=true:NoSchedule

--//--</code></pre>
</div>
</div>
<div class="paragraph">
<p>For EKS-managed nodes based on native AMIs, the default <code>NodeConfig</code> is being added by EKS MNG under the hood, appended directly to the EC2&#8217;s userdata. Thus, in this scenario, <code>eksctl</code> does not need to include it within the launch template.</p>
</div>
</div>
<div class="sect4">
<h5 id="configuring-the-bootstrapping-process">Configuring the bootstrapping process</h5>
<div class="paragraph">
<p>To set advanced properties of <code>NodeConfig</code>, or simply override the default values, eksctl allows you to specify a custom <code>NodeConfig</code> via <code>nodeGroup.overrideBootstrapCommand</code> or <code>managedNodeGroup.overrideBootstrapCommand</code>  e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">managedNodeGroups:
  - name: mng-1
    amiFamily: AmazonLinux2023
    ami: ami-0253856dd7ab7dbc8
    overrideBootstrapCommand: |
      apiVersion: node.eks.aws/v1alpha1
      kind: NodeConfig
      spec:
        instance:
          localStorage:
            strategy: RAID0</code></pre>
</div>
</div>
<div class="paragraph">
<p>This custom config will be prepended to the userdata by eksctl, and merged by <code>nodeadm</code> with the default config. Read more about <code>nodeadm</code>'s capability of merging multiple configuration objects <a href="https://awslabs.github.io/amazon-eks-ami/nodeadm/doc/examples/#merging-multiple-configuration-objects">here</a>.</p>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="launch-template-support">5.4. Launch Template support for Managed Nodegroups</h3>
<div class="paragraph">
<p>eksctl supports launching managed nodegroups using a provided <a href="AWSEC2/latest/UserGuide/ec2-launch-templates.html">EC2 Launch Template</a>.
This enables multiple customization options for nodegroups including providing custom AMIs and security groups, and passing user data for node bootstrapping.</p>
</div>
<div class="sect3">
<h4 id="creating-managed-nodegroups-using-a-provided-launch-template">5.4.1. Creating managed nodegroups using a provided launch template</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># managed-cluster.yaml
# A cluster with two managed nodegroups
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: managed-cluster
  region: us-west-2

managedNodeGroups:
  - name: managed-ng-1
    launchTemplate:
      id: lt-12345
      version: "2" # optional (uses the default launch template version if unspecified)

  - name: managed-ng-2
    minSize: 2
    desiredCapacity: 2
    maxSize: 4
    labels:
      role: worker
    tags:
      nodegroup-name: managed-ng-2
    privateNetworking: true
    launchTemplate:
      id: lt-12345</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="upgrading-a-managed-nodegroup-to-use-a-different-launch-template-version">5.4.2. Upgrading a managed nodegroup to use a different launch template version</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl upgrade nodegroup --name=managed-ng-1 --cluster=managed-cluster --launch-template-version=3</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If a launch template is using a custom AMI, then the new version should also use a custom AMI or the upgrade operation will fail</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>If a launch template is not using a custom AMI, the Kubernetes version to upgrade to can also be specified:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl upgrade nodegroup --name=managed-ng-1 --cluster=managed-cluster --launch-template-version=3 --kubernetes-version=1.17</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="notes-on-custom-ami-and-launch-template-support">5.4.3. Notes on custom AMI and launch template support</h4>
<div class="ulist">
<ul>
<li>
<p>When a launch template is provided, the following fields are not supported: <code>instanceType</code>, <code>ami</code>, <code>ssh.allow</code>, <code>ssh.sourceSecurityGroupIds</code>, <code>securityGroups</code>,
<code>instancePrefix</code>, <code>instanceName</code>, <code>ebsOptimized</code>, <code>volumeEncrypted</code>, <code>volumeKmsKeyID</code>, <code>volumeIOPS</code>, <code>maxPodsPerNode</code>, <code>preBootstrapCommands</code>, <code>overrideBootstrapCommand</code> and <code>disableIMDSv1</code>.</p>
</li>
<li>
<p>When using a custom AMI (<code>ami</code>), <code>overrideBootstrapCommand</code> must also be set to perform the bootstrapping.</p>
</li>
<li>
<p><code>overrideBootstrapCommand</code> can only be set when using a custom AMI.</p>
</li>
<li>
<p>When a launch template is provided, tags specified in the nodegroup config apply to the EKS Nodegroup resource only and are not propagated to EC2 instances.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="custom-subnets">5.5. Custom subnets</h3>
<div class="paragraph">
<p>It&#8217;s possible to extend an existing VPC with a new subnet and add a Nodegroup to that subnet.</p>
</div>
<div class="sect3">
<h4 id="why">5.5.1. Why</h4>
<div class="paragraph">
<p>Should the cluster run out of pre-configured IPs, it&#8217;s possible to resize the existing VPC with
a new CIDR to add a new subnet to it. To see how to do that, read this guide on AWS <a href="vpc/latest/userguide/VPC_Subnets.html#vpc-resize">Extending VPCs</a>.</p>
</div>
<div class="sect4">
<h5 id="tldr">TL;DR</h5>
<div class="paragraph">
<p>Go to the VPC&#8217;s configuration and add click on Actions-&gt;Edit CIDRs and add a new range.
For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-diff" data-lang="diff">192.168.0.0/19 -&gt; existing CIDR
+ 192.169.0.0/19 -&gt; new CIDR</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now you need to add a new Subnet. Depending on if it&#8217;s a new Private or a Public subnet, you will have
to copy the routing information from a private or a public subnet respectively.</p>
</div>
<div class="paragraph">
<p>Once the subnet is created, add routing, and copy either the NAT gateway ID or the Internet Gateway
from another subnet in the VPC. Take care that if it&#8217;s a public subnet Enable Automatic IP Assignment.
Actions-&gt;Modify auto-assign IP settings-&gt;Enable auto-assign public IPv4 address.</p>
</div>
<div class="paragraph">
<p>Don&#8217;t forget to also copy the TAGS of the existing subnets depending on Public or Private subnet configuration.
This is important, otherwise the subnet will not be part of the cluster and instances in the subnet
will be unable to join.</p>
</div>
<div class="paragraph">
<p>When finished, copy the new subnet&#8217;s ID. Repeat as often as necessary.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="how">5.5.2. How</h4>
<div class="paragraph">
<p>To create a nodegroup in the created subnet(s) run the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl create nodegroup --cluster &lt;cluster-name&gt; --name my-new-subnet --subnet-ids subnet-0edeb3a04bec27141,subnet-0edeb3a04bec27142,subnet-0edeb3a04bec27143
# or for a single subnet id
eksctl create nodegroup --cluster &lt;cluster-name&gt; --name my-new-subnet --subnet-ids subnet-0edeb3a04bec27141</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or, use the configuration as such:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create nodegroup -f cluster-managed.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>With a configuration like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># A simple example of ClusterConfig object with two nodegroups:
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-3
  region: eu-north-1

nodeGroups:
  - name: new-subnet-nodegroup
    instanceType: m5.large
    desiredCapacity: 1
    subnets:
      - subnet-id1
      - subnet-id2</code></pre>
</div>
</div>
<div class="paragraph">
<p>Wait for the nodegroup to be created and the new instances should have the new IP ranges of the subnet(s).</p>
</div>
</div>
<div class="sect3">
<h4 id="deleting-the-cluster">5.5.3. Deleting the cluster</h4>
<div class="paragraph">
<p>Since the new addition modified the existing VPC by adding a dependency outside of the CloudFormation stack, CloudFormation
can no longer remove the cluster.</p>
</div>
<div class="paragraph">
<p>Before deleting the cluster, remove all created extra subnets by hand, then proceed by calling <code>eksctl</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl delete cluster -n &lt;cluster-name&gt; --wait</pre>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="custom-dns">5.6. Custom DNS</h3>
<div class="paragraph">
<p>There are two ways of overwriting the DNS server IP address used for all the internal and external DNS lookups. This
is the equivalent of the <code>--cluster-dns</code> flag for the <code>kubelet</code>.</p>
</div>
<div class="paragraph">
<p><code>clusterDNS</code> with the IP address of the DNS server to use.
This will be passed to the <code>kubelet</code> that in turn will pass it to the pods through the <code>/etc/resolv.conf</code> file.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-1
  region: eu-north-1

nodeGroups:
- name: ng-1
  clusterDNS: 169.254.20.10</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that this configuration only accepts one IP address. To specify more than one address, use the
<a href="#customizing-the-kubelet"><code>kubeletExtraConfig</code> parameter</a>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-1
  region: eu-north-1

nodeGroups:
  - name: ng-1
    kubeletExtraConfig:
      clusterDNS: ["169.254.20.10","172.20.0.10"]</code></pre>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="taints">5.7. Taints</h3>
<div class="paragraph">
<p>To apply <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">taints</a> to a specific nodegroup use the <code>taints</code> config section like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">    taints:
      - key: your.domain.com/db
        value: "true"
        effect: NoSchedule
      - key: your.domain.com/production
        value: "true"
        effect: NoExecute</code></pre>
</div>
</div>
<div class="paragraph">
<p>A full example can be found <a href="https://github.com/eksctl-io/eksctl/blob/main/examples/34-taints.yaml">here</a>.</p>
</div>
</div>
<div class="sect2 topic">
<h3 id="instance-selector">5.8. Instance Selector</h3>
<div class="paragraph">
<p>eksctl supports specifying multiple instance types for managed and self-managed nodegroups, but with over 270 EC2 instance types,
users have to spend time figuring out which instance types would be well suited for their nodegroup. It&#8217;s even harder
when using Spot instances because you need to choose a set of instances that works together well with the Cluster Autoscaler.</p>
</div>
<div class="paragraph">
<p>eksctl now integrates with the <a href="https://github.com/aws/amazon-ec2-instance-selector">EC2 instance selector</a>,
which addresses this problem by generating a list of instance types based on resource criteria: vCPUs, memory, # of GPUs and CPU architecture.
When the instance selector criteria is passed, eksctl creates a nodegroup with the instance types set to the instance types
matching the supplied criteria.</p>
</div>
<div class="sect3">
<h4 id="create-cluster-and-nodegroups">5.8.1. Create cluster and nodegroups</h4>
<div class="paragraph">
<p>To create a cluster with a single nodegroup that uses instance types matched by the instance selector resource
criteria passed to eksctl, run</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create cluster --instance-selector-vcpus=2 --instance-selector-memory=4</code></pre>
</div>
</div>
<div class="paragraph">
<p>This will create a cluster and a managed nodegroup with the <code>instanceTypes</code> field set to
<code>[c5.large, c5a.large, c5ad.large, c5d.large, t2.medium, t3.medium, t3a.medium]</code> (the set of instance types returned may change).</p>
</div>
<div class="paragraph">
<p>For unmanaged nodegroups, the <code>instancesDistribution.instanceTypes</code> field will be set:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create cluster --managed=false --instance-selector-vcpus=2 --instance-selector-memory=4</code></pre>
</div>
</div>
<div class="paragraph">
<p>The instance selector criteria can also be specified in ClusterConfig:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># instance-selector-cluster.yaml
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster
  region: us-west-2

nodeGroups:
- name: ng
  instanceSelector:
    vCPUs: 2
    memory: "4" # 4 GiB, unit defaults to GiB

managedNodeGroups:
- name: mng
  instanceSelector:
    vCPUs: 2
    memory: 2GiB #
    cpuArchitecture: x86_64 # default value</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create cluster -f instance-selector-cluster.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following instance selector CLI options are supported by <code>eksctl create cluster</code> and <code>eksctl create nodegroup</code>:</p>
</div>
<div class="paragraph">
<p><code>--instance-selector-vcpus</code>, <code>--instance-selector-memory</code>, <code>--instance-selector-gpus</code> and <code>instance-selector-cpu-architecture</code></p>
</div>
<div class="paragraph">
<p>An example file can be found <a href="https://github.com/eksctl-io/eksctl/blob/main/examples/28-instance-selector.yaml">here</a>.</p>
</div>
<div class="sect4">
<h5 id="dry-run-3">Dry Run</h5>
<div class="paragraph">
<p>The <a href="#dry-run">dry-run</a> feature allows you to inspect and change the instances matched by the instance selector before proceeding
to creating a nodegroup.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create cluster --name development --instance-selector-vcpus=2 --instance-selector-memory=4 --dry-run

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
# ...
managedNodeGroups:
- amiFamily: AmazonLinux2
  instanceSelector:
    memory: "4"
    vCPUs: 2
  instanceTypes:
  - c5.large
  - c5a.large
  - c5ad.large
  - c5d.large
  - t2.medium
  - t3.medium
  - t3a.medium
...
# other config</code></pre>
</div>
</div>
<div class="paragraph">
<p>The generated ClusterConfig can then be passed to <code>eksctl create cluster</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create cluster -f generated-cluster.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>instanceSelector</code> field representing the CLI options will also be added to the ClusterConfig file for visibility and documentation purposes.
When <code>--dry-run</code> is omitted, this field will be ignored and the <code>instanceTypes</code> field will be used, otherwise any
changes to <code>instanceTypes</code> would get overridden by eksctl.</p>
</div>
<div class="paragraph">
<p>When a ClusterConfig file is passed with <code>--dry-run</code>, eksctl will output a ClusterConfig file containing the same set of nodegroups after expanding each nodegroup&#8217;s instance selector resource criteria.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># instance-selector-cluster.yaml
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster
  region: us-west-2

nodeGroups:
- name: ng
  instanceSelector:
    vCPUs: 2
    memory: 4 # 4 GiB, unit defaults to GiB

managedNodeGroups:
- name: mng
  instanceSelector:
    vCPUs: 2
    memory: 2GiB #
    cpuArchitecture: x86_64 # default value</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create cluster -f instance-selector-cluster.yaml --dry-run

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
# ...
managedNodeGroups:
- amiFamily: AmazonLinux2
  # ...
  instanceSelector:
    cpuArchitecture: x86_64
    memory: 2GiB
    vCPUs: 2
  instanceTypes:
  - t3.small
  - t3a.small
nodeGroups:
- amiFamily: AmazonLinux2
  # ...
  instanceSelector:
    memory: "4"
    vCPUs: 2
  instanceType: mixed
  instancesDistribution:
    capacityRebalance: false
    instanceTypes:
    - c5.large
    - c5a.large
    - c5ad.large
    - c5d.large
    - t2.medium
    - t3.medium
    - t3a.medium
# ...</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="spot-instances">5.9. Spot instances</h3>
<div class="sect3">
<h4 id="managed-nodegroups">5.9.1. Managed Nodegroups</h4>
<div class="paragraph">
<p><code>eksctl</code> supports <a href="https://aws.amazon.com/blogs/containers/amazon-eks-now-supports-provisioning-and-managing-ec2-spot-instances-in-managed-node-groups/">Spot worker nodes using EKS Managed Nodegroups</a>, a feature that allows EKS customers with
fault-tolerant applications to easily provision and manage EC2 Spot Instances for their EKS clusters.
EKS Managed Nodegroup will configure and launch an EC2 Autoscaling group of Spot Instances following Spot best
practices and draining Spot worker nodes automatically before the instances are interrupted by AWS. There is no
incremental charge to use this feature and customers pay only for using the AWS resources, such as EC2 Spot Instances
and EBS volumes.</p>
</div>
<div class="paragraph">
<p>To create a cluster with a managed nodegroup using Spot instances, pass the <code>--spot</code> flag and an optional list of instance types:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create cluster --spot --instance-types=c3.large,c4.large,c5.large</code></pre>
</div>
</div>
<div class="paragraph">
<p>To create a managed nodegroup using Spot instances on an existing cluster:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create nodegroup --cluster=&lt;clusterName&gt; --spot --instance-types=c3.large,c4.large,c5.large</code></pre>
</div>
</div>
<div class="paragraph">
<p>To create Spot instances using managed nodegroups via a config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># spot-cluster.yaml

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: spot-cluster
  region: us-west-2

managedNodeGroups:
- name: spot
  instanceTypes: ["c3.large","c4.large","c5.large","c5d.large","c5n.large","c5a.large"]
  spot: true


# `instanceTypes` defaults to [`m5.large`]
- name: spot-2
  spot: true

# On-Demand instances
- name: on-demand
  instanceTypes: ["c3.large", "c4.large", "c5.large"]</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">$ eksctl create cluster -f spot-cluster.yaml</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Unmanaged nodegroups do not support the <code>spot</code> and <code>instanceTypes</code> fields, instead the <code>instancesDistribution</code> field
is used to configure Spot instances. <a href="#spot-unmanaged">See below</a></p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="further-information-6">Further information</h5>
<div class="ulist">
<ul>
<li>
<p><a href="https://aws.amazon.com/blogs/containers/amazon-eks-now-supports-provisioning-and-managing-ec2-spot-instances-in-managed-node-groups/">EKS Spot Nodegroups</a></p>
</li>
<li>
<p><a href="eks/latest/userguide/managed-node-groups.html#managed-node-group-capacity-types">EKS Managed Nodegroup Capacity Types</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect3">
<h4 id="spot-unmanaged">5.9.2. Unmanaged Nodegroups</h4>
<div class="paragraph">
<p><code>eksctl</code> has support for spot instances through the MixedInstancesPolicy for Auto Scaling Groups.</p>
</div>
<div class="paragraph">
<p>Here is an example of a nodegroup that uses 50% spot instances and 50% on demand instances:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng-1
    minSize: 2
    maxSize: 5
    instancesDistribution:
      maxPrice: 0.017
      instanceTypes: ["t3.small", "t3.medium"] # At least one instance type should be specified
      onDemandBaseCapacity: 0
      onDemandPercentageAboveBaseCapacity: 50
      spotInstancePools: 2</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that the <code>nodeGroups.X.instanceType</code> field shouldn&#8217;t be set when using the <code>instancesDistribution</code> field.</p>
</div>
<div class="paragraph">
<p>This example uses GPU instances:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng-gpu
    instanceType: mixed
    desiredCapacity: 1
    instancesDistribution:
      instanceTypes:
        - p2.xlarge
        - p2.8xlarge
        - p2.16xlarge
      maxPrice: 0.50</code></pre>
</div>
</div>
<div class="paragraph">
<p>This example uses the capacity-optimized spot allocation strategy:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng-capacity-optimized
    minSize: 2
    maxSize: 5
    instancesDistribution:
      maxPrice: 0.017
      instanceTypes: ["t3.small", "t3.medium"] # At least one instance type should be specified
      onDemandBaseCapacity: 0
      onDemandPercentageAboveBaseCapacity: 50
      spotAllocationStrategy: "capacity-optimized"</code></pre>
</div>
</div>
<div class="paragraph">
<p>This example uses the capacity-optimized-prioritized spot allocation strategy:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng-capacity-optimized-prioritized
    minSize: 2
    maxSize: 5
    instancesDistribution:
      maxPrice: 0.017
      instanceTypes: ["t3a.small", "t3.small"] # At least two instance types should be specified
      onDemandBaseCapacity: 0
      onDemandPercentageAboveBaseCapacity: 0
      spotAllocationStrategy: "capacity-optimized-prioritized"</code></pre>
</div>
</div>
<div class="paragraph">
<p><a href="autoscaling/ec2/userguide/asg-purchase-options.html#asg-spot-strategy">Use the <code>capacity-optimized-prioritized</code> allocation strategy and then set the order of instance types in the list of launch template overrides from highest to lowest priority (first to last in the list). Amazon EC2 Auto Scaling honors the instance type priorities on a best-effort basis but optimizes for capacity first. This is a good option for workloads where the possibility of disruption must be minimized, but also the preference for certain instance types matters.</a></p>
</div>
<div class="paragraph">
<p>Note that the <code>spotInstancePools</code> field shouldn&#8217;t be set when using the <code>spotAllocationStrategy</code> field. If the <code>spotAllocationStrategy</code> is not specified, EC2 will default to use the <code>lowest-price</code> strategy.</p>
</div>
<div class="paragraph">
<p>Here is a minimal example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng-1
    instancesDistribution:
      instanceTypes: ["t3.small", "t3.medium"] # At least one instance type should be specified</code></pre>
</div>
</div>
<div class="paragraph">
<p>To distinguish nodes between spot or on-demand instances you can use the kubernetes label <code>node-lifecycle</code> which will have the value <code>spot</code> or <code>on-demand</code> depending on its type.</p>
</div>
<div class="sect4">
<h5 id="parameters-in-instancesdistribution">Parameters in instancesDistribution</h5>

</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="gpu-support">5.10. GPU Support</h3>
<div class="paragraph">
<p>Eksctl supports selecting GPU instance types for nodegroups. Simply supply a
compatible instance type to the create command, or via the config file.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create cluster --node-type=p2.xlarge</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>It is no longer necessary to subscribe to the marketplace AMI for GPU support on EKS.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The AMI resolvers (<code>auto</code> and <code>auto-ssm</code>) will see that you want to use a
GPU instance type and they will select the correct EKS optimized accelerated AMI.</p>
</div>
<div class="paragraph">
<p>Eksctl will detect that an AMI with a GPU-enabled instance type has been selected and
will install the <a href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA Kubernetes device plugin</a> automatically.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Windows and Ubuntu AMIs do not ship with GPU drivers installed, hence running GPU-accelerated workloads will not work out of the box.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To disable the automatic plugin installation, and manually install a specific version,
use <code>--install-nvidia-plugin=false</code> with the create command. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create cluster --node-type=p2.xlarge --install-nvidia-plugin=false</pre>
</div>
</div>
<div class="paragraph">
<p>and, for versions 0.15.0 and above,</p>
</div>
<div class="listingblock">
<div class="content">
<pre>kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/&lt;VERSION&gt;/deployments/static/nvidia-device-plugin.yml</pre>
</div>
</div>
<div class="paragraph">
<p>or, for older versions,</p>
</div>
<div class="listingblock">
<div class="content">
<pre>kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/&lt;VERSION&gt;/nvidia-device-plugin.yml</pre>
</div>
</div>
<div class="paragraph">
<p>The installation of the <a href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA Kubernetes device plugin</a> will be skipped if the cluster only includes Bottlerocket nodegroups, since Bottlerocket already handles the execution of the device plugin.
If you use different AMI families in your cluster&#8217;s configurations, you may need to use taints and tolerations to keep the device plugin from running on Bottlerocket nodes.</p>
</div>
</div>
<div class="sect2 topic">
<h3 id="arm-support">5.11. ARM Support</h3>
<div class="paragraph">
<p>EKS supports 64-bit ARM architecture with its <a href="https://aws.amazon.com/ec2/graviton/">Graviton processors</a>. To create a cluster,
select one of the Graviton-based instance types (<code>a1</code>, <code>t4g</code>, <code>m6g</code>, <code>m7g</code>, <code>m6gd</code>, <code>c6g</code>, <code>c7g</code>, <code>c6gd</code>, <code>r6g</code>, <code>r7g</code>, <code>r6gd</code>, <code>m8g</code>, <code>r8g</code>, <code>c8g</code>) and run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create cluster --node-type=a1.large</pre>
</div>
</div>
<div class="paragraph">
<p>or use a config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-arm-1
  region: us-west-2


nodeGroups:
  - name: ng-arm-1
    instanceType: m6g.medium
    desiredCapacity: 1</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create cluster -f cluster-arm-1.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>ARM is also supported in managed nodegroups:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-arm-2
  region: us-west-2

managedNodeGroups:
  - name: mng-arm-1
    instanceType: m6g.medium
    desiredCapacity: 1</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create cluster -f cluster-arm-2.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>The AMI resolvers, <code>auto</code> and <code>auto-ssm</code>, will infer the correct AMI based on the ARM instance type. Only AmazonLinux2023, AmazonLinux2 and Bottlerocket families have EKS optimized AMIs for ARM.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>ARM is supported for clusters with version 1.15 and higher.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2 topic">
<h3 id="auto-scaling">5.12. Auto Scaling</h3>
<div class="sect3">
<h4 id="enable-auto-scaling">5.12.1. Enable Auto Scaling</h4>
<div class="paragraph">
<p>You can create a cluster (or nodegroup in an existing cluster) with IAM role that will allow use of <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md">cluster autoscaler</a>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create cluster --asg-access</code></pre>
</div>
</div>
<div class="paragraph">
<p>This flag also sets <code>k8s.io/cluster-autoscaler/enabled</code>
and <code>k8s.io/cluster-autoscaler/&lt;clusterName&gt;</code> tags, so nodegroup discovery should work.</p>
</div>
<div class="paragraph">
<p>Once the cluster is running, you will need to install <a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md">Cluster Autoscaler</a> itself.</p>
</div>
<div class="paragraph">
<p>You should also add the following to your managed or unmanaged nodegroup definition(s) to add the tags required for the Cluster Autoscaler to scale the nodegroup:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng1-public
    iam:
      withAddonPolicies:
        autoScaler: true</code></pre>
</div>
</div>
<div class="sect4">
<h5 id="scaling-up-from-0">Scaling up from 0</h5>
<div class="paragraph">
<p>If you would like to be able to scale your node group up from 0 and you have
labels and/or taints defined on your nodegroups, you will need to propagate these as
tags on your Auto Scaling Groups (ASGs).</p>
</div>
<div class="paragraph">
<p>One way to do this is by setting the ASG tags in the <code>tags</code> field of your nodegroup
definitions. For example, given a nodegroup with the following labels and
taints:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng1-public
    ...
    labels:
      my-cool-label: pizza
    taints:
      key: feaster
      value: "true"
      effect: NoSchedule</code></pre>
</div>
</div>
<div class="paragraph">
<p>You would need to add the following ASG tags:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng1-public
    ...
    labels:
      my-cool-label: pizza
    taints:
      feaster: "true:NoSchedule"
    tags:
      k8s.io/cluster-autoscaler/node-template/label/my-cool-label: pizza
      k8s.io/cluster-autoscaler/node-template/taint/feaster: "true:NoSchedule"</code></pre>
</div>
</div>
<div class="paragraph">
<p>For both managed and unmanaged nodegroups, this can be done automatically by setting <code>propagateASGTags</code> to <code>true</code>, which will add the labels and taints as tags to the Auto Scaling group:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng1-public
    ...
    labels:
      my-cool-label: pizza
    taints:
      feaster: "true:NoSchedule"
    propagateASGTags: true</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="zone-aware-auto-scaling">Zone-aware Auto Scaling</h5>
<div class="paragraph">
<p>If your workloads are zone-specific you&#8217;ll need to create separate nodegroups for each zone. This is because the <code>cluster-autoscaler</code> assumes that all nodes in a group are exactly equivalent. So, for example, if a scale-up event is triggered by a pod which needs a zone-specific PVC (e.g. an EBS volume), the new node might get scheduled in the wrong AZ and the pod will fail to start.</p>
</div>
<div class="paragraph">
<p>You won&#8217;t need a separate nodegroup for each AZ if your environment meets the following criteria:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>No zone-specific storage requirements.</p>
</li>
<li>
<p>No required podAffinity with topology other than host.</p>
</li>
<li>
<p>No required nodeAffinity on zone label.</p>
</li>
<li>
<p>No nodeSelector on a zone label.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>(Read more <a href="https://github.com/kubernetes/autoscaler/pull/1802#issuecomment-474295002">here</a> and <a href="https://github.com/eksctl-io/eksctl/pull/647#issuecomment-474698054">here</a>.)</p>
</div>
<div class="paragraph">
<p>If you meet all of the above requirements (and possibly others) then you should be safe with a single nodegroup which spans multiple AZs. Otherwise you&#8217;ll want to create separate, single-AZ nodegroups:</p>
</div>
<div class="paragraph">
<p>BEFORE:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng1-public
    instanceType: m5.xlarge
    # availabilityZones: ["eu-west-2a", "eu-west-2b"]</code></pre>
</div>
</div>
<div class="paragraph">
<p>AFTER:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng1-public-2a
    instanceType: m5.xlarge
    availabilityZones: ["eu-west-2a"]
  - name: ng1-public-2b
    instanceType: m5.xlarge
    availabilityZones: ["eu-west-2b"]</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="custom-ami-support">5.13. Custom AMI support</h3>
<div class="sect3">
<h4 id="setting-the-node-ami-id">5.13.1. Setting the node AMI ID</h4>
<div class="paragraph">
<p>The <code>--node-ami</code> flag enables a number of advanced use cases such as using a custom AMI or querying AWS in realtime to determine which AMI to use.
The flag can be used for both non-GPU and GPU images.</p>
</div>
<div class="paragraph">
<p>The flag can take the AMI image id for an image to explicitly use. It also can take the following 'special' keywords:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Keyword</th>
<th class="tableblock halign-left valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">auto</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Indicates that the AMI to use for the nodes should be found by querying AWS EC2. This relates to the auto resolver.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">auto-ssm</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Indicates that the AMI to use for the nodes should be found by querying AWS SSM Parameter Store.</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When setting <code>--node-ami</code> to an ID string, <code>eksctl</code> will assume that a custom AMI has been requested.
For AmazonLinux2 and Ubuntu nodes, both EKS managed and self-managed, this will mean that <code>overrideBootstrapCommand</code> is required.
For AmazonLinux2023, since it stops using the <code>/etc/eks/bootstrap.sh</code> script for node bootstrapping, in favour of a nodeadm initialization process (for more information, please refer to <a href="https://github.com/eksctl-io/eksctl/blob/main/pkg/nodebootstrap/README.md">node bootstrapping docs</a>), <code>overrideBootstrapCommand</code> is not supported.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>CLI flag examples:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sh" data-lang="sh">eksctl create cluster --node-ami=auto

# with a custom ami id
eksctl create cluster --node-ami=ami-custom1234</code></pre>
</div>
</div>
<div class="paragraph">
<p>Config file example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng1
    instanceType: p2.xlarge
    amiFamily: AmazonLinux2
    ami: auto
  - name: ng2
    instanceType: m5.large
    amiFamily: AmazonLinux2
    ami: ami-custom1234
managedNodeGroups:
  - name: m-ng-2
    amiFamily: AmazonLinux2
    ami: ami-custom1234
    instanceType: m5.large
    overrideBootstrapCommand: |
      #!/bin/bash
      /etc/eks/bootstrap.sh &lt;cluster-name&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>--node-ami</code> flag can also be used with <code>eksctl create nodegroup</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="setting-the-node-ami-family">5.13.2. Setting the node AMI Family</h4>
<div class="paragraph">
<p>The <code>--node-ami-family</code> can take following keywords:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Keyword</th>
<th class="tableblock halign-center valign-top">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">AmazonLinux2</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">Indicates that the EKS AMI image based on Amazon Linux 2 should be used (default).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">AmazonLinux2023</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">Indicates that the EKS AMI image based on Amazon Linux 2023 should be used.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ubuntu1804</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">Indicates that the EKS AMI image based on Ubuntu 18.04 LTS (Bionic) should be used.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ubuntu2004</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">Indicates that the EKS AMI image based on Ubuntu 20.04 LTS (Focal) should be used (supported for EKS &lt;= 1.29).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Ubuntu2204</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">Indicates that the EKS AMI image based on Ubuntu 22.04 LTS (Jammy) should be used (available for EKS &gt;= 1.29).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">UbuntuPro2204</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">Indicates that the EKS AMI image based on Ubuntu Pro 22.04 LTS (Jammy) should be used (available for EKS &gt;= 1.29).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Bottlerocket</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">Indicates that the EKS AMI image based on Bottlerocket should be used.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">WindowsServer2019FullContainer</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">Indicates that the EKS AMI image based on Windows Server 2019 Full Container should be used.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">WindowsServer2019CoreContainer</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">Indicates that the EKS AMI image based on Windows Server 2019 Core Container should be used.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">WindowsServer2022FullContainer</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">Indicates that the EKS AMI image based on Windows Server 2022 Full Container should be used.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">WindowsServer2022CoreContainer</p></td>
<td class="tableblock halign-center valign-top"><p class="tableblock">Indicates that the EKS AMI image based on Windows Server 2022 Core Container should be used.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>CLI flag example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-sh" data-lang="sh">eksctl create cluster --node-ami-family=AmazonLinux2</code></pre>
</div>
</div>
<div class="paragraph">
<p>Config file example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng1
    instanceType: m5.large
    amiFamily: AmazonLinux2
managedNodeGroups:
  - name: m-ng-2
    instanceType: m5.large
    amiFamily: Ubuntu2204</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>--node-ami-family</code> flag can also be used with <code>eksctl create nodegroup</code>. <code>eksctl</code> requires AMI Family to be explicitly set via config file or via <code>--node-ami-family</code> CLI flag, whenever working with a custom AMI.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>At the moment, EKS managed nodegroups only support the following AMI Families when working with custom AMIs: <code>AmazonLinux2023</code>, <code>AmazonLinux2</code>, <code>Ubuntu1804</code>, <code>Ubuntu2004</code> and <code>Ubuntu2204</code></p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="windows-custom-ami-support">5.13.3. Windows custom AMI support</h4>
<div class="paragraph">
<p>Only self-managed Windows nodegroups can specify a custom AMI. <code>amiFamily</code> should be set to a valid Windows AMI family.</p>
</div>
<div class="paragraph">
<p>The following PowerShell variables will be available to the bootstrap script:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$EKSBootstrapScriptFile
$EKSClusterName
$APIServerEndpoint
$Base64ClusterCA
$ServiceCIDR
$KubeletExtraArgs
$KubeletExtraArgsMap: A hashtable containing arguments for the kubelet, e.g., @{ 'node-labels' = ''; 'register-with-taints' = ''; 'max-pods' = '10'}
$DNSClusterIP
$ContainerRuntime</pre>
</div>
</div>
<div class="paragraph">
<p>Config file example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: custom-windows
    amiFamily: WindowsServer2022FullContainer
    ami: ami-01579b74557facaf7
    overrideBootstrapCommand: |
      &amp; $EKSBootstrapScriptFile -EKSClusterName "$EKSClusterName" -APIServerEndpoint "$APIServerEndpoint" -Base64ClusterCA "$Base64ClusterCA" -ContainerRuntime "containerd" -KubeletExtraArgs "$KubeletExtraArgs" 3&gt;&amp;1 4&gt;&amp;1 5&gt;&amp;1 6&gt;&amp;1</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="bottlerocket-custom-ami-support">5.13.4. Bottlerocket custom AMI support</h4>
<div class="paragraph">
<p>For Bottlerocket nodes, the <code>overrideBootstrapCommand</code> is not supported. Instead, to designate their own bootstrap container, one should use the <code>bottlerocket</code> field as part of the configuration file. E.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">  nodeGroups:
  - name: bottlerocket-ng
    ami: ami-custom1234
    amiFamily: Bottlerocket
    bottlerocket:
      enableAdminContainer: true
      settings:
        bootstrap-containers:
          bootstrap:
            source: &lt;MY-CONTAINER-URI&gt;</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="define-container-runtime">5.14. Define Container Runtime</h3>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Starting with Kubernetes version <code>1.24</code>, dockershim support has been deprecated. Therefore, if you create a cluster using <code>eksctl</code> on version <code>1.24</code> or higher, the information below no longer applies, and the only supported container runtime is <code>containerd</code>. Trying to set it otherwise will return a validation error. Additionally, AL2023 AMIs only support <code>containerd</code> regadless of K8s version.</p>
</div>
<div class="paragraph">
<p>At some point, we will completely remove the option to set <code>containerRuntime</code> in config file, together with the support for older Kubernetes versions support (i.e. <code>1.22</code> or <code>1.23</code>).</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>For AL2 ( AmazonLinux2 ) and Windows AMIs, it&#8217;s possible to set container runtime to <code>containerd</code>.</p>
</div>
<div class="sect3">
<h4 id="un-managed-nodes">5.14.1. Un-managed Nodes</h4>
<div class="paragraph">
<p>For un-managed nodes, simply provide the following configuration when creating a new node:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: container-runtime-test
  region: us-west-2

nodeGroups:
  - name: ng-1
    instanceType: m5.xlarge
    desiredCapacity: 2
    amiFamily: AmazonLinux2
    containerRuntime: containerd</code></pre>
</div>
</div>
<div class="paragraph">
<p>This value is set to <code>dockerd</code> by default to preserve backwards compatibility, but will soon be
deprecated.</p>
</div>
<div class="paragraph">
<p><em>Note that there is no equivalent flag for setting the container runtime, this can only be done via a config file.</em></p>
</div>
<div class="paragraph">
<p>At the time of this writing the following container runtime values are allowed:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>containerd</p>
</li>
<li>
<p>dockerd (docker for Windows)</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="managed-nodes">5.14.2. Managed Nodes</h4>
<div class="paragraph">
<p>For managed nodes we don&#8217;t explicitly provide a bootstrap script, and thus it&#8217;s up to the user
to define a different runtime if they wish, using <code>overrideBootstrapCommand</code>.
The <code>overrideBootstrapCommand</code> option requires that you specify an AMI for the managed node group.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">managedNodeGroups:
  - name: m-ng-1
    ami: ami-XXXXXXXXXXXXXX
    instanceType: m5.large
    overrideBootstrapCommand: |
      #!/bin/bash
      /etc/eks/bootstrap.sh &lt;cluster-name&gt; &lt;other flags&gt; --container-runtime containerd</code></pre>
</div>
</div>
<div class="paragraph">
<p>For Windows managed nodes, you will need to use a custom launch template with an ami-id and pass in the required bootstrap arguments in the userdata.
Read more on <a href="eks/latest/userguide/launch-templates.html">creating a launch template</a>, and <a href="#launch-template-support">using a launch template with eksctl</a>.</p>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="windows-worker-nodes">5.15. Windows Worker Nodes</h3>
<div class="paragraph">
<p>From version 1.14, Amazon EKS supports <a href="eks/latest/userguide/windows-support.html">Windows Nodes</a> that allow running Windows containers.
In addition to having Windows nodes, a Linux node in the cluster is required to run CoreDNS, as Microsoft doesn&#8217;t support host-networking mode yet. Thus, a Windows EKS cluster will be a mixture of Windows nodes and at least one Linux node.
The Linux nodes are critical to the functioning of the cluster, and thus, for a production-grade cluster, it&#8217;s recommended to have at least two <code>t2.large</code> Linux nodes for HA.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You no longer need to install the VPC resource controller on Linux worker nodes to run Windows workloads in EKS clusters
created after October 22, 2021.
You can enable Windows IP address management on the EKS control plane via a ConﬁgMap setting (see link:eks/latest/userguide/windows-support.html for details).
eksctl will automatically patch the ConfigMap to enable Windows IP address management when a Windows nodegroup is created.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="creating-a-new-windows-cluster">5.15.1. Creating a new Windows cluster</h4>
<div class="paragraph">
<p>The config file syntax allows creating a fully-functioning Windows cluster in a single command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre># cluster.yaml
# An example of ClusterConfig containing Windows and Linux node groups to support Windows workloads
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: windows-cluster
  region: us-west-2

nodeGroups:
  - name: windows-ng
    amiFamily: WindowsServer2019FullContainer
    minSize: 2
    maxSize: 3

managedNodeGroups:
  - name: linux-ng
    instanceType: t2.large
    minSize: 2
    maxSize: 3

  - name: windows-managed-ng
    amiFamily: WindowsServer2019FullContainer
    minSize: 2
    maxSize: 3</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create cluster -f cluster.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>To create a new cluster with Windows un-managed nodegroup without using a config file, issue the following commands:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create cluster --managed=false --name=windows-cluster --node-ami-family=WindowsServer2019CoreContainer</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="adding-windows-support-to-an-existing-linux-cluster">5.15.2. Adding Windows support to an existing Linux cluster</h4>
<div class="paragraph">
<p>To enable running Windows workloads on an existing cluster with Linux nodes (<code>AmazonLinux2</code> AMI family), you need to add a Windows nodegroup.</p>
</div>
<div class="paragraph">
<p><strong>NEW</strong> Support for Windows managed nodegroup has been added (--managed=true or omit the flag).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create nodegroup --managed=false --cluster=existing-cluster --node-ami-family=WindowsServer2019CoreContainer
eksctl create nodegroup --cluster=existing-cluster --node-ami-family=WindowsServer2019CoreContainer</code></pre>
</div>
</div>
<div class="paragraph">
<p>To ensure workloads are scheduled on the right OS, they must have a <code>nodeSelector</code> targeting the OS it must run on:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># Targeting Windows
  nodeSelector:
    kubernetes.io/os: windows
    kubernetes.io/arch: amd64</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># Targeting Linux
  nodeSelector:
    kubernetes.io/os: linux
    kubernetes.io/arch: amd64</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you are using a cluster older than <code>1.19</code> the <code>kubernetes.io/os</code> and <code>kubernetes.io/arch</code> labels need to be replaced with <code>beta.kubernetes.io/os</code> and <code>beta.kubernetes.io/arch</code> respectively.</p>
</div>
<div class="sect4">
<h5 id="further-information-7">Further information</h5>
<div class="ulist">
<ul>
<li>
<p><a href="eks/latest/userguide/windows-support.html">EKS Windows Support</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="additional-volume-mappings">5.16. Additional Volume Mappings</h3>
<div class="paragraph">
<p>As an additional configuration option, when dealing with volume mappings, it&#8217;s possible to configure extra mappings
when the nodegroup is created.</p>
</div>
<div class="paragraph">
<p>To do this, set the field <code>additionalVolumes</code> as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: dev-cluster
  region: eu-north-1

managedNodeGroups:
  - name: ng-1-workers
    labels: { role: workers }
    instanceType: m5.xlarge
    desiredCapacity: 10
    volumeSize: 80
    additionalVolumes:
      - volumeName: '/tmp/mount-1' # required
        volumeSize: 80
        volumeType: 'gp3'
        volumeEncrypted: true
        volumeKmsKeyID: 'id'
        volumeIOPS: 3000
        volumeThroughput: 125
      - volumeName: '/tmp/mount-2'  # required
        volumeSize: 80
        volumeType: 'gp2'
        snapshotID: 'snapshot-id'</code></pre>
</div>
</div>
<div class="paragraph">
<p>For more details about selecting volumeNames, see the <a href="AWSEC2/latest/UserGuide/device_naming.html">device naming documentation</a>.
To find out more about EBS volumes, Instance volume limits or Block device mappings visit <a href="AWSEC2/latest/UserGuide/Storage.html">this page</a>.</p>
</div>
</div>
<div class="sect2 topic">
<h3 id="eks-hybrid-nodes">5.17. EKS Hybrid Nodes</h3>
<div class="sect3">
<h4 id="introduction-3">5.17.1. Introduction</h4>
<div class="paragraph">
<p>AWS EKS introduces Hybrid Nodes, a new feature that enables you to run on-premises and edge applications on customer-managed infrastructure with the same AWS EKS clusters, features, and tools you use in the AWS Cloud. AWS EKS Hybird Nodes brings an AWS-managed Kubernetes experience to on-premises environments for customers to simplify and standardize how you run applications across on-premises, edge and cloud environments. Read more at <a href="eks/latest/userguide/hybrid-nodes-overview.html">EKS Hybrid Nodes</a>.</p>
</div>
<div class="paragraph">
<p>To facilitate support for this feature, eksctl introduces a new top-level field called <code>remoteNetworkConfig</code>. Any Hybrid Nodes related configuration shall be set up via this field, as part of the config file; there are no CLI flags counterparts. Additionally, at launch, any remote network config can only be set up during cluster creation and cannot be updated afterwards. This means, you won&#8217;t be able to update existing clusters to use Hybrid Nodes.</p>
</div>
<div class="paragraph">
<p>The <code>remoteNetworkConfig</code> section of the config file allows you to setup the two core areas when it comes to joining remote nodes to you EKS clusters: <strong>networking</strong> and <strong>credentials</strong>.</p>
</div>
</div>
<div class="sect3">
<h4 id="hybrid-networking">5.17.2. Networking</h4>
<div class="paragraph">
<p>EKS Hybrid Nodes is ﬂexible to your preferred method of connecting your on-premises network(s) to an AWS VPC. There are several <a href="whitepapers/latest/aws-vpc-connectivity-options/network-to-amazon-vpc-connectivity-options.html">documented options</a> available, including AWS Site-to-Site VPN and AWS Direct Connect, and you can choose the method that best fits your use case. In most of the methods you might choose, your VPC will be attached to either a virtual private gateway (VGW) or a transit gateway (TGW). If you rely on eksctl to create a VPC for you, eksctl will also configure, <strong>within the scope of your VPC</strong>, any networking related pre-requisites in order to facilitate communication between your EKS control plane and the remote nodes i.e.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>ingress/egress SG rules</p>
</li>
<li>
<p>routes in the private subnets' route tables</p>
</li>
<li>
<p>the VPC gateway attachment to the given TGW or VGW</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Example config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">remoteNetworkConfig:
  vpcGatewayID: tgw-xxxx # either VGW or TGW to be attached to your VPC
  remoteNodeNetworks:
    # eksctl will create, behind the scenes, SG rules, routes, and a VPC gateway attachment,
    # to facilitate communication between remote network(s) and EKS control plane, via the attached gateway
    - cidrs: ["10.80.146.0/24"]
  remotePodNetworks:
    - cidrs: ["10.86.30.0/23"]</code></pre>
</div>
</div>
<div class="paragraph">
<p>If your connectivity method of choice does not involve using a TGW or VGW, you must not rely on eksctl to create the VPC for you, and instead provide a pre-existing one. On a related note, if you are using a pre-existing VPC, eksctl won&#8217;t make any amendments to it, and ensuring all networking requirements are in place falls under your responsibility.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>eksctl does not setup any networking infrastructure outside your AWS VPC (i.e. any infrastructure from VGW/TGW to the remote networks)</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="credentials">5.17.3. Credentials</h4>
<div class="paragraph">
<p>EKS Hybrid Nodes use the AWS IAM Authenticator and temporary IAM credentials provisioned by either <strong>AWS SSM</strong> or <strong>AWS IAM Roles Anywhere</strong>
to authenticate with the EKS cluster. Similar to the self-managed nodegroups, if not otherwise provided, eksctl will create for you a Hybrid Nodes IAM Role to be assumed by the remote nodes. Additioanlly, when using IAM Roles Anywhere as your credentials provider, eksctl will setup a profile, and trust anchor based on a given certificate authority bundle (<code>iam.caBundleCert</code>) e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">remoteNetworkConfig:
  iam:
    # the provider for temporary IAM credentials. Default is SSM.
    provider: IRA
    # the certificate authority bundle that serves as the root of trust,
    # used to validate the X.509 certificates provided by your nodes.
    # can only be set when provider is IAMRolesAnywhere.
    caBundleCert: xxxx</code></pre>
</div>
</div>
<div class="paragraph">
<p>The ARN of the Hybrid Nodes Role created by eksctl is needed later in the process of joining your remote nodes to the cluster, to setup <code>NodeConfig</code> for <code>nodeadm</code>, and to create activations (if using SSM). To fetch it, use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">aws cloudformation describe-stacks \
  --stack-name eksctl-&lt;CLUSTER_NAME&gt;-cluster \
  --query 'Stacks[].Outputs[?OutputKey==`RemoteNodesRoleARN`].[OutputValue]' \
  --output text</code></pre>
</div>
</div>
<div class="paragraph">
<p>Similarly, if using IAM Roles Anywhere, you can fetch the ARN of the trust anchor and of the anywhere profile created by eksctl, amending the previous command by replacing <code>RemoteNodesRoleARN</code> with <code>RemoteNodesTrustAnchorARN</code> or <code>RemoteNodesAnywhereProfileARN</code>, respectively.</p>
</div>
<div class="paragraph">
<p>If you have a pre-existing IAM Roles Anywhere configuration in place, or you are using SSM, you can provide a IAM Role for Hybrid nodes via <code>remoteNetworkConfig.iam.roleARN</code>. Bear in mind that in this scenario, eksctl won&#8217;t create the trust anchor and anywhere profile for you. e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">remoteNetworkConfig:
  iam:
    roleARN: arn:aws:iam::000011112222:role/HybridNodesRole</code></pre>
</div>
</div>
<div class="paragraph">
<p>To map the role to a Kubernetes identity and authorise the remote nodes to join the EKS cluster, eksctl creates an access entry with Hybrid Nodes IAM Role as principal ARN and of type <code>HYBRID_LINUX</code>. i.e.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl get accessentry --cluster my-cluster --principal-arn arn:aws:iam::000011112222:role/eksctl-my-cluster-clust-HybridNodesSSMRole-XiIAg0d29PkO --output json
[
    {
        "principalARN": "arn:aws:iam::000011112222:role/eksctl-my-cluster-clust-HybridNodesSSMRole-XiIAg0d29PkO",
        "kubernetesGroups": [
            "system:nodes"
        ]
    }
]</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="add-ons-support">5.17.4. Add-ons support</h4>
<div class="paragraph">
<p>Container Networking Interface (CNI): The AWS VPC CNI can&#8217;t be used with hybrid nodes. The core capabilities of Cilium and Calico are supported for use with hybrid nodes. You can manage your CNI with your choice of tooling such as Helm. For more information, see <a href="eks/latest/userguide/hybrid-nodes-cni.html">Configure a CNI for hybrid nodes</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you install VPC CNI in your cluster for your self-managed or EKS-managed nodegroups, you have to use <code>v1.19.0-eksbuild.1</code> or later, as this includes an udpate to the add-on&#8217;s daemonset to exclude it from being installed on Hybrid Nodes.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="further-references">5.17.5. Further references</h4>
<div class="ulist">
<ul>
<li>
<p><a href="eks/latest/userguide/hybrid-nodes-overview.html">EKS Hybrid Nodes UserDocs</a></p>
</li>
<li>
<p><a href="https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-eks-hybrid-nodes">Launch Announcement</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="support-for-node-repair-config-in-eks-managed-nodegroups">5.18. Support for Node Repair Config in EKS Managed Nodegroups</h3>
<div class="paragraph">
<p>EKS Managed Nodegroups now supports Node Repair, where the health of managed nodes are monitored,
and unhealthy worker nodes are replaced or rebooted in response.</p>
</div>
<div class="sect3">
<h4 id="creating-a-cluster-a-managed-nodegroup-with-node-repair-enabled">5.18.1. Creating a cluster a managed nodegroup with node repair enabled</h4>
<div class="paragraph">
<p>To create a cluster with a managed nodegroup using node repair, pass the <code>--enable-node-repair</code> flag:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create cluster --enable-node-repair</code></pre>
</div>
</div>
<div class="paragraph">
<p>To create a managed nodegroup using node repair on an existing cluster:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create nodegroup --cluster=&lt;clusterName&gt; --enable-node-repair</code></pre>
</div>
</div>
<div class="paragraph">
<p>To create a cluster with a managed nodegroup using node repair via a config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># node-repair-nodegroup-cluster.yaml
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-44
  region: us-west-2

managedNodeGroups:
- name: ng-1
  nodeRepairConfig:
    enabled: true</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create cluster -f node-repair-nodegroup-cluster.yaml</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="further-information-8">5.18.2. Further information</h4>
<div class="ulist">
<ul>
<li>
<p><a href="eks/latest/userguide/node-health.html">EKS Managed Nodegroup Node Health</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="creating-nodegroups">5.19. Creating nodegroups</h3>
<div class="paragraph">
<p>You can add one or more nodegroups in addition to the initial nodegroup created along with the cluster.</p>
</div>
<div class="paragraph">
<p>To create an additional nodegroup, use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create nodegroup --cluster=&lt;clusterName&gt; [--name=&lt;nodegroupName&gt;]</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><code>--version</code> flag is not supported for managed nodegroups. It always inherits the version from control plane.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="literalblock">
<div class="content">
<pre>By default, new unmanaged nodegroups inherit the version from the control plane (`--version=auto`), but you can specify a different
version e.g. `--version=1.10`, you can also use `--version=latest` to force use of whichever is the latest version.</pre>
</div>
</div>
<div class="paragraph">
<p>Additionally, you can use the same config file used for <code>eksctl create cluster</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create nodegroup --config-file=&lt;path&gt;</pre>
</div>
</div>
<div class="sect3">
<h4 id="creating-a-nodegroup-from-a-config-file">5.19.1. Creating a nodegroup from a config file</h4>
<div class="paragraph">
<p>Nodegroups can also be created through a cluster definition or config file. Given the following example config file
and an existing cluster called <code>dev-cluster</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># dev-cluster.yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: dev-cluster
  region: eu-north-1

managedNodeGroups:
  - name: ng-1-workers
    labels: { role: workers }
    instanceType: m5.xlarge
    desiredCapacity: 10
    volumeSize: 80
    privateNetworking: true
  - name: ng-2-builders
    labels: { role: builders }
    instanceType: m5.2xlarge
    desiredCapacity: 2
    volumeSize: 100
    privateNetworking: true</code></pre>
</div>
</div>
<div class="paragraph">
<p>The nodegroups <code>ng-1-workers</code> and <code>ng-2-builders</code> can be created with this command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl create nodegroup --config-file=dev-cluster.yaml</code></pre>
</div>
</div>
<div class="sect4">
<h5 id="load-balancing">Load Balancing</h5>
<div class="paragraph">
<p>If you have already prepared for attaching existing classic load balancers or/and target groups to the nodegroups,
you can specify these in the config file. The classic load balancers or/and target groups are automatically associated with the ASG when creating nodegroups. This is only supported for self-managed nodegroups defined via the <code>nodeGroups</code> field.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># dev-cluster-with-lb.yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: dev-cluster
  region: eu-north-1

nodeGroups:
  - name: ng-1-web
    labels: { role: web }
    instanceType: m5.xlarge
    desiredCapacity: 10
    privateNetworking: true
    classicLoadBalancerNames:
      - dev-clb-1
      - dev-clb-2
    asgMetricsCollection:
      - granularity: 1Minute
        metrics:
          - GroupMinSize
          - GroupMaxSize
          - GroupDesiredCapacity
          - GroupInServiceInstances
          - GroupPendingInstances
          - GroupStandbyInstances
          - GroupTerminatingInstances
          - GroupTotalInstances
  - name: ng-2-api
    labels: { role: api }
    instanceType: m5.2xlarge
    desiredCapacity: 2
    privateNetworking: true
    targetGroupARNs:
      - arn:aws:elasticloadbalancing:eu-north-1:01234567890:targetgroup/dev-target-group-1/abcdef0123456789</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="nodegroup-selection-in-config-files">5.20. Nodegroup selection in config files</h3>
<div class="paragraph">
<p>To perform a <code>create</code> or <code>delete</code> operation on only a subset of the nodegroups specified in a config file, there are two
CLI flags that accept a list of globs, <code>--include=&lt;glob,glob,...&gt;</code> and <code>--exclude=&lt;glob,glob,...&gt;</code>, e.g.:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create nodegroup --config-file=&lt;path&gt; --include='ng-prod-*-??' --exclude='ng-test-1-ml-a,ng-test-2-?'</pre>
</div>
</div>
<div class="paragraph">
<p>Using the example config file above, one can create all the workers nodegroup except the workers one with the following
command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl create nodegroup --config-file=dev-cluster.yaml --exclude=ng-1-workers</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or one could delete the builders nodegroup with:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl delete nodegroup --config-file=dev-cluster.yaml --include=ng-2-builders --approve</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this case, we also need to supply the <code>--approve</code> command to actually delete the nodegroup.</p>
</div>
<div class="sect3">
<h4 id="node-include">5.20.1. Include and exclude rules</h4>
<div class="ulist">
<ul>
<li>
<p>if no <code>--include</code> or <code>--exclude</code> is specified everything is included</p>
</li>
<li>
<p>if only <code>--include</code> is specified, only nodegroups that match those globs will be included</p>
</li>
<li>
<p>if only <code>--exclude</code> is specified, all nodegroups that do not match those globs are included</p>
</li>
<li>
<p>if both are specified then <code>--exclude</code> rules take precedence over <code>--include</code> (i.e. nodegroups that match rules in
both groups will be excluded)</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="listing-nodegroups">5.21. Listing nodegroups</h3>
<div class="paragraph">
<p>To list the details about a nodegroup or all of the nodegroups, use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl get nodegroup --cluster=&lt;clusterName&gt; [--name=&lt;nodegroupName&gt;]</code></pre>
</div>
</div>
<div class="paragraph">
<p>To list one or more nodegroups in YAML or JSON format, which outputs more info than the default log table, use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash"># YAML format
eksctl get nodegroup --cluster=&lt;clusterName&gt; [--name=&lt;nodegroupName&gt;] --output=yaml

# JSON format
eksctl get nodegroup --cluster=&lt;clusterName&gt; [--name=&lt;nodegroupName&gt;] --output=json</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="nodegroup-immutability">5.22. Nodegroup immutability</h3>
<div class="paragraph">
<p>By design, nodegroups are immutable. This means that if you need to change something (other than scaling) like the
AMI or the instance type of a nodegroup, you would need to create a new nodegroup with the desired changes, move the
load and delete the old one. See the <a href="#nodegroup-delete">Deleting and draining nodegroups</a> section.</p>
</div>
</div>
<div class="sect2">
<h3 id="scaling-nodegroups">5.23. Scaling nodegroups</h3>
<div class="paragraph">
<p>Nodegroup scaling is a process that can take up to a few minutes. When the <code>--wait</code> flag is not specified,
<code>eksctl</code> optimistically expects the nodegroup to be scaled and returns as soon as the AWS API request has been sent. To make
<code>eksctl</code> wait until the nodes are available, add a <code>--wait</code> flag like the example below.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Scaling a nodegroup down/in (i.e. reducing the number of nodes) may result in errors as we rely purely on changes to the ASG. This means that the node(s) being removed/terminated aren&#8217;t explicitly drained. This may be an area for improvement in the future.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Scaling a managed nodegroup is achieved by directly calling the EKS API that updates a managed node group configuration.</p>
</div>
<div class="sect3">
<h4 id="scaling-a-single-nodegroup">5.23.1. Scaling a single nodegroup</h4>
<div class="paragraph">
<p>A nodegroup can be scaled by using the <code>eksctl scale nodegroup</code> command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl scale nodegroup --cluster=&lt;clusterName&gt; --nodes=&lt;desiredCount&gt; --name=&lt;nodegroupName&gt; [ --nodes-min=&lt;minSize&gt; ] [ --nodes-max=&lt;maxSize&gt; ] --wait</pre>
</div>
</div>
<div class="paragraph">
<p>For example, to scale nodegroup <code>ng-a345f4e1</code> in <code>cluster-1</code> to 5 nodes, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl scale nodegroup --cluster=cluster-1 --nodes=5 ng-a345f4e1</pre>
</div>
</div>
<div class="paragraph">
<p>A nodegroup can also be scaled by using a config file passed to <code>--config-file</code> and specifying the name of the nodegroup that should be scaled with <code>--name</code>. Eksctl will search the config file and discover that nodegroup as well as its scaling configuration values.</p>
</div>
<div class="paragraph">
<p>If the desired number of nodes is <code>NOT</code> within the range of current minimum and current maximum number nodes, one specific error will be shown.
These values can also be passed with flags <code>--nodes-min</code> and <code>--nodes-max</code> respectively.</p>
</div>
</div>
<div class="sect3">
<h4 id="scaling-multiple-nodegroups">5.23.2. Scaling multiple nodegroups</h4>
<div class="paragraph">
<p>Eksctl can discover and scale all the nodegroups found in a config file that is passed with <code>--config-file</code>.</p>
</div>
<div class="paragraph">
<p>Similarly to scaling a single nodegroup, the same set of validations apply to each nodegroup. For example, the desired number of nodes must be within the range of the minimum and maximum number of nodes.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="nodegroup-delete">5.24. Deleting and draining nodegroups</h3>
<div class="paragraph">
<p>To delete a nodegroup, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl delete nodegroup --cluster=&lt;clusterName&gt; --name=&lt;nodegroupName&gt;</pre>
</div>
</div>
<div class="paragraph">
<p><a href="#node-include">Include and exclude rules</a> can also be used with this command.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>This will drain all pods from that nodegroup before the instances are deleted.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To skip eviction rules during the drain process, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl delete nodegroup --cluster=&lt;clusterName&gt; --name=&lt;nodegroupName&gt; --disable-eviction</pre>
</div>
</div>
<div class="paragraph">
<p>All nodes are cordoned and all pods are evicted from a nodegroup on deletion,
but if you need to drain a nodegroup without deleting it, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl drain nodegroup --cluster=&lt;clusterName&gt; --name=&lt;nodegroupName&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>To uncordon a nodegroup, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl drain nodegroup --cluster=&lt;clusterName&gt; --name=&lt;nodegroupName&gt; --undo</pre>
</div>
</div>
<div class="paragraph">
<p>To ignore eviction rules such as PodDisruptionBudget settings, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl drain nodegroup --cluster=&lt;clusterName&gt; --name=&lt;nodegroupName&gt; --disable-eviction</pre>
</div>
</div>
<div class="paragraph">
<p>To speed up the drain process you can specify <code>--parallel &lt;value&gt;</code> for the number of nodes to drain in parallel.</p>
</div>
</div>
<div class="sect2">
<h3 id="other-features">5.25. Other features</h3>
<div class="paragraph">
<p>You can also enable SSH, ASG access and other features for a nodegroup, e.g.:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create nodegroup --cluster=cluster-1 --node-labels="autoscaling=enabled,purpose=ci-worker" --asg-access --full-ecr-access --ssh-access</pre>
</div>
</div>
<div class="sect3">
<h4 id="update-labels">5.25.1. Update labels</h4>
<div class="paragraph">
<p>There are no specific commands in <code>eksctl</code> to update the labels of a nodegroup, but it can easily be achieved using
<code>kubectl</code>, e.g.:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">kubectl label nodes -l alpha.eksctl.io/nodegroup-name=ng-1 new-label=foo</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="ssh-access">5.25.2. SSH Access</h4>
<div class="paragraph">
<p>You can enable SSH access for nodegroups by configuring one of <code>publicKey</code>, <code>publicKeyName</code> and <code>publicKeyPath</code> in your
nodegroup configuration. Alternatively you can use <a href="systems-manager/latest/userguide/session-manager-working-with-sessions-start.html#sessions-start-cli">AWS Systems Manager (SSM)</a> to SSH onto nodes, by configuring the nodegroup with <code>enableSsm</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">managedNodeGroups:
  - name: ng-1
    instanceType: m5.large
    desiredCapacity: 1
    ssh: # import public key from file
      publicKeyPath: ~/.ssh/id_rsa_tests.pub
  - name: ng-2
    instanceType: m5.large
    desiredCapacity: 1
    ssh: # use existing EC2 key
      publicKeyName: ec2_dev_key
  - name: ng-3
    instanceType: m5.large
    desiredCapacity: 1
    ssh: # import inline public key
      publicKey: "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDqZEdzvHnK/GVP8nLngRHu/GDi/3PeES7+Bx6l3koXn/Oi/UmM9/jcW5XGziZ/oe1cPJ777eZV7muEvXg5ZMQBrYxUtYCdvd8Rt6DIoSqDLsIPqbuuNlQoBHq/PU2IjpWnp/wrJQXMk94IIrGjY8QHfCnpuMENCucVaifgAhwyeyuO5KiqUmD8E0RmcsotHKBV9X8H5eqLXd8zMQaPl+Ub7j5PG+9KftQu0F/QhdFvpSLsHaxvBzA5nhIltjkaFcwGQnD1rpCM3+UnQE7Izoa5Yt1xoUWRwnF+L2TKovW7+bYQ1kxsuuiX149jXTCJDVjkYCqi7HkrXYqcC1sbsror someuser@hostname"
  - name: ng-4
    instanceType: m5.large
    desiredCapacity: 1
    ssh: # enable SSH using SSM
      enableSsm: true</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="networking">6. Networking</h2>
<div class="sectionbody">
<div class="paragraph">
<p>By default <code>eksctl create cluster</code> will create a dedicated VPC for the cluster.
This is done in order to avoid interference with existing resources for a
variety of reasons, including security, but also because it is challenging to detect all settings in an existing VPC.</p>
</div>
<div class="paragraph">
<p>The default VPC CIDR used by <code>eksctl</code> is <code>192.168.0.0/16</code>. It is divided into 8 (<code>/19</code>) subnets (3 private, 3 public &amp; 2 reserved).
The initial nodegroup is created in public subnets, with SSH access disabled unless <code>--allow-ssh</code> is specified.
The nodegroup by default allows inbound traffic from the control plane security group on ports 1025 - 65535.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In <code>us-east-1</code> eksctl only creates 2 public and 2 private subnets by default.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>From <code>eksctl</code> version <code>0.17.0</code> and onwards public subnets will have the property <code>MapPublicIpOnLaunch</code> enabled, and
the property <code>AssociatePublicIpAddress</code> disabled in the Auto Scaling Group for the nodegroups. This means that when
creating a <strong>new nodegroup</strong> on a <strong>cluster made with an earlier version</strong> of <code>eksctl</code>, the nodegroup must <strong>either</strong> be private
<strong>or</strong> have <code>MapPublicIpOnLaunch</code> enabled in its public subnets. Without one of these, the new nodes won&#8217;t have access to
the internet and won&#8217;t be able to download the basic add-ons (CNI plugin, kube-proxy, etc.). To help set up
subnets correctly for old clusters you can use the new command <code>eksctl utils update-legacy-subnet-settings</code>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>If the default functionality doesn&#8217;t suit you, the following sections explain how to customize VPC configuration further:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="#vpc-configuration">VPC Configuration</a></p>
</li>
<li>
<p><a href="#vpc-subnet-settings">Subnet Settings</a></p>
</li>
<li>
<p><a href="#vpc-cluster-access">Cluster Access</a></p>
</li>
<li>
<p><a href="#vpc-ip-family">IP Family</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>View information about networking in eksctl.</p>
</div>
<div class="sect2 topic">
<h3 id="vpc-configuration">6.1. VPC Configuration</h3>
<div class="sect3">
<h4 id="change-vpc-cidr">6.1.1. Change VPC CIDR</h4>
<div class="paragraph">
<p>If you need to set up peering with another VPC, or simply need a larger or smaller range of IPs, you can use <code>--vpc-cidr</code> flag to
change it. Please refer to <a href="vpc/latest/userguide/VPC_Subnets.html#VPC_Sizing">the AWS docs</a> for guides on choosing CIDR blocks which are permitted for use in an AWS VPC.</p>
</div>
<div class="paragraph">
<p>If you are creating an IPv6 cluster you can also bring your own IPv6 pool by configuring <code>VPC.IPv6Cidr</code> and <code>VPC.IPv6Pool</code>.
See <a href="AWSEC2/latest/UserGuide/ec2-byoip.html">AWS docs</a> on how to import your own pool.</p>
</div>
</div>
<div class="sect3">
<h4 id="use-an-existing-vpc-shared-with-kops">6.1.2. Use an existing VPC: shared with kops</h4>
<div class="paragraph">
<p>You can use the VPC of an existing Kubernetes cluster managed by <a href="https://github.com/kubernetes/kops">kops</a>. This feature is provided to facilitate migration and/or cluster peering.</p>
</div>
<div class="paragraph">
<p>If you have previously created a cluster with kops, e.g. using commands similar to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>export KOPS_STATE_STORE=s3://kops
kops create cluster cluster-1.k8s.local --zones=us-west-2c,us-west-2b,us-west-2a --networking=weave --yes</pre>
</div>
</div>
<div class="paragraph">
<p>You can create an EKS cluster in the same AZs using the same VPC subnets (NOTE: at least 2 AZs/subnets are required):</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create cluster --name=cluster-2 --region=us-west-2 --vpc-from-kops-cluster=cluster-1.k8s.local</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="use-existing-vpc-other-custom-configuration">6.1.3. Use existing VPC: other custom configuration</h4>
<div class="paragraph">
<p><code>eksctl</code> provides some, but not complete, flexibility for custom VPC and subnet topologies.</p>
</div>
<div class="paragraph">
<p>You can use an existing VPC by supplying private and/or public subnets using the <code>--vpc-private-subnets</code> and <code>--vpc-public-subnets</code> flags.
It is up to you to ensure the subnets you use are categorised correctly, as there is no simple way to verify whether a subnet is actually private or
public, because configurations vary.</p>
</div>
<div class="paragraph">
<p>Given these flags, <code>eksctl create cluster</code> will determine the VPC ID automatically, but it will not create any routing tables or other
resources, such as internet/NAT gateways. It will, however, create dedicated security groups for the initial nodegroup and the control
plane.</p>
</div>
<div class="paragraph">
<p>You must ensure to provide <strong>at least 2 subnets in different AZs</strong>. There are other requirements that you will need to follow (listed below), but it&#8217;s
entirely up to you to address those. (For example, tagging is not strictly necessary, tests have shown that it is possible to create
a functional cluster without any tags set on the subnets, however there is no guarantee that this will always hold and tagging is
recommended.)</p>
</div>
<div class="paragraph">
<p>Standard requirements:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>all given subnets must be in the same VPC, within the same block of IPs</p>
</li>
<li>
<p>a sufficient number IP addresses are available, based on needs</p>
</li>
<li>
<p>sufficient number of subnets (minimum 2), based on needs</p>
</li>
<li>
<p>subnets are tagged with at least the following:</p>
<div class="ulist">
<ul>
<li>
<p><code>kubernetes.io/cluster/&lt;name&gt;</code> tag set to either <code>shared</code> or <code>owned</code></p>
</li>
<li>
<p><code>kubernetes.io/role/internal-elb</code> tag set to <code>1</code> for <em>private</em> subnets</p>
</li>
<li>
<p><code>kubernetes.io/role/elb</code> tag set to <code>1</code> for <em>public</em> subnets</p>
</li>
</ul>
</div>
</li>
<li>
<p>correctly configured internet and/or NAT gateways</p>
</li>
<li>
<p>routing tables have correct entries and the network is functional</p>
</li>
<li>
<p><strong>NEW</strong>: all public subnets should have the property <code>MapPublicIpOnLaunch</code> enabled (i.e. <code>Auto-assign public IPv4 address</code> in the AWS console)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>There may be other requirements imposed by EKS or Kubernetes, and it is entirely up to you to stay up-to-date on any requirements and/or
recommendations, and implement those as needed/possible.</p>
</div>
<div class="paragraph">
<p>Default security group settings applied by <code>eksctl</code> may or may not be sufficient for sharing access with resources in other security
groups. If you wish to modify the ingress/egress rules of the security groups, you might need to use another tool to automate
changes, or do it via EC2 console.</p>
</div>
<div class="paragraph">
<p>When in doubt, don&#8217;t use a custom VPC. Using <code>eksctl create cluster</code> without any <code>--vpc-*</code> flags will always configure the cluster
with a fully-functional dedicated VPC.</p>
</div>
<div class="paragraph">
<p><strong>Examples</strong></p>
</div>
<div class="paragraph">
<p>Create a cluster using a custom VPC with 2x private and 2x public subnets:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create cluster \
  --vpc-private-subnets=subnet-0ff156e0c4a6d300c,subnet-0426fb4a607393184 \
  --vpc-public-subnets=subnet-0153e560b3129a696,subnet-009fa0199ec203c37</pre>
</div>
</div>
<div class="paragraph">
<p>or use the following equivalent config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: my-test
  region: us-west-2

vpc:
  id: "vpc-11111"
  subnets:
    private:
      us-west-2a:
          id: "subnet-0ff156e0c4a6d300c"
      us-west-2c:
          id: "subnet-0426fb4a607393184"
    public:
      us-west-2a:
          id: "subnet-0153e560b3129a696"
      us-west-2c:
          id: "subnet-009fa0199ec203c37"

nodeGroups:
  - name: ng-1</code></pre>
</div>
</div>
<div class="paragraph">
<p>Create a cluster using a custom VPC with 3x private subnets and make initial nodegroup use those subnets:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create cluster \
  --vpc-private-subnets=subnet-0ff156e0c4a6d300c,subnet-0549cdab573695c03,subnet-0426fb4a607393184 \
  --node-private-networking</pre>
</div>
</div>
<div class="paragraph">
<p>or use the following equivalent config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: my-test
  region: us-west-2

vpc:
  id: "vpc-11111"
  subnets:
    private:
      us-west-2d:
          id: "subnet-0ff156e0c4a6d300c"
      us-west-2c:
          id: "subnet-0549cdab573695c03"
      us-west-2a:
          id: "subnet-0426fb4a607393184"

nodeGroups:
  - name: ng-1
    privateNetworking: true</code></pre>
</div>
</div>
<div class="paragraph">
<p>Create a cluster using a custom VPC 4x public subnets:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create cluster \
  --vpc-public-subnets=subnet-0153e560b3129a696,subnet-0cc9c5aebe75083fd,subnet-009fa0199ec203c37,subnet-018fa0176ba320e45</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: my-test
  region: us-west-2

vpc:
  id: "vpc-11111"
  subnets:
    public:
      us-west-2d:
          id: "subnet-0153e560b3129a696"
      us-west-2c:
          id: "subnet-0cc9c5aebe75083fd"
      us-west-2a:
          id: "subnet-009fa0199ec203c37"
      us-west-2b:
          id: "subnet-018fa0176ba320e45"

nodeGroups:
  - name: ng-1</code></pre>
</div>
</div>
<div class="paragraph">
<p>More examples can be found in the repo&#8217;s <code>examples</code> folder:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://github.com/eksctl-io/eksctl/blob/master/examples/04-existing-vpc.yaml">using an existing VPC</a></p>
</li>
<li>
<p><a href="https://github.com/eksctl-io/eksctl/blob/master/examples/02-custom-vpc-cidr-no-nodes.yaml">using a custom VPC CIDR</a></p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="custom-shared-node-security-group">6.1.4. Custom Shared Node Security Group</h4>
<div class="paragraph">
<p><code>eksctl</code> will create and manage a shared node security group that allows communication between
unmanaged nodes and the cluster control plane and managed nodes.</p>
</div>
<div class="paragraph">
<p>If you wish to provide your own custom security group instead, you may override the <code>sharedNodeSecurityGroup</code>
field in the config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">vpc:
  sharedNodeSecurityGroup: sg-0123456789</code></pre>
</div>
</div>
<div class="paragraph">
<p>By default, when creating the cluster, <code>eksctl</code> will add rules to this security group to allow communication to and
from the default cluster security group that EKS creates. The default cluster security group is used by both
the EKS control plane and managed node groups.</p>
</div>
<div class="paragraph">
<p>If you wish to manage the security group rules yourself, you may prevent <code>eksctl</code> from creating the rules
by setting <code>manageSharedNodeSecurityGroupRules</code> to <code>false</code> in the config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">vpc:
  sharedNodeSecurityGroup: sg-0123456789
  manageSharedNodeSecurityGroupRules: false</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="nat-gateway">6.1.5. NAT Gateway</h4>
<div class="paragraph">
<p>The NAT Gateway for a cluster can be configured to be <code>Disable</code>, <code>Single</code> (default) or <code>HighlyAvailable</code>.
The <code>HighlyAvailable</code> option will deploy a NAT Gateway in each Availability Zone of the Region, so that if
an AZ is down, nodes in the other AZs will still be able to communicate to the Internet.</p>
</div>
<div class="paragraph">
<p>It can be specified through the <code>--vpc-nat-mode</code> CLI flag or in the cluster config file like the example below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">vpc:
  nat:
    gateway: HighlyAvailable # other options: Disable, Single (default)</code></pre>
</div>
</div>
<div class="paragraph">
<p>See the complete example <a href="https://github.com/eksctl-io/eksctl/blob/master/examples/09-nat-gateways.yaml">here</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Specifying the NAT Gateway is only supported during cluster creation. It isn&#8217;t touched during a cluster
upgrade. There are plans to support changing between different modes on cluster update in the future.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="vpc-subnet-settings">6.2. Subnet Settings</h3>
<div class="sect3">
<h4 id="use-private-subnets-for-initial-nodegroup">6.2.1. Use private subnets for initial nodegroup</h4>
<div class="paragraph">
<p>If you prefer to isolate the initial nodegroup from the public internet, you can use the <code>--node-private-networking</code> flag.
When used in conjunction with the <code>--ssh-access</code> flag, the SSH port can only be accessed from inside the VPC.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Using the <code>--node-private-networking</code> flag will result in outgoing traffic to go through the NAT gateway using its
Elastic IP. On the other hand, if the nodes are in a public subnet, the outgoing traffic won&#8217;t go through the
NAT gateway and hence the outgoing traffic has the IP of each individual node.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="custom-subnet-topology">6.2.2. Custom subnet topology</h4>
<div class="paragraph">
<p><code>eksctl</code> version <code>0.32.0</code> introduced further subnet topology customisation with the ability to:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>List multiple subnets per AZ in VPC configuration</p>
</li>
<li>
<p>Specify subnets in nodegroup configuration</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In earlier versions custom subnets had to be provided by availability zone, meaning just one subnet per AZ could be listed.
From <code>0.32.0</code> the identifying keys can be arbitrary.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">vpc:
  id: "vpc-11111"
  subnets:
    public:
      public-one:                           # arbitrary key
          id: "subnet-0153e560b3129a696"
      public-two:
          id: "subnet-0cc9c5aebe75083fd"
      us-west-2b:                           # or list by AZ
          id: "subnet-018fa0176ba320e45"
    private:
      private-one:
          id: "subnet-0153e560b3129a696"
      private-two:
          id: "subnet-0cc9c5aebe75083fd"</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If using the AZ as the identifying key, the <code>az</code> value can be omitted.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="literalblock">
<div class="content">
<pre>If using an arbitrary string as the identifying key, like above, either:

* `id` must be set (`az` and `cidr` optional)
* or `az` must be set (`cidr` optional)

If a user specifies a subnet by AZ without specifying CIDR and ID, a subnet
in that AZ will be chosen from the VPC, arbitrarily if multiple such subnets
exist.</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>A complete subnet spec must be provided, i.e. both <code>public</code> and <code>private</code> configurations
declared in the VPC spec.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Nodegroups can be restricted to named subnets via the configuration.
When specifying subnets on nodegroup configuration, use the identifying key as given in the VPC spec <strong>not</strong> the subnet id.
For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">vpc:
  id: "vpc-11111"
  subnets:
    public:
      public-one:
          id: "subnet-0153e560b3129a696"
    ... # subnet spec continued

nodeGroups:
  - name: ng-1
    instanceType: m5.xlarge
    desiredCapacity: 2
    subnets:
      - public-one</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Only one of <code>subnets</code> or <code>availabilityZones</code> can be provided in nodegroup configuration.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>When placing nodegroups inside a private subnet, <code>privateNetworking</code> must be set to <code>true</code>
on the nodegroup:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">vpc:
  id: "vpc-11111"
  subnets:
    public:
      private-one:
          id: "subnet-0153e560b3129a696"
    ... # subnet spec continued

nodeGroups:
  - name: ng-1
    instanceType: m5.xlarge
    desiredCapacity: 2
    privateNetworking: true
    subnets:
      - private-one</code></pre>
</div>
</div>
<div class="paragraph">
<p>See <a href="https://github.com/eksctl-io/eksctl/blob/master/examples/24-nodegroup-subnets.yaml">here</a> for a full
configuration example.</p>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="vpc-cluster-access">6.3. Cluster Access</h3>
<div class="sect3">
<h4 id="managing-access-to-the-kubernetes-api-server-endpoints">6.3.1. Managing Access to the Kubernetes API Server Endpoints</h4>
<div class="paragraph">
<p>The default creation of an EKS cluster exposes the Kubernetes API server publicly but not directly from within the
VPC subnets (public=true, private=false). Traffic destined for the API server from within the VPC must first exit the
VPC networks (but not Amazon&#8217;s network) and then re-enter to reach the API server.</p>
</div>
<div class="paragraph">
<p>The Kubernetes API server endpoint access for a cluster can be configured for public and private access when creating
the cluster using the cluster config file. Example below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">vpc:
  clusterEndpoints:
    publicAccess:  &lt;true|false&gt;
    privateAccess: &lt;true|false&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>There are some additional caveats when configuring Kubernetes API endpoint access:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>EKS doesn&#8217;t allow one to create or update a cluster without at least one private or public access being
enabled.</p>
</li>
<li>
<p>EKS does allow creating a configuration that allows only private access to be enabled, but eksctl doesn&#8217;t
support it during cluster creation as it prevents eksctl from being able to join the worker nodes to the cluster.</p>
</li>
<li>
<p>Updating a cluster to have private only Kubernetes API endpoint access means that Kubernetes commands, by default,
(e.g. <code>kubectl</code>) as well as <code>eksctl delete cluster</code>, <code>eksctl utils write-kubeconfig</code>, and possibly the command
<code>eksctl utils update-kube-proxy</code> must be run within the cluster VPC.  This requires some changes to various AWS
resources.  See:
<a href="en_pv/eks/latest/userguide/cluster-endpoint">EKS user guide</a>
A user can provide <code>vpc.extraCIDRs</code> which will append additional CIDR ranges to the ControlPlaneSecurityGroup,
allowing subnets outside the VPC to reach the kubernetes API endpoint. Similarly you can provide <code>vpc.extraIPv6CIDRs</code>
to append IPv6 CIDR ranges as well.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>The following is an example of how one could configure the Kubernetes API endpoint access using the <code>utils</code> sub-command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils update-cluster-vpc-config --cluster=&lt;clustername&gt; --private-access=true --public-access=false</code></pre>
</div>
</div>
<div class="paragraph">
<p>!!! warning
    <code>eksctl utils update-cluster-endpoints</code> has been deprecated in favour of <code>eksctl utils update-cluster-vpc-config</code>
    and will be removed soon.</p>
</div>
<div class="paragraph">
<p>To update the setting using a <code>ClusterConfig</code> file, use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils update-cluster-vpc-config -f config.yaml --approve</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note that if you don&#8217;t pass a flag, it will keep the current value. Once you are satisfied with the proposed changes,
add the <code>approve</code> flag to make the change to the running cluster.</p>
</div>
</div>
<div class="sect3">
<h4 id="restricting-access-to-the-eks-kubernetes-public-api-endpoint">6.3.2. Restricting Access to the EKS Kubernetes Public API endpoint</h4>
<div class="paragraph">
<p>The default creation of an EKS cluster exposes the Kubernetes API server publicly. To restrict access to the public API
endpoint to a set of CIDRs when creating a cluster, set the <code>publicAccessCIDRs</code> field:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">vpc:
  publicAccessCIDRs: ["1.1.1.1/32", "2.2.2.0/24"]</code></pre>
</div>
</div>
<div class="paragraph">
<p>To update the restrictions on an existing cluster, use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils update-cluster-vpc-config --cluster=&lt;cluster&gt; 1.1.1.1/32,2.2.2.0/24</code></pre>
</div>
</div>
<div class="paragraph">
<p>!!! warning
    <code>eksctl utils set-public-access-cidrs</code> has been deprecated in favour of <code>eksctl utils update-cluster-vpc-config</code>
    and will be removed soon.</p>
</div>
<div class="paragraph">
<p>To update the restrictions using a <code>ClusterConfig</code> file, set the new CIDRs in <code>vpc.publicAccessCIDRs</code> and run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils update-cluster-vpc-config -f config.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>!!! warning
    If setting <code>publicAccessCIDRs</code> and creating node-groups either <code>privateAccess</code> should be set to <code>true</code> or
    the nodes' IPs should be added to the <code>publicAccessCIDRs</code> list. Otherwise creation will fail with
    <code>context deadline exceeded</code> due to the nodes being unable to access the public endpoint and hence failing
    to join the cluster.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>This feature only applies to the public endpoint. The
<a href="eks/latest/userguide/cluster-endpoint.html">API server endpoint access configuration options</a>
won&#8217;t change, and you will still have the option to disable the public endpoint so your cluster is not accessible from
the internet. (Source: <a href="https://github.com/aws/containers-roadmap/issues/108#issuecomment-552766489" class="bare">https://github.com/aws/containers-roadmap/issues/108#issuecomment-552766489</a>)</p>
</div>
</td>
</tr>
</table>
</div>
<div class="literalblock">
<div class="content">
<pre>Implementation notes: https://github.com/aws/containers-roadmap/issues/108#issuecomment-552698875</pre>
</div>
</div>
<div class="paragraph">
<p>To update both API server endpoint access and public access CIDRs for a cluster in a single command, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils update-cluster-vpc-config --cluster=&lt;cluster&gt; --public-access=true --private-access=true --public-access-cidrs=1.1.1.1/32,2.2.2.0/24</code></pre>
</div>
</div>
<div class="paragraph">
<p>To update the setting using a config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">vpc:
  clusterEndpoints:
    publicAccess:  &lt;true|false&gt;
    privateAccess: &lt;true|false&gt;
  publicAccessCIDRs: ["1.1.1.1/32"]</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils update-cluster-vpc-config --cluster=&lt;cluster&gt; -f config.yaml</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="updating-control-plane-subnets-and-security-groups">6.4. Updating control plane subnets and security groups</h3>
<div class="sect3">
<h4 id="updating-control-plane-subnets">6.4.1. Updating control plane subnets</h4>
<div class="paragraph">
<p>When a cluster is created with eksctl, a set of public and private subnets are created and passed to the EKS API.
EKS creates 2 to 4 cross-account elastic network interfaces (ENIs) in those subnets to enable communication between the EKS
managed Kubernetes control plane and your VPC.</p>
</div>
<div class="paragraph">
<p>To update the subnets used by the EKS control plane, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils update-cluster-vpc-config --cluster=&lt;cluster&gt; --control-plane-subnet-ids=subnet-1234,subnet-5678</code></pre>
</div>
</div>
<div class="paragraph">
<p>To update the setting using a config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: cluster
  region: us-west-2

vpc:
  controlPlaneSubnetIDs: [subnet-1234, subnet-5678]</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils update-cluster-vpc-config -f config.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Without the <code>--approve</code> flag, eksctl only logs the proposed changes. Once you are satisfied with the proposed changes, rerun the command with
the  <code>--approve</code> flag.</p>
</div>
</div>
<div class="sect3">
<h4 id="updating-control-plane-security-groups">6.4.2. Updating control plane security groups</h4>
<div class="paragraph">
<p>To manage traffic between the control plane and worker nodes, EKS supports passing additional security groups that are applied to the cross-account network interfaces
provisioned by EKS. To update the security groups for the EKS control plane, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils update-cluster-vpc-config --cluster=&lt;cluster&gt; --control-plane-security-group-ids=sg-1234,sg-5678 --approve</code></pre>
</div>
</div>
<div class="paragraph">
<p>To update the setting using a config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: cluster
  region: us-west-2

vpc:
  controlPlaneSecurityGroupIDs: [sg-1234, sg-5678]</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils update-cluster-vpc-config -f config.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>To update both control plane subnets and security groups for a cluster, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils update-cluster-vpc-config --cluster=&lt;cluster&gt; --control-plane-subnet-ids=&lt;&gt; --control-plane-security-group-ids=&lt;&gt; --approve</code></pre>
</div>
</div>
<div class="paragraph">
<p>To update both fields using a config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: cluster
  region: us-west-2

vpc:
  controlPlaneSubnetIDs: [subnet-1234, subnet-5678]
  controlPlaneSecurityGroupIDs: [sg-1234, sg-5678]</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils update-cluster-vpc-config -f config.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>For a complete example, refer to <a href="https://github.com/eksctl-io/eksctl/blob/main/examples/38-cluster-subnets-sgs.yaml">cluster-subnets-sgs.yaml</a>.</p>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="vpc-ip-family">6.5. IPv6 Support</h3>
<div class="sect3">
<h4 id="define-ip-family">6.5.1. Define IP Family</h4>
<div class="paragraph">
<p>When <code>eksctl</code> creates a vpc, you can define the IP version that will be used. The following options are available to be configured:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>IPv4</p>
</li>
<li>
<p>IPv6</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To define it, use the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: my-test
  region: us-west-2
  version: "1.21"

kubernetesNetworkConfig:
  ipFamily: IPv6 # or IPv4

addons:
  - name: vpc-cni
  - name: coredns
  - name: kube-proxy

iam:
  withOIDC: true</code></pre>
</div>
</div>
<div class="paragraph">
<p>This is an in config file setting only. When IPv6 is set, the following restriction must be followed:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>OIDC is enabled</p>
</li>
<li>
<p>managed addons are defined as shows above</p>
</li>
<li>
<p>cluster version must be =&gt; 1.21</p>
</li>
<li>
<p>vpc-cni addon version must be =&gt; 1.10.0</p>
</li>
<li>
<p>unmanaged nodegroups are not yet supported with IPv6 clusters</p>
</li>
<li>
<p>managed nodegroup creation is not supported with un-owned IPv6 clusters</p>
</li>
<li>
<p><code>vpc.NAT</code> and <code>serviceIPv4CIDR</code> fields are created by eksctl for ipv6 clusters and thus, are not supported configuration options</p>
</li>
<li>
<p>AutoAllocateIPv6 is not supported together with IPv6</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The default value is <code>IPv4</code>.</p>
</div>
<div class="paragraph">
<p>Private networking can be done with IPv6 IP family as well. Please follow the instruction outlined under <a href="#eks-private-cluster">EKS Private Cluster</a>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="iam">7. IAM</h2>
<div class="sectionbody">
<div class="paragraph">
<p>View information about iam in eksctl.</p>
</div>
<div class="sect2 topic">
<h3 id="minimum-iam-policies">7.1. Minimum IAM policies</h3>
<div class="paragraph">
<p>This document describes the minimum IAM policies needed to run the main use cases of eksctl. These are the ones used to
run the integration tests.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
remember to replace <code>&lt;account_id&gt;</code> with your own.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>An AWS Managed Policy is created and administered by AWS. You cannot change the permissions defined in AWS managed policies.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>AmazonEC2FullAccess (AWS Managed Policy)</p>
</div>
<div class="listingblock">
<div class="content">
<pre>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": "ec2:*",
            "Effect": "Allow",
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": "elasticloadbalancing:*",
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": "cloudwatch:*",
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": "autoscaling:*",
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": "iam:CreateServiceLinkedRole",
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "iam:AWSServiceName": [
                        "autoscaling.amazonaws.com",
                        "ec2scheduled.amazonaws.com",
                        "elasticloadbalancing.amazonaws.com",
                        "spot.amazonaws.com",
                        "spotfleet.amazonaws.com",
                        "transitgateway.amazonaws.com"
                    ]
                }
            }
        }
    ]
}</pre>
</div>
</div>
<div class="paragraph">
<p>AWSCloudFormationFullAccess (AWS Managed Policy)</p>
</div>
<div class="listingblock">
<div class="content">
<pre>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "cloudformation:*"
            ],
            "Resource": "*"
        }
    ]
}</pre>
</div>
</div>
<div class="paragraph">
<p>EksAllAccess</p>
</div>
<div class="listingblock">
<div class="content">
<pre>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "eks:*",
            "Resource": "*"
        },
        {
            "Action": [
                "ssm:GetParameter",
                "ssm:GetParameters"
            ],
            "Resource": [
                "arn:aws:ssm:*:&lt;account_id&gt;:parameter/aws/*",
                "arn:aws:ssm:*::parameter/aws/*"
            ],
            "Effect": "Allow"
        },
        {
             "Action": [
               "kms:CreateGrant",
               "kms:DescribeKey"
             ],
             "Resource": "*",
             "Effect": "Allow"
        },
        {
             "Action": [
               "logs:PutRetentionPolicy"
             ],
             "Resource": "*",
             "Effect": "Allow"
        }
    ]
}</pre>
</div>
</div>
<div class="paragraph">
<p>IamLimitedAccess</p>
</div>
<div class="listingblock">
<div class="content">
<pre>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "iam:CreateInstanceProfile",
                "iam:DeleteInstanceProfile",
                "iam:GetInstanceProfile",
                "iam:RemoveRoleFromInstanceProfile",
                "iam:GetRole",
                "iam:CreateRole",
                "iam:DeleteRole",
                "iam:AttachRolePolicy",
                "iam:PutRolePolicy",
                "iam:UpdateAssumeRolePolicy",
                "iam:AddRoleToInstanceProfile",
                "iam:ListInstanceProfilesForRole",
                "iam:PassRole",
                "iam:DetachRolePolicy",
                "iam:DeleteRolePolicy",
                "iam:GetRolePolicy",
                "iam:GetOpenIDConnectProvider",
                "iam:CreateOpenIDConnectProvider",
                "iam:DeleteOpenIDConnectProvider",
                "iam:TagOpenIDConnectProvider",
                "iam:ListAttachedRolePolicies",
                "iam:TagRole",
                "iam:UntagRole",
                "iam:GetPolicy",
                "iam:CreatePolicy",
                "iam:DeletePolicy",
                "iam:ListPolicyVersions"
            ],
            "Resource": [
                "arn:aws:iam::&lt;account_id&gt;:instance-profile/eksctl-*",
                "arn:aws:iam::&lt;account_id&gt;:role/eksctl-*",
                "arn:aws:iam::&lt;account_id&gt;:policy/eksctl-*",
                "arn:aws:iam::&lt;account_id&gt;:oidc-provider/*",
                "arn:aws:iam::&lt;account_id&gt;:role/aws-service-role/eks-nodegroup.amazonaws.com/AWSServiceRoleForAmazonEKSNodegroup",
                "arn:aws:iam::&lt;account_id&gt;:role/eksctl-managed-*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:GetRole",
                "iam:GetUser"
            ],
            "Resource": [
                "arn:aws:iam::&lt;account_id&gt;:role/*",
                "arn:aws:iam::&lt;account_id&gt;:user/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "iam:CreateServiceLinkedRole"
            ],
            "Resource": "*",
            "Condition": {
                "StringEquals": {
                    "iam:AWSServiceName": [
                        "eks.amazonaws.com",
                        "eks-nodegroup.amazonaws.com",
                        "eks-fargate.amazonaws.com"
                    ]
                }
            }
        }
    ]
}</pre>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="iam-permissions-boundary">7.2. IAM permissions boundary</h3>
<div class="paragraph">
<p>A <a href="IAM/latest/UserGuide/access_policies_boundaries.html">permissions boundary</a> is an advanced AWS IAM feature in which the maximum permissions that an identity-based policy can grant to an IAM entity have been set; where those entities are either users or roles. When a permissions boundary is set for an entity, that entity can only perform the actions that are allowed by both its identity-based policies and its permissions boundaries.</p>
</div>
<div class="paragraph">
<p>You can provide your permissions boundary so that all identity-based entities created by eksctl are created within that boundary. This example demonstrates how a permissions boundary can be provided to the various identity-based entities that are created by eksctl:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-17
  region: us-west-2

iam:
  withOIDC: true
  serviceRolePermissionsBoundary: "arn:aws:iam::11111:policy/entity/boundary"
  fargatePodExecutionRolePermissionsBoundary: "arn:aws:iam::11111:policy/entity/boundary"
  serviceAccounts:
    - metadata:
        name: s3-reader
      attachPolicyARNs:
      - "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
      permissionsBoundary: "arn:aws:iam::11111:policy/entity/boundary"

nodeGroups:
  - name: "ng-1"
    desiredCapacity: 1
    iam:
      instanceRolePermissionsBoundary: "arn:aws:iam::11111:policy/entity/boundary"</code></pre>
</div>
</div>
<div class="paragraph">
<p>!!! warning
    It is not possible to provide both a role ARN and a permissions boundary!</p>
</div>
<div class="sect3">
<h4 id="setting-the-vpc-cni-permission-boundary">7.2.1. Setting the VPC CNI Permission Boundary</h4>
<div class="paragraph">
<p>Please note that when you create a cluster with OIDC enabled eksctl will automatically create an <code>iamserviceaccount</code> for the VPC-CNI for <a href="#security">security reasons</a>. If
you would like to add a permission boundary to it then you must specify the <code>iamserviceaccount</code> in your config file manually:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">iam:
  serviceAccounts:
    - metadata:
        name: aws-node
        namespace: kube-system
      attachPolicyARNs:
      - "arn:aws:iam::&lt;arn&gt;:policy/AmazonEKS_CNI_Policy"
      permissionsBoundary: "arn:aws:iam::11111:policy/entity/boundary"</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="iam-policies">7.3. IAM policies</h3>
<div class="sect3">
<h4 id="supported-iam-add-on-policies">7.3.1. Supported IAM add-on policies</h4>
<div class="paragraph">
<p>Example of all supported add-on policies:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: ng-1
    instanceType: m5.xlarge
    desiredCapacity: 1
    iam:
      withAddonPolicies:
        imageBuilder: true
        autoScaler: true
        externalDNS: true
        certManager: true
        appMesh: true
        appMeshPreview: true
        ebs: true
        fsx: true
        efs: true
        awsLoadBalancerController: true
        xRay: true
        cloudWatch: true</code></pre>
</div>
</div>
<div class="sect4">
<h5 id="image-builder-policy">Image Builder Policy</h5>
<div class="paragraph">
<p>The <code>imageBuilder</code> policy allows for full ECR (Elastic Container Registry) access. This is useful for building, for
example, a CI server that needs to push images to ECR.</p>
</div>
</div>
<div class="sect4">
<h5 id="ebs-policy">EBS Policy</h5>
<div class="paragraph">
<p>The <code>ebs</code> policy enables the new EBS CSI (Elastic Block Store Container Storage Interface) driver.</p>
</div>
</div>
<div class="sect4">
<h5 id="cert-manager-policy">Cert Manager Policy</h5>
<div class="paragraph">
<p>The <code>certManager</code> policy enables the ability to add records to Route 53 in order to solve the DNS01 challenge. More information can be found <a href="https://cert-manager.io/docs/configuration/acme/dns01/route53/#set-up-a-iam-role">here</a>.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="adding-a-custom-instance-role">7.3.2. Adding a custom instance role</h4>
<div class="paragraph">
<p>This example creates a nodegroup that reuses an existing IAM Instance Role from another cluster:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha4
kind: ClusterConfig
metadata:
  name: test-cluster-c-1
  region: eu-north-1

nodeGroups:
  - name: ng2-private
    instanceType: m5.large
    desiredCapacity: 1
    iam:
      instanceProfileARN: "arn:aws:iam::123:instance-profile/eksctl-test-cluster-a-3-nodegroup-ng2-private-NodeInstanceProfile-Y4YKHLNINMXC"
      instanceRoleARN: "arn:aws:iam::123:role/eksctl-test-cluster-a-3-nodegroup-NodeInstanceRole-DNGMQTQHQHBJ"</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="attaching-inline-policies">7.3.3. Attaching inline policies</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: my-special-nodegroup
    iam:
      attachPolicy:
        Version: "2012-10-17"
        Statement:
        - Effect: Allow
          Action:
          - 's3:GetObject'
          Resource: 'arn:aws:s3:::example-bucket/*'</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="attaching-policies-by-arn">7.3.4. Attaching policies by ARN</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">nodeGroups:
  - name: my-special-nodegroup
    iam:
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/ElasticLoadBalancingFullAccess
        - arn:aws:iam::1111111111:policy/kube2iam
      withAddonPolicies:
        autoScaler: true
        imageBuilder: true</code></pre>
</div>
</div>
<div class="paragraph">
<p>!!! warning
    If a nodegroup includes the <code>attachPolicyARNs</code> it <strong>must</strong> also include the default node policies, like <code>AmazonEKSWorkerNodePolicy</code>, <code>AmazonEKS_CNI_Policy</code> and <code>AmazonEC2ContainerRegistryReadOnly</code> in this example.</p>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="manage-iam-users-and-roles">7.4. Manage IAM users and roles</h3>
<div class="paragraph">
<p>EKS clusters use IAM users and roles to control access to the cluster. The rules are implemented in a config map
called <code>aws-auth</code>. <code>eksctl</code> provides commands to read and edit this config map.</p>
</div>
<div class="paragraph">
<p>Get all identity mappings:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl get iamidentitymapping --cluster &lt;clusterName&gt; --region=&lt;region&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Get all identity mappings matching an arn:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl get iamidentitymapping --cluster &lt;clusterName&gt; --region=&lt;region&gt; --arn arn:aws:iam::123456:role/testing-role</code></pre>
</div>
</div>
<div class="paragraph">
<p>Create an identity mapping:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash"> eksctl create iamidentitymapping --cluster  &lt;clusterName&gt; --region=&lt;region&gt; --arn arn:aws:iam::123456:role/testing --group system:masters --username admin</code></pre>
</div>
</div>
<div class="paragraph">
<p>The identity mappings can also be specified in ClusterConfig:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-with-iamidentitymappings
  region: us-east-1

iamIdentityMappings:
  - arn: arn:aws:iam::000000000000:role/myAdminRole
    groups:
      - system:masters
    username: admin
    noDuplicateARNs: true # prevents shadowing of ARNs

  - arn: arn:aws:iam::000000000000:user/myUser
    username: myUser
    noDuplicateARNs: true # prevents shadowing of ARNs

  - serviceName: emr-containers
    namespace: emr # serviceName requires namespace

  - account: "000000000000" # account must be configured with no other options

nodeGroups:
  - name: ng-1
    instanceType: m5.large
    desiredCapacity: 1</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash"> eksctl create iamidentitymapping -f cluster-with-iamidentitymappings.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Delete an identity mapping:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl delete iamidentitymapping --cluster  &lt;clusterName&gt; --region=&lt;region&gt; --arn arn:aws:iam::123456:role/testing</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Above command deletes a single mapping FIFO unless <code>--all</code> is given in which case it removes all matching. Will warn if
more mappings matching this role are found.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Create an account mapping:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash"> eksctl create iamidentitymapping --cluster  &lt;clusterName&gt; --region=&lt;region&gt; --account user-account</code></pre>
</div>
</div>
<div class="paragraph">
<p>Delete an account mapping:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash"> eksctl delete iamidentitymapping --cluster  &lt;clusterName&gt; --region=&lt;region&gt; --account user-account</code></pre>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="iamserviceaccounts">7.5. IAM Roles for Service Accounts</h3>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>check out newer, improved feature</strong></p>
</div>
<div class="paragraph">
<p><code>eksctl</code> supports configuring fine-grained permissions to EKS running apps via <a href="#pod-id">EKS Pod Identity Associations</a></p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="introduction-4">7.5.1. Introduction</h4>
<div class="paragraph">
<p>Amazon EKS supports <a href="eks/latest/userguide/access-policies.html#access-policy-permissions">here</a> Roles for Service Accounts (IRSA)] that allows cluster operators to map AWS IAM Roles to Kubernetes Service Accounts.</p>
</div>
<div class="paragraph">
<p>This provides fine-grained permission management for apps that run on EKS and use other AWS services. These could be apps that use S3,
any other data services (RDS, MQ, STS, DynamoDB), or Kubernetes components like AWS Load Balancer controller or ExternalDNS.</p>
</div>
<div class="paragraph">
<p>You can easily create IAM Role and Service Account pairs with <code>eksctl</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you used <a href="#iam-policies">instance roles</a>, and are considering to use IRSA instead, you shouldn&#8217;t mix the two.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="iam-how-works">7.5.2. How it works</h4>
<div class="paragraph">
<p>It works via IAM OpenID Connect Provider (OIDC) that EKS exposes, and IAM Roles must be constructed with reference to the IAM OIDC Provider (specific to a given EKS cluster), and a reference to the Kubernetes Service Account it will be bound to.
Once an IAM Role is created, a service account should include the ARN of that role as an annotation (<code>eks.amazonaws.com/role-arn</code>).
By default the service account will be created or updated to include the role annotation, this can be disabled using the flag <code>--role-only</code>.</p>
</div>
<div class="paragraph">
<p>Inside EKS, there is an <a href="https://github.com/aws/amazon-eks-pod-identity-webhook/">admission controller</a> that injects AWS session credentials into pods respectively of the roles based on the annotation on the Service Account used by the pod. The credentials will get exposed by <code>AWS_ROLE_ARN</code> &amp; <code>AWS_WEB_IDENTITY_TOKEN_FILE</code> environment variables. Given a recent version of AWS SDK is used (see <a href="eks/latest/userguide/access-policies.html#access-policy-permissions">here</a> documentation] for details of exact version), the application will use these credentials.</p>
</div>
<div class="paragraph">
<p>In <code>eksctl</code> the name of the resource is <em>iamserviceaccount</em>, which represents an IAM Role and Service Account pair.</p>
</div>
<div class="sect4">
<h5 id="usage-without-config-files">Usage without config files</h5>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>IAM Roles for Service Accounts require Kubernetes version 1.13 or above.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The IAM OIDC Provider is not enabled by default, you can use the following command to enable it, or use config file (see below):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils associate-iam-oidc-provider --cluster=&lt;clusterName&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once you have the IAM OIDC Provider associated with the cluster, to create a IAM role bound to a service account, run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create iamserviceaccount --cluster=&lt;clusterName&gt; --name=&lt;serviceAccountName&gt; --namespace=&lt;serviceAccountNamespace&gt; --attach-policy-arn=&lt;policyARN&gt;</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You can specify <code>--attach-policy-arn</code> multiple times to use more than one policy.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>More specifically, you can create a service account with read-only access to S3 by running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create iamserviceaccount --cluster=&lt;clusterName&gt; --name=s3-read-only --attach-policy-arn=arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess</code></pre>
</div>
</div>
<div class="paragraph">
<p>By default, it will be created in <code>default</code> namespace, but you can specify any other namespace, e.g.:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create iamserviceaccount --cluster=&lt;clusterName&gt; --name=s3-read-only --namespace=s3-app --attach-policy-arn=arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If the namespace doesn&#8217;t exist already, it will be created.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>If you have service account already created in the cluster (without an IAM Role), you will need to use <code>--override-existing-serviceaccounts</code> flag.</p>
</div>
<div class="paragraph">
<p>Custom tagging may also be applied to the IAM Role by specifying <code>--tags</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create iamserviceaccount --cluster=&lt;clusterName&gt; --name=&lt;serviceAccountName&gt; --tags "Owner=John Doe,Team=Some Team"</code></pre>
</div>
</div>
<div class="paragraph">
<p>CloudFormation will generate a role name that includes a random string. If you prefer a predetermined role name you can specify <code>--role-name</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create iamserviceaccount --cluster=&lt;clusterName&gt; --name=&lt;serviceAccountName&gt; --role-name "custom-role-name"</code></pre>
</div>
</div>
<div class="paragraph">
<p>When the service account is created and managed by some other tool, such as helm, use <code>--role-only</code> to prevent conflicts.
The other tool is then responsible for maintaining the role ARN annotation. Note that <code>--override-existing-serviceaccounts</code> has no effect on <code>roleOnly</code>/<code>--role-only</code> service accounts, the role will always be created.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create iamserviceaccount --cluster=&lt;clusterName&gt; --name=&lt;serviceAccountName&gt; --role-only --role-name=&lt;customRoleName&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>When you have an existing role which you want to use with a service account, you can provide the <code>--attach-role-arn</code> flag instead of providing the policies. To ensure the role can only be assumed by the specified service account, you should set a <a href="eks/latest/userguide/access-policies.html#access-policy-permissions">here</a> relationship policy document].</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create iamserviceaccount --cluster=&lt;clusterName&gt; --name=&lt;serviceAccountName&gt; --attach-role-arn=&lt;customRoleARN&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>To update a service accounts roles permissions you can run <code>eksctl update iamserviceaccount</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><code>eksctl delete iamserviceaccount</code> deletes Kubernetes <code>ServiceAccounts</code> even if they were not created by <code>eksctl</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect4">
<h5 id="usage-with-config-files">Usage with config files</h5>
<div class="paragraph">
<p>To manage <code>iamserviceaccounts</code> using config file, you will be looking to set <code>iam.withOIDC: true</code> and list account you want under <code>iam.serviceAccount</code>.</p>
</div>
<div class="paragraph">
<p>All of the commands support <code>--config-file</code>, you can manage <em>iamserviceaccounts</em> the same way as <em>nodegroups</em>.
The <code>eksctl create iamserviceaccount</code> command supports <code>--include</code> and <code>--exclude</code> flags (see
<a href="#node-include">this section</a> for more details about how these work).
And the <code>eksctl delete iamserviceaccount</code> command supports <code>--only-missing</code> as well, so you can perform deletions the same way as nodegroups.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>IAM service accounts are scoped within a namespace, i.e. two service accounts with the same name may exist in different namespaces. Thus, to uniquely define a service account as part of <code>--include</code>, <code>--exclude</code> flags, you will need to pass the name string in the <code>namespace/name</code> format. E.g.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="literalblock">
<div class="content">
<pre>```
eksctl create iamserviceaccount --config-file=&lt;path&gt; --include backend-apps/s3-reader
```</pre>
</div>
</div>
<div class="paragraph">
<p>The option to enable <code>wellKnownPolicies</code> is included for using IRSA with well-known
use cases like <code>cluster-autoscaler</code> and <code>cert-manager</code>, as a shorthand for lists
of policies.</p>
</div>
<div class="paragraph">
<p>Supported well-known policies and other properties of <code>serviceAccounts</code> are documented at
<a href="https://eksctl.io/usage/schema/#iam-serviceAccounts">the config schema</a>.</p>
</div>
<div class="paragraph">
<p>You use the following config example with <code>eksctl create cluster</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-YAML" data-lang="YAML"># An example of ClusterConfig with IAMServiceAccounts:
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-13
  region: us-west-2

iam:
  withOIDC: true
  serviceAccounts:
  - metadata:
      name: s3-reader
      # if no namespace is set, "default" will be used;
      # the namespace will be created if it doesn't exist already
      namespace: backend-apps
      labels: {aws-usage: "application"}
    attachPolicyARNs:
    - "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
    tags:
      Owner: "John Doe"
      Team: "Some Team"
  - metadata:
      name: cache-access
      namespace: backend-apps
      labels: {aws-usage: "application"}
    attachPolicyARNs:
    - "arn:aws:iam::aws:policy/AmazonDynamoDBReadOnlyAccess"
    - "arn:aws:iam::aws:policy/AmazonElastiCacheFullAccess"
  - metadata:
      name: cluster-autoscaler
      namespace: kube-system
      labels: {aws-usage: "cluster-ops"}
    wellKnownPolicies:
      autoScaler: true
    roleName: eksctl-cluster-autoscaler-role
    roleOnly: true
  - metadata:
      name: some-app
      namespace: default
    attachRoleARN: arn:aws:iam::123:role/already-created-role-for-app
nodeGroups:
  - name: "ng-1"
    tags:
      # EC2 tags required for cluster-autoscaler auto-discovery
      k8s.io/cluster-autoscaler/enabled: "true"
      k8s.io/cluster-autoscaler/cluster-13: "owned"
    desiredCapacity: 1</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you create a cluster without these fields set, you can use the following commands to enable all you need:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl utils associate-iam-oidc-provider --config-file=&lt;path&gt;
eksctl create iamserviceaccount --config-file=&lt;path&gt;</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="further-information-9">Further information</h5>
<div class="ulist">
<ul>
<li>
<p><a href="https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/">Introducing Fine-grained IAM Roles For Service Accounts</a></p>
</li>
<li>
<p><a href="eks/latest/userguide/access-policies.html#access-policy-permissions">here</a> EKS User Guide - IAM Roles For Service Accounts]</p>
</li>
<li>
<p><a href="https://eksctl.io/usage/iam-identity-mappings/">Mapping IAM users and role to Kubernetes RBAC roles</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect2 topic">
<h3 id="pod-id">7.6. EKS Pod Identity Associations</h3>
<div class="sect3">
<h4 id="introduction-5">7.6.1. Introduction</h4>
<div class="paragraph">
<p>AWS EKS has introduced a new enhanced mechanism called Pod Identity Association for cluster administrators to configure Kubernetes applications to receive IAM permissions required to connect with AWS services outside of the cluster. Pod Identity Association leverages IRSA, however, it makes it configurable directly through EKS API, eliminating the need for using IAM API altogether.</p>
</div>
<div class="paragraph">
<p>As a result, IAM roles no longer need to reference an <a href="#iam-how-works">OIDC provider</a> and hence won&#8217;t be tied to a single cluster anymore. This means, IAM roles can now be used across multiple EKS clusters without the need to update the role trust policy each time a new cluster is created. This in turn, eliminates the need for role duplication and simplifies the process of automating IRSA altogether.</p>
</div>
</div>
<div class="sect3">
<h4 id="prerequisites">7.6.2. Prerequisites</h4>
<div class="paragraph">
<p>Behind the scenes, the implementation of pod identity associations is running an agent as a daemonset on the worker nodes. To run the pre-requisite agent on the cluster, EKS provides a new add-on called EKS Pod Identity Agent. Therefore, creating pod identity associations (in general, and with <code>eksctl</code>) requires the <code>eks-pod-identity-agent</code> addon pre-installed on the cluster. This addon can be <a href="#addons-create">created using <code>eksctl</code></a> in the same fashion any other supported addon is, e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create addon --cluster my-cluster --name eks-pod-identity-agent</pre>
</div>
</div>
<div class="paragraph">
<p>Additionally, if using a pre-existing IAM role when creating a pod identity association, you must configure the role to trust the newly introduced EKS service principal (<code>pods.eks.amazonaws.com</code>). An example IAM trust policy can be found below:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "pods.eks.amazonaws.com"
            },
            "Action": [
                "sts:AssumeRole",
                "sts:TagSession"
            ]
        }
    ]
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>If instead you do not provide the ARN of an existing role to the create command, <code>eksctl</code> will create one behind the scenes and configure the above trust policy.</p>
</div>
</div>
<div class="sect3">
<h4 id="creating-pod-identity-associations">7.6.3. Creating Pod Identity Associations</h4>
<div class="paragraph">
<p>For manipulating pod identity associations, <code>eksctl</code> has added a new field under <code>iam.podIdentityAssociations</code>, e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">iam:
  podIdentityAssociations:
  - namespace: &lt;string&gt; #required
    serviceAccountName: &lt;string&gt; #required
    createServiceAccount: true #optional, default is false
    roleARN: &lt;string&gt; #required if none of permissionPolicyARNs, permissionPolicy and wellKnownPolicies is specified. Also, cannot be used together with any of the three other referenced fields.
    roleName: &lt;string&gt; #optional, generated automatically if not provided, ignored if roleARN is provided
    permissionPolicy: {} #optional
    permissionPolicyARNs: [] #optional
    wellKnownPolicies: {} #optional
    permissionsBoundaryARN: &lt;string&gt; #optional
    tags: {} #optional</code></pre>
</div>
</div>
<div class="paragraph">
<p>For a complete example, refer to <a href="https://github.com/eksctl-io/eksctl/blob/main/examples/39-pod-identity-association.yaml">pod-identity-associations.yaml</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Apart from <code>permissionPolicy</code> which is used as an inline policy document, all other fields have a CLI flag counterpart.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Creating pod identity associations can be achieved in the following ways. During cluster creation, by specifying the desired pod identity associations as part of the config file and running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create cluster -f config.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>Post cluster creation, using either a config file e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl create podidentityassociation -f config.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>OR using CLI flags e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl create podidentityassociation \
    --cluster my-cluster \
    --namespace default \
    --service-account-name s3-reader \
    --permission-policy-arns="arn:aws:iam::111122223333:policy/permission-policy-1, arn:aws:iam::111122223333:policy/permission-policy-2" \
    --well-known-policies="autoScaler,externalDNS" \
    --permissions-boundary-arn arn:aws:iam::111122223333:policy/permissions-boundary</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Only a single IAM role can be associated with a service account at a time. Therefore, trying to create a second pod identity association for the same service account will result in an error.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="fetching-pod-identity-associations">7.6.4. Fetching Pod Identity Associations</h4>
<div class="paragraph">
<p>To retrieve all pod identity associations for a certain cluster, run one of the following commands:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl get podidentityassociation -f config.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>OR</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl get podidentityassociation --cluster my-cluster</pre>
</div>
</div>
<div class="paragraph">
<p>Additionally, to retrieve only the pod identity associations within a given namespace, use the <code>--namespace</code> flag, e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl get podidentityassociation --cluster my-cluster --namespace default</pre>
</div>
</div>
<div class="paragraph">
<p>Finally, to retrieve a single association, corresponding to a certain K8s service account, also include the <code>--service-account-name</code> to the command above, i.e.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl get podidentityassociation --cluster my-cluster --namespace default --service-account-name s3-reader</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="updating-pod-identity-associations">7.6.5. Updating Pod Identity Associations</h4>
<div class="paragraph">
<p>To update the IAM role of one or more pod identity associations, either pass the new <code>roleARN(s)</code> to the config file e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">iam:
  podIdentityAssociations:
    - namespace: default
      serviceAccountName: s3-reader
      roleARN: new-role-arn-1
    - namespace: dev
      serviceAccountName: app-cache-access
      roleARN: new-role-arn-2</code></pre>
</div>
</div>
<div class="paragraph">
<p>and run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl update podidentityassociation -f config.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>OR (to update a single association) pass the new <code>--role-arn</code> via CLI flags:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl update podidentityassociation --cluster my-cluster --namespace default --service-account-name s3-reader --role-arn new-role-arn</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="deleting-pod-identity-associations">7.6.6. Deleting Pod Identity Associations</h4>
<div class="paragraph">
<p>To delete one or more pod identity associations, either pass <code>namespace(s)</code> and <code>serviceAccountName(s)</code> to the config file e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">iam:
  podIdentityAssociations:
    - namespace: default
      serviceAccountName: s3-reader
    - namespace: dev
      serviceAccountName: app-cache-access</code></pre>
</div>
</div>
<div class="paragraph">
<p>and run:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl delete podidentityassociation -f config.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>OR (to delete a single association) pass the <code>--namespace</code> and <code>--service-account-name</code> via CLI flags:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl delete podidentityassociation --cluster my-cluster --namespace default --service-account-name s3-reader</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="pod-id-support">7.6.7. EKS Add-ons support for pod identity associations</h4>
<div class="paragraph">
<p>EKS Add-ons also support receiving IAM permissions via EKS Pod Identity Associations. The config file exposes three fields that allow configuring these: <code>addon.podIdentityAssociations</code>, <code>addonsConfig.autoApplyPodIdentityAssociations</code> and <code>addon.useDefaultPodIdentityAssociations</code>. You can either explicitly configure the desired pod identity associations, using <code>addon.podIdentityAssociations</code>, or have <code>eksctl</code> automatically resolve (and apply) the recommended pod identity configuration, using either <code>addonsConfig.autoApplyPodIdentityAssociations</code> or <code>addon.useDefaultPodIdentityAssociations</code>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Not all EKS Add-ons will support pod identity associations at launch. For this case, required IAM permissions shall continue to be provided using <a href="#addons-create">IRSA settings</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="creating-addons-with-iam-permissions">Creating addons with IAM permissions</h5>
<div class="paragraph">
<p>When creating an addon that requires IAM permissions, <code>eksctl</code> will first check if either pod identity associations or IRSA settings are being explicitly configured as part of the config file, and if so, use one of those to configure the permissions for the addon. e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">addons:
- name: vpc-cni
  podIdentityAssociations:
  - serviceAccountName: aws-node
    permissionPolicyARNs: ["arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"]</code></pre>
</div>
</div>
<div class="paragraph">
<p>and run</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl create addon -f config.yaml
2024-05-13 15:38:58 [ℹ] pod identity associations are set for "vpc-cni" addon; will use these to configure required IAM permissions</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Setting both pod identities and IRSA at the same time is not allowed, and will result in a validation error.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>For EKS Add-ons that support pod identities, <code>eksctl</code> offers the option to automatically configure any recommended IAM permissions, on addon creation. This can be achieved by simply setting <code>addonsConfig.autoApplyPodIdentityAssociations: true</code> in the config file. e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">addonsConfig:
  autoApplyPodIdentityAssociations: true
# bear in mind that if either pod identity or IRSA configuration is explicitly set in the config file,
# or if the addon does not support pod identities,
# addonsConfig.autoApplyPodIdentityAssociations won't have any effect.
addons:
- name: vpc-cni</code></pre>
</div>
</div>
<div class="paragraph">
<p>and run</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl create addon -f config.yaml
2024-05-13 15:38:58 [ℹ] "addonsConfig.autoApplyPodIdentityAssociations" is set to true; will lookup recommended pod identity configuration for "vpc-cni" addon</code></pre>
</div>
</div>
<div class="paragraph">
<p>Equivalently, the same can be done via CLI flags e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl create addon --cluster my-cluster --name vpc-cni --auto-apply-pod-identity-associations</code></pre>
</div>
</div>
<div class="paragraph">
<p>To migrate an existing addon to use pod identity with the recommended IAM policies, use</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">addons:
- name: vpc-cni
  useDefaultPodIdentityAssociations: true</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ eksctl update addon -f config.yaml</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="updating-addons-with-iam-permissions">Updating addons with IAM permissions</h5>
<div class="paragraph">
<p>When updating an addon, specifying <code>addon.PodIdentityAssociations</code> will represent the single source of truth for the state that the addon shall have, after the update operation is completed. Behind the scenes, different types of operations are performed in order to achieve the desired state i.e.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>create pod identites that are present in the config file, but missing on the cluster</p>
</li>
<li>
<p>delete existing pod identites that were removed from the config file, together with any associated IAM resources</p>
</li>
<li>
<p>update existing pod identities that are also present in the config file, and for which the set of IAM permissions has changed</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The lifecycle of pod identity associations owned by EKS Add-ons is directly handled by the EKS Addons API, thus, using <code>eksctl update podidentityassociation</code> (to update IAM permissions) or <code>eksctl delete podidentityassociations</code> (to remove the association) is not supported for this type of associations. Instead, <code>eksctl update addon</code> or <code>eksctl delete addon</code> shall be used.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Let&#8217;s see an example for the above, starting by analyzing the initial pod identity config for the addon:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl get podidentityassociation --cluster my-cluster --namespace opentelemetry-operator-system --output json
[
    {
        ...
        "ServiceAccountName": "adot-col-prom-metrics",
        "RoleARN": "arn:aws:iam::111122223333:role/eksctl-my-cluster-addon-adot-podident-Role1-JwrGA4mn1Ny8",
        # OwnerARN is populated when the pod identity lifecycle is handled by the EKS Addons API
        "OwnerARN": "arn:aws:eks:us-west-2:111122223333:addon/my-cluster/adot/b2c7bb45-4090-bf34-ec78-a2298b8643f6"
    },
    {
        ...
        "ServiceAccountName": "adot-col-otlp-ingest",
        "RoleARN": "arn:aws:iam::111122223333:role/eksctl-my-cluster-addon-adot-podident-Role1-Xc7qVg5fgCqr",
        "OwnerARN": "arn:aws:eks:us-west-2:111122223333:addon/my-cluster/adot/b2c7bb45-4090-bf34-ec78-a2298b8643f6"
    }
]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now use the below configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">addons:
- name: adot
  podIdentityAssociations:

  # For the first association, the permissions policy of the role will be updated
  - serviceAccountName: adot-col-prom-metrics
    permissionPolicyARNs:
    #- arn:aws:iam::aws:policy/AmazonPrometheusRemoteWriteAccess
    - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy

  # The second association will be deleted, as it's been removed from the config file
  #- serviceAccountName: adot-col-otlp-ingest
  #  permissionPolicyARNs:
  #  - arn:aws:iam::aws:policy/AWSXrayWriteOnlyAccess

  # The third association will be created, as it's been added to the config file
  - serviceAccountName: adot-col-container-logs
    permissionPolicyARNs:
    - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy</code></pre>
</div>
</div>
<div class="paragraph">
<p>and run</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl update addon -f config.yaml
...
# updating the permission policy for the first association
2024-05-14 13:27:43 [ℹ]  updating IAM resources stack "eksctl-my-cluster-addon-adot-podidentityrole-adot-col-prom-metrics" for pod identity association "a-reaxk2uz1iknwazwj"
2024-05-14 13:27:44 [ℹ]  waiting for CloudFormation changeset "eksctl-opentelemetry-operator-system-adot-col-prom-metrics-update-1715682463" for stack "eksctl-my-cluster-addon-adot-podidentityrole-adot-col-prom-metrics"
2024-05-14 13:28:47 [ℹ]  waiting for CloudFormation stack "eksctl-my-cluster-addon-adot-podidentityrole-adot-col-prom-metrics"
2024-05-14 13:28:47 [ℹ]  updated IAM resources stack "eksctl-my-cluster-addon-adot-podidentityrole-adot-col-prom-metrics" for "a-reaxk2uz1iknwazwj"
# creating the IAM role for the second association
2024-05-14 13:28:48 [ℹ]  deploying stack "eksctl-my-cluster-addon-adot-podidentityrole-adot-col-container-logs"
2024-05-14 13:28:48 [ℹ]  waiting for CloudFormation stack "eksctl-my-cluster-addon-adot-podidentityrole-adot-col-container-logs"
2024-05-14 13:29:19 [ℹ]  waiting for CloudFormation stack "eksctl-my-cluster-addon-adot-podidentityrole-adot-col-container-logs"
# updating the addon, which handles the pod identity config changes behind the scenes
2024-05-14 13:29:19 [ℹ]  updating addon
# deleting the IAM role for the third association
2024-05-14 13:29:19 [ℹ]  deleting IAM resources for pod identity service account adot-col-otlp-ingest
2024-05-14 13:29:20 [ℹ]  will delete stack "eksctl-my-cluster-addon-adot-podidentityrole-adot-col-otlp-ingest"
2024-05-14 13:29:20 [ℹ]  waiting for stack "eksctl-my-cluster-addon-adot-podidentityrole-adot-col-otlp-ingest" to get deleted
2024-05-14 13:29:51 [ℹ]  waiting for CloudFormation stack "eksctl-my-cluster-addon-adot-podidentityrole-adot-col-otlp-ingest"
2024-05-14 13:29:51 [ℹ]  deleted IAM resources for addon adot</code></pre>
</div>
</div>
<div class="paragraph">
<p>now check that pod identity config was updated correctly</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl get podidentityassociation --cluster my-cluster --output json
[
    {
        ...
        "ServiceAccountName": "adot-col-prom-metrics",
        "RoleARN": "arn:aws:iam::111122223333:role/eksctl-my-cluster-addon-adot-podident-Role1-nQAlp0KktS2A",
        "OwnerARN": "arn:aws:eks:us-west-2:111122223333:addon/my-cluster/adot/1ec7bb63-8c4e-ca0a-f947-310c4b55052e"
    },
    {
        ...
        "ServiceAccountName": "adot-col-otlp-ingest",
        "RoleARN": "arn:aws:iam::111122223333:role/eksctl-my-cluster-addon-adot-podident-Role1-1k1XhAdziGzX",
        "OwnerARN": "arn:aws:eks:us-west-2:111122223333:addon/my-cluster/adot/1ec7bb63-8c4e-ca0a-f947-310c4b55052e"
    }
]</code></pre>
</div>
</div>
<div class="paragraph">
<p>To remove all pod identity associations from an addon, <code>addon.PodIdentityAssociations</code> must be explicitly set to <code>[]</code>, e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">addons:
- name: vpc-cni
  # omitting the `podIdentityAssociations` field from the config file,
  # instead of explicitly setting it to [], will result in a validation error
  podIdentityAssociations: []</code></pre>
</div>
</div>
<div class="paragraph">
<p>and run</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">eksctl update addon -f config.yaml</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="deleting-addons-with-iam-permissions">Deleting addons with IAM permissions</h5>
<div class="paragraph">
<p>Deleting an addon will also remove all pod identities associated with the addon. Deleting the cluster will achieve the same effect, for all addons. Any IAM roles for pod identities, created by <code>eksctl</code>, will be deleted as-well.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="migrating-existing-iamserviceaccounts-and-addons-to-pod-identity-associations">7.6.8. Migrating existing iamserviceaccounts and addons to pod identity associations</h4>
<div class="paragraph">
<p>There is an <code>eksctl</code> utils command for migrating existing IAM Roles for service accounts to pod identity associations, i.e.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils migrate-to-pod-identity --cluster my-cluster --approve</pre>
</div>
</div>
<div class="paragraph">
<p>Behind the scenes, the command will apply the following steps:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>install the <code>eks-pod-identity-agent</code> addon if not already active on the cluster</p>
</li>
<li>
<p>identify all IAM Roles that are associated with iamserviceaccounts</p>
</li>
<li>
<p>identify all IAM Roles that are associated with EKS addons that support pod identity associations</p>
</li>
<li>
<p>update the IAM trust policy of all identified roles, with an additional trusted entity, pointing to the new EKS Service principal (and, optionally, remove exising OIDC provider trust relationship)</p>
</li>
<li>
<p>create pod identity associations for filtered roles associated with iamserviceaccounts</p>
</li>
<li>
<p>update EKS addons with pod identities (EKS API will create the pod identities behind the scenes)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Running the command without the <code>--approve</code> flag will only output a plan consisting of a set of tasks reflecting the steps above, e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">[ℹ]  (plan) would migrate 2 iamserviceaccount(s) and 2 addon(s) to pod identity association(s) by executing the following tasks
[ℹ]  (plan)

3 sequential tasks: { install eks-pod-identity-agent addon,
    ## tasks for migrating the addons
    2 parallel sub-tasks: {
        2 sequential sub-tasks: {
            update trust policy for owned role "eksctl-my-cluster--Role1-DDuMLoeZ8weD",
            migrate addon aws-ebs-csi-driver to pod identity,
        },
        2 sequential sub-tasks: {
            update trust policy for owned role "eksctl-my-cluster--Role1-xYiPFOVp1aeI",
            migrate addon vpc-cni to pod identity,
        },
    },
    ## tasks for migrating the iamserviceaccounts
    2 parallel sub-tasks: {
        2 sequential sub-tasks: {
            update trust policy for owned role "eksctl-my-cluster--Role1-QLXqHcq9O1AR",
            create pod identity association for service account "default/sa1",
        },
        2 sequential sub-tasks: {
            update trust policy for unowned role "Unowned-Role1",
            create pod identity association for service account "default/sa2",
        },
    }
}
[ℹ]  all tasks were skipped
[!]  no changes were applied, run again with '--approve' to apply the changes</code></pre>
</div>
</div>
<div class="paragraph">
<p>The existing OIDC provider trust relationship is always being removed from IAM Roles associated with EKS Add-ons. Additionally, to remove the existing OIDC provider trust relationship from IAM Roles associated with iamserviceaccounts, run the command with <code>--remove-oidc-provider-trust-relationship</code> flag, e.g.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>eksctl utils migrate-to-pod-identity --cluster my-cluster --approve --remove-oidc-provider-trust-relationship</pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="further-references-2">7.6.9. Further references</h4>
<div class="paragraph">
<p><a href="eks/latest/userguide/add-ons-iam.html">Official AWS Userdocs for EKS Add-ons support for pod identities</a></p>
</div>
<div class="paragraph">
<p><a href="https://aws.amazon.com/blogs/aws/amazon-eks-pod-identity-simplifies-iam-permissions-for-applications-on-amazon-eks-clusters/">Official AWS Blog Post on Pod Identity Associations</a></p>
</div>
<div class="paragraph">
<p><a href="eks/latest/userguide/pod-identities.html">Official AWS userdocs for Pod Identity Associations</a></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="gitops">8. GitOps with Flux v2</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://www.weave.works/technologies/gitops/">Gitops</a> is a way to do Kubernetes application delivery. It
works by using Git as a single source of truth for Kubernetes resources
and everything else. With Git at the center of your delivery pipelines,
you and your team can make pull requests to accelerate and simplify
application deployments and operations tasks to Kubernetes.</p>
</div>
<div class="sect2">
<h3 id="installing-flux-v2-gitops-toolkit">8.1. Installing Flux v2 (GitOps Toolkit)</h3>
<div class="paragraph">
<p>Starting with version <code>0.53.0</code>, <code>eksctl</code> provides the option to bootstrap <a href="https://fluxcd.io/flux/">Flux v2</a> components into an EKS cluster, with the <code>enable flux</code> subcommand.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl enable flux --config-file &lt;config-file&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>enable flux</code> command will shell out to the <code>flux</code> binary and run the <code>flux bootstrap</code> command against the cluster.</p>
</div>
<div class="paragraph">
<p>In order to allow users to specify whichever <code>bootstrap</code> flags they like, the <code>eksctl</code>
API exposes an arbitrary <code>map[string]string</code> of <code>flags</code>. To find out which flags you need
to bootstrap your cluster, simply run <code>flux bootstrap --help</code>.</p>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-YAML" data-lang="YAML">---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: cluster-12
  region: eu-north-1

# other cluster config ...

gitops:
  flux:
    gitProvider: github      # required. options are github, gitlab or git
    flags:                   # required. arbitrary map[string]string for all flux args.
      owner: "dr-who"
      repository: "our-org-gitops-repo"
      private: "true"
      branch: "main"
      namespace: "flux-system"
      path: "clusters/cluster-12"
      team: "team1,team2"</code></pre>
</div>
</div>
<div class="paragraph">
<p>This example configuration can be found <a href="https://github.com/eksctl-io/eksctl/blob/main/examples/12-gitops-toolkit.yaml">here</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Flux v2 configuration can <strong>only</strong> be provided via configuration file; no flags
are exposed on this subcommand other than <code>--config-file</code>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Flux will install default toolkit components to the cluster, unless told otherwise by your configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">kubectl get pods --namespace flux-system
NAME                                       READY   STATUS    RESTARTS   AGE
helm-controller-7cfb98d895-zmmfc           1/1     Running   0          3m30s
kustomize-controller-557986cf44-2jwjh      1/1     Running   0          3m35s
notification-controller-65694dc94d-rhbxk   1/1     Running   0          3m20s
source-controller-7f856877cf-jgwdk         1/1     Running   0          3m39s</code></pre>
</div>
</div>
<div class="paragraph">
<p>For instructions on how to use your newly installed Gitops Toolkit,
refer to the <a href="https://fluxcd.io/flux/">official docs</a>.</p>
</div>
<div class="sect3">
<h4 id="bootstrap-after-cluster-create">8.1.1. Bootstrap after cluster create</h4>
<div class="paragraph">
<p>You can have your cluster bootstrapped immediately following a cluster create
by including your Flux configuration in your config file and running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-console" data-lang="console">eksctl create cluster --config-file &lt;config-file&gt;</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="requirements">8.1.2. Requirements</h4>
<div class="sect4">
<h5 id="environment-variables">Environment variables</h5>
<div class="paragraph">
<p>Before running <code>eksctl enable flux</code>, ensure that you have read the <a href="https://fluxcd.io/flux/get-started/">Flux getting started docs</a>. If you are using Github or Gitlab as your git provider, either <code>GITHUB_TOKEN</code> or <code>GITLAB_TOKEN</code> must be exported with your Personal Access Token in your session. Please refer to the Flux docs for any other requirements.</p>
</div>
</div>
<div class="sect4">
<h5 id="flux-version">Flux version</h5>
<div class="paragraph">
<p>Eksctl requires a minimum Flux version of <code>0.32.0</code>.</p>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="quickstart-profiles">8.2. Quickstart profiles</h3>
<div class="paragraph">
<p>Quickstart profiles will <strong>not</strong> be supported with Flux v2.</p>
</div>
</div>
<div class="sect2">
<h3 id="further-reading">8.3. Further reading</h3>
<div class="paragraph">
<p>To learn more about gitops and Flux, check out <a href="https://fluxcd.io/flux/">Flux official webpage</a>.</p>
</div>
<div class="paragraph">
<p>For Enterprise support with SLA guarantees for open source Flux and eksctl explore <a href="https://www.weave.works/product/gitops/">Weave GitOps Assured</a>.</p>
</div>
<div class="paragraph">
<p>For a trusted, fleet-scale commercial GitOps automation solution explore <a href="https://www.weave.works/product/gitops-enterprise/">Weave GitOps Enterprise</a>.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="deployment">9. Deployment options</h2>
<div class="sectionbody">
<div class="paragraph">
<p>eksctl is a command-line utility tool that automates and simplifies the process of creating, managing, and operating Amazon Elastic Kubernetes Service (Amazon EKS) clusters. Written in Go, eksctl provides a declarative syntax through YAML configurations and CLI commands to handle complex EKS cluster operations that would otherwise require multiple manual steps across different AWS services.</p>
</div>
<div class="paragraph">
<p>eksctl is particularly valuable for DevOps engineers, platform teams, and Kubernetes administrators who need to consistently deploy and manage EKS clusters at scale. It&#8217;s especially useful for organizations transitioning from self-managed Kubernetes to EKS, or those implementing infrastructure as code (IaC) practices, as it can be integrated into existing CI/CD pipelines and automation workflows. The tool abstracts away many of the complex interactions between AWS services required for EKS cluster setup, such as VPC configuration, IAM role creation, and security group management.</p>
</div>
<div class="paragraph">
<p>Key features of eksctl include the ability to create fully functional EKS clusters with a single command, support for custom networking configurations, automated node group management, and GitOps workflow integration. The tool manages cluster upgrades, scales node groups, and handles add-on management through a declarative approach. eksctl also provides advanced capabilities such as Fargate profile configuration, managed node group customization, and spot instance integration, while maintaining compatibility with other AWS tools and services through native AWS SDK integration.</p>
</div>
<div class="sect2">
<h3 id="eksctl-anywhere">9.1. EKS Anywhere</h3>
<div class="paragraph">
<p><code>eksctl</code> provides access to AWS' feature called <code>EKS Anywhere</code> with the sub command <code>eksctl anywhere</code>.
This requires the <code>eksctl-anywhere</code> binary present on <code>PATH</code>. Please follow the instruction outlined here <a href="https://anywhere.eks.amazonaws.com/docs/getting-started/install/">Install eksctl-anywhere</a>
to install it.</p>
</div>
<div class="paragraph">
<p>Once done, execute anywhere commands by running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl anywhere version
v0.5.0</code></pre>
</div>
</div>
<div class="paragraph">
<p>For more information about EKS Anywhere, please visit <a href="https://anywhere.eks.amazonaws.com/">EKS Anywhere Website</a>.</p>
</div>
</div>
<div class="sect2 topic">
<h3 id="outposts">9.2. AWS Outposts Support</h3>
<div class="paragraph">
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-outposts.html">AWS
Outposts</a> support in eksctl lets you create local clusters with the
entire Kubernetes cluster, including the EKS control plane and worker
nodes, running locally on AWS Outposts. Customers can either create a
local cluster with both the EKS control plane and worker nodes running
locally on AWS Outposts, or they can extend an existing EKS cluster
running in an AWS region to AWS Outposts by creating worker nodes on
Outposts.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
EKS Managed Nodegroups are not supported on Outposts.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>INFO: Local clusters support Outpost racks only.</p>
</div>
<div class="sect3">
<h4 id="creating-a-local-cluster-on-aws-outposts">9.2.1. Creating a local cluster on AWS Outposts</h4>
<div class="paragraph">
<p>To create the EKS control plane and nodegroups on AWS Outposts, set
<code>outpost.controlPlaneOutpostARN</code> to the Outpost ARN, as in:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># outpost.yaml
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: outpost
  region: us-west-2

outpost:
  # Required.
  controlPlaneOutpostARN: "arn:aws:outposts:us-west-2:1234:outpost/op-1234"
  # Optional, defaults to the smallest available instance type on the Outpost.
  controlPlaneInstanceType: m5d.large</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create cluster -f outpost.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>This instructs eksctl to create the EKS control plane and subnets on the
specified Outpost. Since an Outposts rack exists in a single
availability zone, eksctl creates only one public and private subnet.
eksctl does not associate the created VPC with a
<a href="https://docs.aws.amazon.com/outposts/latest/userguide/outposts-local-gateways.html">local
gateway</a> and, as such, eksctl will lack connectivity to the API server
and will be unable to create nodegroups. Therefore, if the
<code>ClusterConfig</code> contains any nodegroups during cluster creation, the
command must be run with <code>--without-nodegroup</code>, as in:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">eksctl create cluster -f outpost.yaml --without-nodegroup</code></pre>
</div>
</div>
<div class="paragraph">
<p>It is the customer’s responsibility to associate the eksctl-created VPC
with the local gateway after cluster creation to enable connectivity to
the API server. After this step, nodegroups can be created using
<code>eksctl create nodegroup</code>.</p>
</div>
<div class="paragraph">
<p>You can optionally specify the instance type for the control plane nodes
in <code>outpost.controlPlaneInstanceType</code> or for the nodegroups in
<code>nodeGroup.instanceType</code>, but the instance type must exist on Outpost
or eksctl will return an error. By default, eksctl attempts to choose
the smallest available instance type on Outpost for the control plane
nodes and nodegroups.</p>
</div>
<div class="paragraph">
<p>When the control plane is on Outposts, nodegroups are created on that
Outpost. You can optionally specify the Outpost ARN for the nodegroup in
<code>nodeGroup.outpostARN</code> but it must match the control plane’s Outpost
ARN.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># outpost-fully-private.yaml
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: outpost-fully-private
  region: us-west-2

privateCluster:
  enabled: true

outpost:
  # Required.
  controlPlaneOutpostARN: "arn:aws:outposts:us-west-2:1234:outpost/op-1234"
  # Optional, defaults to the smallest available instance type on the Outpost.
  controlPlaneInstanceType: m5d.large</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># outpost.yaml
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: outpost
  region: us-west-2

outpost:
  # Required.
  controlPlaneOutpostARN: "arn:aws:outposts:us-west-2:1234:outpost/op-1234"
  # Optional, defaults to the smallest available instance type on the Outpost.
  controlPlaneInstanceType: m5d.large

  controlPlanePlacement:
    groupName: placement-group-name</code></pre>
</div>
</div>
<div class="sect4">
<h5 id="existing-vpc">Existing VPC</h5>
<div class="paragraph">
<p>Customers with an existing VPC can create local clusters on AWS Outposts
by specifying the subnet configuration in <code>vpc.subnets</code>, as in:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># outpost-existing-vpc.yaml
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: outpost
  region: us-west-2

vpc:
  id: vpc-1234
  subnets:
    private:
      outpost-subnet-1:
        id: subnet-1234

nodeGroups:
  - name: outpost-ng
    privateNetworking: true

outpost:
    # Required.
    controlPlaneOutpostARN: "arn:aws:outposts:us-west-2:1234:outpost/op-1234"
    # Optional, defaults to the smallest available instance type on the Outpost.
    controlPlaneInstanceType: m5d.large</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create cluster -f outpost-existing-vpc.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>The subnets must exist on the Outpost specified in
<code>outpost.controlPlaneOutpostARN</code> or eksctl will return an error. You
can also specify nodegroups during cluster creation if you have access
to the local gateway for the subnet, or have connectivity to VPC
resources.</p>
</div>
</div>
<div class="sect4">
<h5 id="extending-existing-clusters-to-aws-outposts">Extending existing clusters to AWS Outposts</h5>
<div class="paragraph">
<p>Customers can extend an existing EKS cluster running in an AWS region to
AWS Outposts by setting <code>nodeGroup.outpostARN</code> for new nodegroups to
create nodegroups on Outposts, as in:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># extended-cluster.yaml
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: existing-cluster
  region: us-west-2

nodeGroups:
  # Nodegroup will be created in an AWS region.
  - name: ng

  # Nodegroup will be created on the specified Outpost.
  - name: outpost-ng
    privateNetworking: true
    outpostARN: "arn:aws:outposts:us-west-2:1234:outpost/op-1234"</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create nodegroup -f extended-cluster.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this setup, the EKS control plane runs in an AWS region while
nodegroups with <code>outpostARN</code> set run on the specified Outpost. When a
nodegroup is being created on Outposts for the first time, eksctl
extends the VPC by creating subnets on the specified Outpost. These
subnets are used to create nodegroups that have <code>outpostARN</code> set.</p>
</div>
<div class="paragraph">
<p>Customers with a pre-existing VPC are required to create the subnets on
Outposts and pass them in <code>nodeGroup.subnets</code>, as in:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># extended-cluster-vpc.yaml
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: extended-cluster-vpc
  region: us-west-2

vpc:
  id: vpc-1234
    subnets:
      private:
        outpost-subnet-1:
          id: subnet-1234

nodeGroups:
  # Nodegroup will be created in an AWS region.
  - name: ng

  # Nodegroup will be created on the specified Outpost.
  - name: outpost-ng
    privateNetworking: true
    # Subnet IDs for subnets created on Outpost.
    subnets: [subnet-5678]
    outpostARN: "arn:aws:outposts:us-west-2:1234:outpost/op-1234"</code></pre>
</div>
</div>
<div class="paragraph">
<p>???+ note - Only Amazon Linux 2 is supported for nodegroups when the
control plane is on Outposts. - Only EBS gp2 volume types are supported
for nodegroups on Outposts.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="features-unsupported-on-local-clusters">9.2.2. Features unsupported on local clusters</h4>
<div class="ulist">
<ul>
<li>
<p>Availability Zones cannot be specified as it defaults to the Outpost
availability zone.</p>
</li>
<li>
<p><code>vpc.publicAccessCIDRs</code> and <code>vpc.autoAllocateIPv6</code> are not
supported.</p>
</li>
<li>
<p>Public endpoint access to the API server is not supported as a local
cluster can only be created with private-only endpoint access.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="further-information-10">9.2.3. Further information</h4>
<div class="ulist">
<ul>
<li>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-outposts.html">Amazon
EKS on AWS Outposts</a></p>
</li>
<li>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-outposts-local-cluster-overview.html">Local
clusters for Amazon EKS on AWS Outposts</a></p>
</li>
<li>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-outposts-local-cluster-create.html">Creating
local clusters</a></p>
</li>
<li>
<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-outposts-self-managed-nodes.html">Launching
self-managed Amazon Linux nodes on an Outpost</a></p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="security">10. Security</h2>
<div class="sectionbody">
<div class="paragraph">
<p><code>eksctl</code> provides some options that can improve the security of your EKS cluster.</p>
</div>
<div class="sect2">
<h3 id="withoidc">10.1. <code>withOIDC</code></h3>
<div class="paragraph">
<p>limit permissions granted to nodes in your cluster, instead granting the necessary permissions
only to the CNI service account.</p>
</div>
<div class="paragraph">
<p>The background is described in <a href="eks/latest/userguide/cni-iam-role.html">this AWS documentation</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="disablepodimds">10.2. <code>disablePodIMDS</code></h3>
<div class="paragraph">
<p>non host networking pods running in this nodegroup from making IMDS requests.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>This can not be used together with <a href="#iam-policies"><code>withAddonPolicies</code></a>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2 topic">
<h3 id="kms-envelope-encryption-for-eks-clusters">10.3. KMS Envelope Encryption for EKS clusters</h3>
<div class="paragraph">
<p>EKS supports using <a href="https://aws.amazon.com/about-aws/whats-new/2021/03/amazon-eks-supports-adding-kms-envelope-encryption-to-existing-clusters/">AWS KMS</a> keys to provide envelope encryption of Kubernetes secrets stored in EKS. Envelope encryption adds an addition, customer-managed layer of encryption for application secrets or user data that is stored within a Kubernetes cluster.</p>
</div>
<div class="paragraph">
<p>Previously, Amazon EKS supported <a href="https://aws.amazon.com/about-aws/whats-new/2020/03/amazon-eks-adds-envelope-encryption-for-secrets-with-aws-kms/">enabling envelope encryption</a> using KMS keys only during cluster creation. Now, you can enable envelope encryption for Amazon EKS clusters at any time.</p>
</div>
<div class="paragraph">
<p>Read more about Using EKS encryption provider support for defense-in-depth post on the <a href="https://aws.amazon.com/blogs/containers/using-eks-encryption-provider-support-for-defense-in-depth/">AWS containers blog</a>.</p>
</div>
<div class="sect3">
<h4 id="creating-a-cluster-with-kms-encryption-enabled">10.3.1. Creating a cluster with KMS encryption enabled</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># kms-cluster.yaml
# A cluster with KMS encryption enabled
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: kms-cluster
  region: us-west-2

managedNodeGroups:
- name: ng
# more config

secretsEncryption:
  # KMS key used for envelope encryption of Kubernetes secrets
  keyARN: arn:aws:kms:us-west-2:&lt;account&gt;:key/&lt;key&gt;</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl create cluster -f kms-cluster.yaml</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="enabling-kms-encryption-on-an-existing-cluster">10.3.2. Enabling KMS encryption on an existing cluster</h4>
<div class="paragraph">
<p>To enable KMS encryption on a cluster that doesn&#8217;t already have it enabled, run</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl utils enable-secrets-encryption -f kms-cluster.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>or without a config file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl utils enable-secrets-encryption --cluster=kms-cluster --key-arn=arn:aws:kms:us-west-2:&lt;account&gt;:key/&lt;key&gt; --region=&lt;region&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>In addition to enabling KMS encryption on the EKS cluster, eksctl also re-encrypts all existing Kubernetes secrets using the new KMS key
by updating them with the annotation <code>eksctl.io/kms-encryption-timestamp</code>. This behaviour can be disabled by passing <code>--encrypt-existing-secrets=false</code>, as in:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ eksctl utils enable-secrets-encryption --cluster=kms-cluster --key-arn=arn:aws:kms:us-west-2:&lt;account&gt;:key/&lt;key&gt; --encrypt-existing-secrets=false --region=&lt;region&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>If a cluster already has KMS encryption enabled, eksctl will proceed to re-encrypting all existing secrets.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Once KMS encryption is enabled, it cannot be disabled or updated to use a different KMS key.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="troubleshooting">11. Troubleshooting</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="failed-stack-creation">11.1. Failed stack creation</h3>
<div class="paragraph">
<p>You can use the <code>--cfn-disable-rollback</code> flag to stop Cloudformation from rolling
back failed stacks to make debugging easier.</p>
</div>
</div>
<div class="sect2">
<h3 id="subnet-id-subnet-11111111-is-not-the-same-as-subnet-22222222">11.2. subnet ID "subnet-11111111" is not the same as "subnet-22222222"</h3>
<div class="paragraph">
<p>Given a config file specifying subnets for a VPC like the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: test
  region: us-east-1

vpc:
  subnets:
    public:
      us-east-1a: {id: subnet-11111111}
      us-east-1b: {id: subnet-22222222}
    private:
      us-east-1a: {id: subnet-33333333}
      us-east-1b: {id: subnet-44444444}

nodeGroups: []</code></pre>
</div>
</div>
<div class="paragraph">
<p>An error <code>subnet ID "subnet-11111111" is not the same as "subnet-22222222"</code> means that the subnets specified are not
placed in the right Availability zone. Check in the AWS console which is the right subnet ID for each Availability Zone.</p>
</div>
<div class="paragraph">
<p>In this example, the correct configuration for the VPC would be:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">vpc:
  subnets:
    public:
      us-east-1a: {id: subnet-22222222}
      us-east-1b: {id: subnet-11111111}
    private:
      us-east-1a: {id: subnet-33333333}
      us-east-1b: {id: subnet-44444444}</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="deletion-issues">11.3. Deletion issues</h3>
<div class="paragraph">
<p>If your delete does not work, or you forget to add <code>--wait</code> on the delete, you may need to go to use amazon&#8217;s other tools to delete the cloudformation stacks. This can be accomplished via the gui or with the aws cli.</p>
</div>
</div>
<div class="sect2">
<h3 id="kubectl-logs-and-kubectl-run-fails-with-authorization-error">11.4. kubectl logs and kubectl run fails with Authorization Error</h3>
<div class="paragraph">
<p>If, when running <code>kubectl logs</code> and <code>kubectl run</code> fails with an error like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Error attaching, falling back to logs: unable to upgrade connection: Authorization error (user=kube-apiserver-kubelet-client, verb=create, resource=nodes, subresource=proxy)</pre>
</div>
</div>
<div class="paragraph">
<p>or</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Error from server (InternalError): Internal error occurred: Authorization error (user=kube-apiserver-kubelet-client, verb=get, resource=nodes, subresource=proxy)</pre>
</div>
</div>
<div class="paragraph">
<p>and your nodes are deployed in a private subnet you may need to set <a href="vpc/latest/userguide/vpc-dns.html#vpc-dns-support">enableDnsHostnames</a>. More details can be found in <a href="https://github.com/eksctl-io/eksctl/issues/4645">this issue</a>.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="announcements">12. Announcements</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="managed-nodegroups-default">12.1. Managed Nodegroups Default</h3>
<div class="paragraph">
<p>As of <a href="https://github.com/eksctl-io/eksctl/releases/tag/0.58.0">eksctl v0.58.0</a>, eksctl creates managed nodegroups by
default when a <code>ClusterConfig</code> file isn&#8217;t specified for <code>eksctl create cluster</code> and <code>eksctl create nodegroup</code>.
To create a self-managed nodegroup, pass <code>--managed=false</code>. This may break scripts not using a config file if a feature
not supported in managed nodegroups, e.g., Windows nodegroups, is being used.
To fix this, pass <code>--managed=false</code>, or specify your nodegroup config in a <code>ClusterConfig</code> file using the
<code>nodeGroups</code> field which creates a self-managed nodegroup.</p>
</div>
</div>
<div class="sect2">
<h3 id="nodegroup-bootstrap-override-for-custom-amis">12.2. Nodegroup Bootstrap Override For Custom AMIs</h3>
<div class="paragraph">
<p>This change was announced in the issue <a href="https://github.com/eksctl-io/eksctl/issues/3563">Breaking: overrideBootstrapCommand soon&#8230;&#8203;</a>.
Now, it has come to pass in <a href="https://github.com/eksctl-io/eksctl/pull/4968">this</a> PR. Please read the attached issue carefully about
why we decided to move away from supporting custom AMIs without bootstrap scripts or with partial bootstrap scripts.</p>
</div>
<div class="paragraph">
<p>We still provide a helper! Migrating hopefully is not that painful. <code>eksctl</code> still provides a script, which when sourced,
will export a couple of helpful environment properties and settings. This script is located <a href="https://github.com/eksctl-io/eksctl/blob/70a289d62e3c82e6177930cf2469c2572c82e104/pkg/nodebootstrap/assets/scripts/bootstrap.helper.sh">here</a>.</p>
</div>
<div class="paragraph">
<p>The following environment properties will be at your disposal:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">API_SERVER_URL
B64_CLUSTER_CA
INSTANCE_ID
INSTANCE_LIFECYCLE
CLUSTER_DNS
NODE_TAINTS
MAX_PODS
NODE_LABELS
CLUSTER_NAME
CONTAINER_RUNTIME # default is docker
KUBELET_EXTRA_ARGS # for details, look at the script</code></pre>
</div>
</div>
<div class="paragraph">
<p>The minimum that needs to be used when overriding so <code>eksctl</code> doesn&#8217;t fail, is labels! <code>eksctl</code> relies on a specific set of
labels to be on the node, so it can find them. When defining the override, please provide this <strong>bare minimum</strong> override
command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">    overrideBootstrapCommand: |
      #!/bin/bash

      source /var/lib/cloud/scripts/eksctl/bootstrap.helper.sh

      # Note "--node-labels=${NODE_LABELS}" needs the above helper sourced to work, otherwise will have to be defined manually.
      /etc/eks/bootstrap.sh ${CLUSTER_NAME} --container-runtime containerd --kubelet-extra-args "--node-labels=${NODE_LABELS}"</code></pre>
</div>
</div>
<div class="paragraph">
<p>For nodegroups that have no outbound internet access, you&#8217;ll need to supply <code>--apiserver-endpoint</code> and <code>--b64-cluster-ca</code>
to the bootstrap script as follows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">    overrideBootstrapCommand: |
      #!/bin/bash

      source /var/lib/cloud/scripts/eksctl/bootstrap.helper.sh

      # Note "--node-labels=${NODE_LABELS}" needs the above helper sourced to work, otherwise will have to be defined manually.
      /etc/eks/bootstrap.sh ${CLUSTER_NAME} --container-runtime containerd --kubelet-extra-args "--node-labels=${NODE_LABELS}" \
        --apiserver-endpoint ${API_SERVER_URL} --b64-cluster-ca ${B64_CLUSTER_CA}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note the <em>`--node-labels`</em> setting. If this is not defined, the node will join the cluster, but <code>eksctl</code> will ultimately
time out on the last step when it&#8217;s waiting for the nodes to be <code>Ready</code>. It&#8217;s doing a Kubernetes lookup for nodes that
have the label <code>alpha.eksctl.io/nodegroup-name=&lt;cluster-name&gt;</code>. This is only true for unmanaged nodegroups. For managed
it&#8217;s using a different label.</p>
</div>
<div class="paragraph">
<p>If, at all, it&#8217;s possible to switch to managed nodegroups to avoid this overhead, the time has come now to do that. Makes
all the overriding a lot easier.</p>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2025-02-13 23:34:13 UTC
</div>
</div>
</body>
</html>